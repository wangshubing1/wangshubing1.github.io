<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CDH6.0.1 HUE 访问HBase报500错误 ，无法连接thrift</title>
    <url>/2019/01/25/CDH6.0.1%20HUE%20%E8%AE%BF%E9%97%AEHBase%E6%8A%A5500%E9%94%99%E8%AF%AF%20%EF%BC%8C%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5thrift/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul>
<li><p>公司决定搞新版本deCDH，然后我就安装部署了CDH6.0.1，安装的路程就不说了，本以为安装好添加服务就可以用了，可是却遇到这个问题，我开始以为thrift jar包问题，问别人要了CDH6.0.0里面的jar包替换，后面又说什么配置问题，均未解决，后来找java大神看thrift jar的源码，然后发现问题是，我们请求的服务跟它自己要去调用的服务不一致。</p>
</li>
<li><p>原因找出来了，就是找不到解决方法，公司暂时也不用HBse，就暂时搁置了，直到今天，我这个服务器之前被安装过其他版本的CDH，导致系统的一直配置链接路径不对主要是/etc/alternatives/目录下面的一些链接是错的这个问题待会再说</p>
<h4 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h4><p>hbase thrift端口报500</p>
</li>
</ul>
<p>hue报HBase Browser Failed to authenticate to HBase Thrift Server, check authentication configurations</p>
<p>HTTP ERROR 500</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)</span><br><span class="line">at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)</span><br><span class="line">at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)</span><br><span class="line">at org.apache.thrift.protocol.TBinaryProtocol.readByte(TBinaryProtocol.java:285)</span><br><span class="line">at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:230)</span><br><span class="line">at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)</span><br><span class="line">at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)</span><br><span class="line">at org.apache.hadoop.hbase.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:122)</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)</span><br><span class="line">at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)</span><br><span class="line">at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)</span><br><span class="line">at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)</span><br><span class="line">at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)</span><br><span class="line">at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)</span><br><span class="line">at org.mortbay.jetty.Server.handle(Server.java:326)</span><br><span class="line">at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)</span><br><span class="line">at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)</span><br><span class="line">at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)</span><br><span class="line">at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)</span><br><span class="line">at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)</span><br><span class="line">at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)</span><br><span class="line">at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)</span><br></pre></td></tr></table></figure>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>我们进入hue服务配置项-类别-高级-hue_safety_valve.ini 的 Hue 服务高级配置代码段（安全阀）<br>或者直接搜索后面这个配置项<br>在里面配置如下</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="section">[hbase]</span></span><br><span class="line"><span class="attr">hbase_conf_dir</span>=&#123;&#123;HBASE_CONF_DIR&#125;&#125;</span><br><span class="line"><span class="attr">thrift_transport</span>=buffered</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：我们要先确定hbase配置里已经启用了thrift服务哦，还有hue也要依赖这个服务<br>同时确保 hbase-&gt;配置-&gt;HBase Thrift Server 中以下两项去掉勾中</p>
</blockquote>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">启用 <span class="selector-tag">HBase</span> <span class="selector-tag">Thrift</span> 服务器压缩协议</span><br><span class="line"><span class="selector-tag">hbase</span><span class="selector-class">.regionserver</span><span class="selector-class">.thrift</span><span class="selector-class">.compact</span></span><br><span class="line"></span><br><span class="line">启用 <span class="selector-tag">HBase</span> <span class="selector-tag">Thrift</span> 服务器框架运输</span><br><span class="line"><span class="selector-tag">hbase</span><span class="selector-class">.regionserver</span><span class="selector-class">.thrift</span><span class="selector-class">.framed</span></span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>error</category>
      </categories>
      <tags>
        <tag>error bug</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink DataSet 迭代</title>
    <url>/2019/01/25/Flink%20DataSet%20%E8%BF%AD%E4%BB%A3/</url>
    <content><![CDATA[<h2 id="批量迭代"><a href="#批量迭代" class="headerlink" title="批量迭代"></a>批量迭代</h2><ul>
<li><p>要创建BulkIteration，请调用iterate(int)迭代的DataSet方法。这将返回一个IterativeDataSet，可以使用常规运算符进行转换。迭代调用的单个参数指定最大迭代次数。</p>
</li>
<li><p>要指定迭代的结束，请调用closeWith(DataSet)方法IterativeDataSet以指定应将哪个转换反馈到下一次迭代。closeWith(DataSet, DataSet)如果此DataSet为空，您可以选择指定终止条件，该条件评估第二个DataSet并终止迭代。如果未指定终止条件，则迭代将在给定的最大数量迭代后终止。</p>
</li>
</ul>
<p>以下示例迭代地估计数量Pi。目标是计算落入单位圆的随机点数。在每次迭代中，挑选一个随机点。如果此点位于单位圆内，我们会增加计数。然后估计Pi作为结果计数除以迭代次数乘以4。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create initial IterativeDataSet</span></span><br><span class="line">IterativeDataSet&lt;Integer&gt; initial = env.fromElements(<span class="number">0</span>).iterate(<span class="number">10000</span>);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Integer&gt; iteration = initial.map(<span class="keyword">new</span> MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(Integer i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> x = Math.random();</span><br><span class="line">        <span class="keyword">double</span> y = Math.random();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> i + ((x * x + y * y &lt; <span class="number">1</span>) ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Iteratively transform the IterativeDataSet</span></span><br><span class="line">DataSet&lt;Integer&gt; count = initial.closeWith(iteration);</span><br><span class="line"></span><br><span class="line">count.map(<span class="keyword">new</span> MapFunction&lt;Integer, Double&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Double <span class="title">map</span><span class="params">(Integer count)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> count / (<span class="keyword">double</span>) <span class="number">10000</span> * <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).print();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Iterative Pi Example"</span>);</span><br></pre></td></tr></table></figure>

<h2 id="Delta迭代"><a href="#Delta迭代" class="headerlink" title="Delta迭代"></a>Delta迭代</h2><p>Delta迭代利用了某些算法在每次迭代中不会更改解决方案的每个数据点的事实。</p>
<p>除了在每次迭代中反馈的部分解决方案（称为工作集）之外，delta迭代还在迭代中维护状态（称为解决方案集），可以通过增量更新。迭代计算的结果是最后一次迭代之后的状态。有关delta迭代的基本原理的概述，请参阅迭代简介。</p>
<p>定义DeltaIteration类似于定义BulkIteration。对于delta迭代，两个数据集构成每次迭代的输入（工作集和解决方案集），并且在每次迭代中生成两个数据集作为结果（新工作集，解决方案集delta）。</p>
<p>创建DeltaIteration调用iterateDelta(DataSet, int, int)（或iterateDelta(DataSet, int, int[])分别）。在初始解决方案集上调用此方法。参数是初始增量集，最大迭代次数和关键位置。返回的 DeltaIteration对象使您可以通过方法iteration.getWorkset()和方式访问表示工作集和解决方案集的DataSet iteration.getSolutionSet()。</p>
<p>下面是delta迭代语法的示例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// read the initial data sets</span></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; initialSolutionSet = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; initialDeltaSet = <span class="comment">// [...]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> maxIterations = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> keyPosition = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">DeltaIteration&lt;Tuple2&lt;Long, Double&gt;, Tuple2&lt;Long, Double&gt;&gt; iteration = initialSolutionSet</span><br><span class="line">    .iterateDelta(initialDeltaSet, maxIterations, keyPosition);</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; candidateUpdates = iteration.getWorkset()</span><br><span class="line">    .groupBy(<span class="number">1</span>)</span><br><span class="line">    .reduceGroup(<span class="keyword">new</span> ComputeCandidateChanges());</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; deltas = candidateUpdates</span><br><span class="line">    .join(iteration.getSolutionSet())</span><br><span class="line">    .where(<span class="number">0</span>)</span><br><span class="line">    .equalTo(<span class="number">0</span>)</span><br><span class="line">    .with(<span class="keyword">new</span> CompareChangesToCurrent());</span><br><span class="line"></span><br><span class="line">DataSet&lt;Tuple2&lt;Long, Double&gt;&gt; nextWorkset = deltas</span><br><span class="line">    .filter(<span class="keyword">new</span> FilterByThreshold());</span><br><span class="line"></span><br><span class="line">iteration.closeWith(deltas, nextWorkset)</span><br><span class="line">	.writeAsCsv(outputPath);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Stream Windows Join</title>
    <url>/2019/01/28/Flink%20Stream%20Windows%20Join/</url>
    <content><![CDATA[<h2 id="Windows-Join"><a href="#Windows-Join" class="headerlink" title="Windows Join"></a>Windows Join</h2><p>窗口连接连接两个共享公共密钥并位于同一窗口中的流的元素。可以使用窗口分配器定义这些窗口，并对来自两个流的元素进行评估。</p>
<p>然后将来自双方的元素传递给用户定义的，JoinFunction或者FlatJoinFunction用户可以发出满足连接条件的结果。</p>
<p>一般用法可概括如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.join(otherStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(&lt;WindowAssigner&gt;)</span><br><span class="line">    .apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure>
<p>关于语义的一些注释：</p>
<ul>
<li>两个流的元素的成对组合的创建表现得像内部连接，意味着如果它们没有来自要连接的另一个流的对应元素，则不会发出来自一个流的元素。</li>
<li>那些加入的元素将在其时间戳中包含仍位于相应窗口中的最大时间戳。例如，[5, 10)具有其边界的窗口将导致连接的元素具有9作为其时间戳。<h3 id="Tumbling-Window-Join"><a href="#Tumbling-Window-Join" class="headerlink" title="Tumbling Window Join"></a>Tumbling Window Join</h3>当执行翻滚窗口连接时，具有公共密钥和公共翻滚窗口的所有元素以成对组合的形式连接并传递给JoinFunction或FlatJoinFunction。因为它的行为类似于内连接，所以不会发出一个流的元素，这些元素在其翻滚窗口中没有来自另一个流的元素！<br><img data-src="https://ci.apache.org/projects/flink/flink-docs-master/fig/tumbling-window-join.svg" alt="images"><br>如图所示，我们定义了一个大小为2毫秒的翻滚窗口，这导致了窗体的窗口[0,1], [2,3], …。图像显示了每个窗口中所有元素的成对组合，这些元素将被传递给JoinFunction。请注意，在翻滚窗口中[6,7]没有任何东西被发射，因为绿色流中不存在与橙色元素⑥和⑦连接的元素。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">2</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">","</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h3 id="Sliding-Window-Join"><a href="#Sliding-Window-Join" class="headerlink" title="Sliding Window Join"></a>Sliding Window Join</h3>执行滑动窗口连接时，具有公共键和公共滑动窗口的所有元素都是成对组合并传递给JoinFunction或FlatJoinFunction。不会释放当前滑动窗口中没有来自其他流的元素的一个流的元素！请注意，某些元素可能在一个滑动窗口中连接而在另一个滑动窗口中不连<img data-src="https://ci.apache.org/projects/flink/flink-docs-master/fig/sliding-window-join.svg" alt="images"><br>在这个例子中，我们使用大小为2毫秒的滑动窗口并将它们滑动一毫秒，从而产生滑动窗口[-1, 0],[0,1],[1,2],[2,3], …。x轴下方的连接元素是传递给JoinFunction每个滑动窗口的元素。在这里，您还可以看到橙色②如何与窗口中的绿色③ [2,3]连接，但未与窗口中的任何内容连接[1,2]。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(SlidingEventTimeWindows.of(Time.milliseconds(<span class="number">2</span>) <span class="comment">/* size */</span>, Time.milliseconds(<span class="number">1</span>) <span class="comment">/* slide */</span>))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">","</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h3 id="Session-Window-Join"><a href="#Session-Window-Join" class="headerlink" title="Session Window Join"></a>Session Window Join</h3>在执行会话窗口连接时，具有相同键的所有元素在“组合”满足会话条件时以成对组合方式连接并传递给JoinFunction或FlatJoinFunction。再次执行内连接，因此如果有一个会话窗口只包含来自一个流的元素，则不会发出任何输出！<img data-src="https://ci.apache.org/projects/flink/flink-docs-master/fig/session-window-join.svg" alt="images"><br>这里我们定义一个会话窗口连接，其中每个会话除以至少1ms的间隙。有三个会话，在前两个会话中，两个流的连接元素都传递给JoinFunction。在第三阶段，绿色流中没有元素，所以⑧和⑨没有连接！<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.EventTimeSessionWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"> </span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream.join(greenStream)</span><br><span class="line">    .where(&lt;KeySelector&gt;)</span><br><span class="line">    .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withGap(Time.milliseconds(<span class="number">1</span>)))</span><br><span class="line">    .apply (<span class="keyword">new</span> JoinFunction&lt;Integer, Integer, String&gt; ()&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Integer first, Integer second)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> first + <span class="string">","</span> + second;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h3 id="Interval-Join"><a href="#Interval-Join" class="headerlink" title="Interval Join"></a>Interval Join</h3>区间连接使用公共密钥连接两个流的元素（我们现在将它们称为A和B），并且流B的元素具有时间戳，该时间戳位于流A中元素的时间戳的相对时间间隔中。这也可以更正式地表达为 b.timestamp ∈ [a.timestamp + lowerBound; a.timestamp + upperBound]或 a.timestamp + lowerBound &lt;= b.timestamp &lt;= a.timestamp + upperBound其中a和b是共享公共密钥的A和B的元素。只要下限总是小于或等于上限，下限和上限都可以是负数或上限。间隔连接当前仅执行内连接。当一对元素传递给ProcessJoinFunction它们时，它们将被赋予ProcessJoinFunction.Context两个元素的更大的时间戳（可以通过它访问）。注意间隔连接当前仅支持事件时间。<img data-src="https://ci.apache.org/projects/flink/flink-docs-master/fig/interval-join.svg" alt="images"><br>在上面的例子中，我们连接两个流’orange’和’green’，下限为-2毫秒，上限为+1毫秒。缺省情况下，这些界限是包容性的，但.lowerBoundExclusive()并.upperBoundExclusive可以应用到改变行为。再次使用更正式的表示法，这将转化为orangeElem.ts + lowerBound &lt;= greenElem.ts &lt;= orangeElem.ts + upperBound如三角形所示。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.co.ProcessJoinFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">DataStream&lt;Integer&gt; orangeStream = ...</span><br><span class="line">DataStream&lt;Integer&gt; greenStream = ...</span><br><span class="line"></span><br><span class="line">orangeStream</span><br><span class="line">    .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">    .intervalJoin(greenStream.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">    .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">    .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">            out.collect(first + <span class="string">","</span> + second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>HBase常用命令</title>
    <url>/2019/03/06/HBase%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="HBase命令："><a href="#HBase命令：" class="headerlink" title="HBase命令："></a>HBase命令：</h2><h4 id="删除-Ctrl-Back-Space"><a href="#删除-Ctrl-Back-Space" class="headerlink" title="删除 Ctrl+Back Space"></a>删除 Ctrl+Back Space</h4><h3 id="1-create-命令"><a href="#1-create-命令" class="headerlink" title="1). create 命令"></a>1). create 命令</h3><p>创建一个名为wang 具有c1，c2两个列族的表</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="built_in">create</span>  <span class="string">'wang'</span> ,<span class="string">'c1'</span>,<span class="string">'c2'</span></span><br></pre></td></tr></table></figure>

<h3 id="2-list-命令"><a href="#2-list-命令" class="headerlink" title="2). list 命令"></a>2). list 命令</h3><p>查看当前 HBase 中具有哪些表。</p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">hbase</span>(main)<span class="selector-pseudo">:012</span><span class="selector-pseudo">:0</span>&gt; <span class="selector-tag">list</span></span><br></pre></td></tr></table></figure>

<h3 id="3-describe-命令"><a href="#3-describe-命令" class="headerlink" title="3). describe 命令"></a>3). describe 命令</h3><p>查看表“wang”的构造。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; describe <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line">&#123;NAME =&gt; <span class="string">'size_v1.0'</span>, <span class="comment">//列族</span></span><br><span class="line">BLOOMFILTER =&gt; <span class="string">'NONE'</span>, <span class="comment">//布隆过滤器</span></span><br><span class="line">VERSIONS =&gt; <span class="string">'3'</span>,<span class="comment">//设置保存的版本数</span></span><br><span class="line">IN_MEMORY =&gt;<span class="string">'false'</span>,<span class="comment">//设置激进缓存，优先考虑将该列族放入块缓存中，</span></span><br><span class="line"> <span class="comment">//针对随机读操作相对较多的列族可以设置该属性为true</span></span><br><span class="line">KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>,<span class="comment">//参见：http://hbase.apache.org/book.html#cf.keep.deleted</span></span><br><span class="line">DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>,<span class="comment">//数据块编码方式设置</span></span><br><span class="line">COMPRESSION =&gt; <span class="string">'NONE'</span>,<span class="comment">//设置压缩算法</span></span><br><span class="line">TTL =&gt; <span class="string">'FOREVER'</span>, <span class="comment">//参见：http://hbase.apache.org/book.html#ttl</span></span><br><span class="line">MIN_VERSIONS =&gt; <span class="string">'0'</span>,<span class="comment">//最小存储版本数</span></span><br><span class="line">BLOCKCACHE =&gt; <span class="string">'false'</span>, <span class="comment">//数据块缓存属性</span></span><br><span class="line">BLOCKSIZE =&gt; <span class="string">'65536'</span>, <span class="comment">//设置HFile数据块大小（默认64kb）</span></span><br><span class="line">REPLICATION_SCOPE =&gt; <span class="string">'0'</span><span class="comment">//配置HBase集群replication时需要将该参数设置为1.</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-put-命令"><a href="#4-put-命令" class="headerlink" title="4). put 命令"></a>4). put 命令</h3><p>使用 put 命令向表中插入数据,参数分别为表名、行名、列名和值,其中列名前需要列族最为前缀,时间戳由系统自动生成。<br>格式: put 表名,行名,列名([列族:列名]),值<br>例子：</p>
<h5 id="a-加入一行数据-行名称为“xiapi”-列族“c1”的列名为”-空字符串-”-值位-1。"><a href="#a-加入一行数据-行名称为“xiapi”-列族“c1”的列名为”-空字符串-”-值位-1。" class="headerlink" title="a. 加入一行数据,行名称为“xiapi”,列族“c1”的列名为”(空字符串)”,值位 1。"></a>a. 加入一行数据,行名称为“xiapi”,列族“c1”的列名为”(空字符串)”,值位 1。</h5><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; put <span class="string">'wang'</span>, <span class="string">'xiapi'</span>, <span class="string">'c1:'</span>, <span class="string">'1'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; put <span class="string">'wang'</span>, <span class="string">'xiapi'</span>, <span class="string">'c1:'</span>, <span class="string">'2'</span></span><br></pre></td></tr></table></figure>
<p> –修改操作(update)</p>
<h5 id="b-给“xiapi”这一行的数据的列族“c2”添加一列“-lt-china-97-gt-”。"><a href="#b-给“xiapi”这一行的数据的列族“c2”添加一列“-lt-china-97-gt-”。" class="headerlink" title="b. 给“xiapi”这一行的数据的列族“c2”添加一列“&lt;china,97&gt;”。"></a>b. 给“xiapi”这一行的数据的列族“c2”添加一列“&lt;china,97&gt;”。</h5><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; put <span class="string">'wang'</span>, <span class="string">'xiapi'</span>,  <span class="string">'c2:china'</span>, <span class="string">'97'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; put <span class="string">'wang'</span>, <span class="string">'xiapi'</span>,  <span class="string">'c2:math'</span>, <span class="string">'128'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; put <span class="string">'wang'</span>, <span class="string">'xiapi'</span>,  <span class="string">'c2:english'</span>, <span class="string">'85'</span></span><br></pre></td></tr></table></figure>

<h3 id="5-get-命令"><a href="#5-get-命令" class="headerlink" title="5). get 命令"></a>5). get 命令</h3><h5 id="a-查看表“wang”中的行“xiapi”的相关数据。"><a href="#a-查看表“wang”中的行“xiapi”的相关数据。" class="headerlink" title="a.查看表“wang”中的行“xiapi”的相关数据。"></a>a.查看表“wang”中的行“xiapi”的相关数据。</h5><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; get <span class="string">'wang'</span>, <span class="string">'xiapi'</span></span><br></pre></td></tr></table></figure>

<h5 id="b-查看表“wang”中行“xiapi”列“c2-math”的值。"><a href="#b-查看表“wang”中行“xiapi”列“c2-math”的值。" class="headerlink" title="b.查看表“wang”中行“xiapi”列“c2 :math”的值。"></a>b.查看表“wang”中行“xiapi”列“c2 :math”的值。</h5><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; get <span class="string">'wang'</span>, <span class="string">'xiapi'</span>, <span class="string">'c2:math'</span></span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; <span class="builtin-name">get</span> <span class="string">'wang'</span>, <span class="string">'xiapi'</span>, &#123;<span class="attribute">COLUMN</span>=&gt;'c2:math'&#125;</span><br><span class="line">hbase(main):012:0&gt; <span class="builtin-name">get</span> <span class="string">'wang'</span>, <span class="string">'xiapi'</span>, &#123;<span class="attribute">COLUMNS</span>=&gt;'c2:math'&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>备注:COLUMN 和 COLUMNS 是不同的,scan 操作中的 COLUMNS 指定的是表的列族, get操作中的 COLUMN 指定的是特定的列,COLUMNS 的值实质上为“列族:列修饰符”。COLUMN 和 COLUMNS 必须为大写。</p>
</blockquote>
<h3 id="6-scan-命令"><a href="#6-scan-命令" class="headerlink" title="6). scan 命令"></a>6). scan 命令</h3><h5 id="a-查看表“scores”中的所有数据。"><a href="#a-查看表“scores”中的所有数据。" class="headerlink" title="a. 查看表“scores”中的所有数据。"></a>a. 查看表“scores”中的所有数据。</h5><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; scan <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意:<br>scan 命令可以指定 startrow,stoprow 来 scan 多个 row。<br>例如:</p>
</blockquote>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">scan <span class="string">'wang'</span> ,&#123;<span class="attribute">LIMIT</span>=&gt;1&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight typescript"><table><tr><td class="code"><pre><span class="line">scan <span class="string">'wang'</span>,&#123;<span class="function"><span class="params">COLUMNS</span> =&gt;</span><span class="string">'c2:sb'</span>,<span class="function"><span class="params">LIMIT</span> =&gt;</span><span class="number">10</span>, <span class="function"><span class="params">STARTROW</span> =&gt;</span> <span class="string">'1'</span>, <span class="function"><span class="params">STOPROW</span>=&gt;</span><span class="string">'3'</span>&#125;</span><br></pre></td></tr></table></figure>
<h5 id="b-查看表wang”中列族“c2”的所有数据。"><a href="#b-查看表wang”中列族“c2”的所有数据。" class="headerlink" title="b.查看表wang”中列族“c2”的所有数据。"></a>b.查看表wang”中列族“c2”的所有数据。</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; scan  'wang', &#123;COLUMN =&gt; 'c1'&#125;</span><br><span class="line">hbase(main):012:0&gt; scan  'wang', &#123;COLUMN=&gt;'c2:math'&#125;</span><br><span class="line">hbase(main):012:0&gt; scan  'wang', &#123;COLUMNS =&gt; 'c2'&#125;</span><br><span class="line">hbase(main):012:0&gt; scan  'wang', &#123;COLUMNS =&gt; 'c2'&#125;</span><br></pre></td></tr></table></figure>

<h3 id="7-count-命令-统计row个数"><a href="#7-count-命令-统计row个数" class="headerlink" title="7). count 命令(统计row个数)"></a>7). count 命令(统计row个数)</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">068</span>:<span class="number">0</span>&gt; count <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<h3 id="8-exists-命令（查看表是否存在）"><a href="#8-exists-命令（查看表是否存在）" class="headerlink" title="8). exists 命令（查看表是否存在）"></a>8). exists 命令（查看表是否存在）</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">071</span>:<span class="number">0</span>&gt; exists <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<h3 id="9-incr-命令-赋值"><a href="#9-incr-命令-赋值" class="headerlink" title="9). incr 命令(赋值)"></a>9). incr 命令(赋值)</h3><h3 id="10-delete-命令"><a href="#10-delete-命令" class="headerlink" title="10). delete 命令"></a>10). delete 命令</h3><p>删除表“wang”中行为“db”, 列族“c2”中的“757”。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt;  delete <span class="string">'wang'</span>, <span class="string">'db'</span>, <span class="string">'c2:757'</span></span><br></pre></td></tr></table></figure>

<p>删除整行 </p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">024</span>:<span class="number">0</span>&gt; deleteall <span class="string">'wang'</span>,<span class="string">'3'</span></span><br></pre></td></tr></table></figure>

<h3 id="11-truncate-命令（将整张表清空）"><a href="#11-truncate-命令（将整张表清空）" class="headerlink" title="11). truncate 命令（将整张表清空）"></a>11). truncate 命令（将整张表清空）</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt;  truncate <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<h3 id="12-disbale、drop-命令"><a href="#12-disbale、drop-命令" class="headerlink" title="12). disbale、drop 命令"></a>12). disbale、drop 命令</h3><p>通过“disable”和“drop”命令删除“wang”表。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt;  disable <span class="string">'wang'</span> --enable <span class="string">'scores'</span> </span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt;  drop <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>
<p>删除一个列簇            </p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">007</span>:<span class="number">0</span>&gt; disable <span class="string">'wang'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">009</span>:<span class="number">0</span>&gt; alter <span class="string">'wang'</span>,&#123;NAME=&gt;<span class="string">'db'</span>, METHOD=&gt;<span class="string">'delete'</span>&#125;</span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">010</span>:<span class="number">0</span>&gt; enable <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>

<h3 id="13-status命令（查询服务器状态）"><a href="#13-status命令（查询服务器状态）" class="headerlink" title="13).  status命令（查询服务器状态）"></a>13).  status命令（查询服务器状态）</h3><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">hbase</span>(main)<span class="selector-pseudo">:072</span><span class="selector-pseudo">:0</span>&gt; <span class="selector-tag">status</span></span><br></pre></td></tr></table></figure>

<h3 id="14-version命令（查询hbase版本）"><a href="#14-version命令（查询hbase版本）" class="headerlink" title="14).  version命令（查询hbase版本）"></a>14).  version命令（查询hbase版本）</h3><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">hbase</span>(main)<span class="selector-pseudo">:073</span><span class="selector-pseudo">:0</span>&gt; <span class="selector-tag">version</span></span><br></pre></td></tr></table></figure>
<h3 id="15-count"><a href="#15-count" class="headerlink" title="15) count"></a>15) count</h3><p>统计表的行数</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">073</span>:<span class="number">0</span>&gt; count <span class="string">'tablename'</span></span><br></pre></td></tr></table></figure>
<h3 id="15-alter"><a href="#15-alter" class="headerlink" title="15) alter"></a>15) alter</h3><p>修改列族的最大版本数</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hbase&gt; alte<span class="string">r't1'</span>，NAME =&gt;<span class="string">'f1'</span>，VERSIONS =&gt; <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>修改列族的最小版本数</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hbase&gt; alte<span class="string">r't1'</span>，NAME =&gt;<span class="string">'f1'</span>，MIN_VERSIONS =&gt; <span class="number">2</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>hbase shell</tag>
      </tags>
  </entry>
  <entry>
    <title>hive lock</title>
    <url>/2019/04/10/Hive%20Lock/</url>
    <content><![CDATA[<h1 id="Hive-Lock"><a href="#Hive-Lock" class="headerlink" title="Hive Lock"></a>Hive Lock</h1><p>hive 目前主要有两种锁，SHARED（共享锁 S）和 Exclusive（排他锁 X）。共享锁 S 和  排他锁 X </p>
<ul>
<li>1）查询操作使用共享锁，共享锁是可以多重、并发使用的</li>
<li>2）修改表操作使用独占锁，它会阻止其他的查询、修改操作</li>
<li>3）可以对分区使用锁。<br>以下情况会触发锁，以及它的类型和锁定范围如下：</li>
</ul>
<table>
<thead>
<tr>
<th>Hive Command</th>
<th>Locks Acquired</th>
</tr>
</thead>
<tbody><tr>
<td>select .. T1 partition P1</td>
<td>S on T1, T1.P1</td>
</tr>
<tr>
<td>insert into T2(partition P2) select .. T1 partition P1</td>
<td>S on T2, T1, T1.P1 and X on T2.P2</td>
</tr>
<tr>
<td>insert into T2(partition P.Q) select .. T1 partition P1</td>
<td>S on T2, T2.P, T1, T1.P1 and X on T2.P.Q</td>
</tr>
<tr>
<td>alter table T1 rename T2</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 add cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 replace cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 change cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 concatenate</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 add partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 drop partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 touch partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 set serdeproperties</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set serializer</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set file format</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set tblproperties</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 partition P1 concatenate</td>
<td>X on T1.P1</td>
</tr>
<tr>
<td>drop table T1</td>
<td>X on T1</td>
</tr>
<tr>
<td>## hive关于锁的配置</td>
<td></td>
</tr>
<tr>
<td>* hive.support.concurrency</td>
<td></td>
</tr>
<tr>
<td>默认值： false</td>
<td></td>
</tr>
<tr>
<td>desc: Hive是否支持并发。一个ZooKeeper的实例必须启动并运行了默认配置单元锁管理器，支持读写锁。</td>
<td></td>
</tr>
<tr>
<td>设置为  true 支持  INSERT … VALUES，UPDATE和DELETE  事务（Hive 0.14.0及更高版本）。有关打开Hive事务所需参数的完整列表，请参阅  hive.txn.manager。</td>
<td></td>
</tr>
<tr>
<td>* hive.lock.manager</td>
<td></td>
</tr>
<tr>
<td>默认值： org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager</td>
<td></td>
</tr>
<tr>
<td>* hive.lock.mapred.only.operation</td>
<td></td>
</tr>
<tr>
<td>默认值： false</td>
<td></td>
</tr>
<tr>
<td>此配置属性用于控制是否仅锁定需要执行至少一个mapred作业的查询。</td>
<td></td>
</tr>
<tr>
<td>* hive.lock.query.string.max.length</td>
<td></td>
</tr>
<tr>
<td>默认值：  1000000</td>
<td></td>
</tr>
<tr>
<td>添加In：Hive 3.0.0</td>
<td></td>
</tr>
<tr>
<td>要存储在锁中的查询字符串的最大长度。默认值为1000000，因为znode的数据限制为1MB</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li><p>hive.lock.numretries<br>默认值： 100<br>您想要尝试获取所有锁的总次数。</p>
</li>
<li><p>hive.unlock.numretries<br>默认值： 10<br>您要解锁的总次数。</p>
</li>
<li><p>hive.lock.sleep.between.retries<br>默认值： 60<br>各种重试之间的休眠时间（以秒为单位）。</p>
</li>
<li><p>hive.zookeeper.quorum<br>默认值:(空）<br>要与之交谈的ZooKeeper服务器列表。只有读/写锁才需要这样做。</p>
</li>
<li><p>hive.zookeeper.client.port<br>默认值：2181<br>要与之交谈的ZooKeeper服务器端口。只有读/写锁才需要这样做。</p>
</li>
<li><p>hive.zookeeper.session.timeout<br>默认值：<br>Hive 1.2.0及更高版本： 1200000ms（HIVE-8890）<br>ZooKeeper客户端的会话超时（以毫秒为单位）。客户端断开连接，因此，如果超时中未发送心跳，则释放所有锁定。</p>
</li>
<li><p>hive.zookeeper.namespace<br>默认值： hive_zookeeper_namespace<br>创建所有ZooKeeper节点的父节点。</p>
</li>
<li><p>hive.zookeeper.clean.extra.nodes<br>默认值： false<br>在会话结束时清理额外的节点。</p>
</li>
<li><p>hive.lockmgr.zookeeper.default.partition.name<br>默认值： <strong>HIVE_DEFAULT_ZOOKEEPER_PARTITION</strong><br>ZooKeeperHiveLockManager是配置单元锁定管理器时的默认分区名称  。</p>
<h2 id="hive-commond"><a href="#hive-commond" class="headerlink" title="hive commond"></a>hive commond</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1）锁表</span></span><br><span class="line">hive&gt; lock table t1 exclusive;</span><br><span class="line"><span class="comment">-- 表被独占锁之后，将不能执行查询操作：</span></span><br><span class="line">hive&gt; SELECT COUNT(*) FROM t1;</span><br><span class="line">conflicting <span class="keyword">lock</span> <span class="keyword">present</span> <span class="keyword">for</span> <span class="keyword">default</span>@people <span class="keyword">mode</span> <span class="keyword">SHARED</span></span><br><span class="line"><span class="keyword">FAILED</span>: <span class="keyword">Error</span> <span class="keyword">in</span> acquiring locks: locks <span class="keyword">on</span> the underlying objects</span><br><span class="line">cannot be acquired. retry <span class="keyword">after</span> <span class="keyword">some</span> <span class="built_in">time</span></span><br><span class="line"><span class="comment">-- 2）解除锁</span></span><br><span class="line">hive&gt; <span class="keyword">unlock</span> <span class="keyword">table</span> t1;</span><br></pre></td></tr></table></figure>
<p>查询是那个SQL锁Hive表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> locks hdp_ubu_zhuanzhuan_defaultdb.token <span class="keyword">extended</span></span><br></pre></td></tr></table></figure></li>
<li><p>查看有锁的表</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> locks;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive注释中文乱码</title>
    <url>/2019/01/31/Hive%E6%B3%A8%E9%87%8A%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</url>
    <content><![CDATA[<h4 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> movie(</span><br><span class="line">userID <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'用户ID'</span>,</span><br><span class="line">movieID <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'电影ID'</span>,</span><br><span class="line">rating <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'电影评分'</span>,</span><br><span class="line">timestamped <span class="built_in">bigint</span> <span class="keyword">comment</span> <span class="string">'评分时间戳'</span>,</span><br><span class="line">movieName <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'电影名字'</span>, </span><br><span class="line">movieType <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'电影类型'</span>, </span><br><span class="line">sex <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'性别'</span>, </span><br><span class="line">age <span class="built_in">int</span> <span class="keyword">comment</span> <span class="string">'年龄'</span>, </span><br><span class="line">occupation <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'职业'</span>, </span><br><span class="line">zipcode <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'邮政编码'</span></span><br><span class="line">) <span class="keyword">comment</span> <span class="string">'影评三表合一'</span> </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">","</span></span><br><span class="line">location <span class="string">'/hive/movie'</span>;</span><br></pre></td></tr></table></figure>
<p>这是因为在hive的元数据库中MySQL中的元数据出现乱码</p>
<ul>
<li><p>那么我们只需要把相应注释的地方的字符集由 latin1 改成 utf-8，就可以了。用到注释的就三个地方，表、分区、视图。如下修改分为两个步骤：</p>
<h5 id="1、进入数据库-Metastore-中执行以下-5-条-SQL-语句"><a href="#1、进入数据库-Metastore-中执行以下-5-条-SQL-语句" class="headerlink" title="1、进入数据库 Metastore 中执行以下 5 条 SQL 语句"></a>1、进入数据库 Metastore 中执行以下 5 条 SQL 语句</h5></li>
<li><p>（1）修改表字段注解和表注解</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 <span class="keyword">modify</span> <span class="keyword">column</span> <span class="keyword">COMMENT</span> <span class="built_in">varchar</span>(<span class="number">256</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>

<ul>
<li>（2）修改分区字段注解</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8 ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS <span class="keyword">modify</span> <span class="keyword">column</span> PKEY_COMMENT <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>

<ul>
<li>（3）修改索引注解</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS <span class="keyword">modify</span> <span class="keyword">column</span> PARAM_VALUE <span class="built_in">varchar</span>(<span class="number">4000</span>) <span class="built_in">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>
<h4 id="2、修改-metastore-的连接-URL"><a href="#2、修改-metastore-的连接-URL" class="headerlink" title="2、修改 metastore 的连接 URL"></a>2、修改 metastore 的连接 URL</h4><p> 修改hive-site.xml配置文件</p>
<p> CDH中修改路径</p>
<p> hive-&gt;配置-&gt;类别-&gt;高级-&gt;hive-site.xml 的 Hive 复制高级配置代码段（安全阀）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://IP:3306/db_name?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useUnicode=true&amp;characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>重启一下hive</p>
<h5 id="查看表结构"><a href="#查看表结构" class="headerlink" title="查看表结构"></a>查看表结构</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span>  movie</span><br></pre></td></tr></table></figure>

<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">1</span>	userID	<span class="built_in">int</span>	客户ID</span><br><span class="line"><span class="number">2</span>	movieID	<span class="built_in">int</span>	用户ID</span><br><span class="line"><span class="number">3</span>	rating	<span class="built_in">int</span>	电影评分</span><br><span class="line"><span class="number">4</span>	timestamped	bigint	评分时间戳</span><br><span class="line"><span class="number">5</span>	movieName	<span class="built_in">string</span>	电影名字</span><br><span class="line"><span class="number">6</span>	movieType	<span class="built_in">string</span>	电影类型</span><br><span class="line"><span class="number">7</span>	sex	<span class="built_in">string</span>	性别</span><br><span class="line"><span class="number">8</span>	age	<span class="built_in">int</span>	年龄</span><br><span class="line"><span class="number">9</span>	occupation	<span class="built_in">string</span>	职业</span><br><span class="line"><span class="number">10</span>	zipcode	<span class="built_in">string</span>	邮政编码</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive 乱码</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive UDF 两种API</title>
    <url>/2019/01/30/Hive%20UDF%20%E4%B8%A4%E7%A7%8DAPI/</url>
    <content><![CDATA[<h5 id="编写Apache-Hive用户自定义函数（UDF）有两个不同的接口"><a href="#编写Apache-Hive用户自定义函数（UDF）有两个不同的接口" class="headerlink" title="编写Apache Hive用户自定义函数（UDF）有两个不同的接口"></a>编写Apache Hive用户自定义函数（UDF）有两个不同的接口</h5><ul>
<li><p>简单API：org.apache.hadoop.hive.ql.exec.UDF</p>
</li>
<li><p>复杂API：  org.apache.hadoop.hive.ql.udf.generic.GenericUDF</p>
</li>
<li><p>如果你的函数读和返回都是基础数据类型（Hadoop&amp;Hive 基本writable类型，如Text,IntWritable,LongWriable,DoubleWritable等等），那么简单的API（org.apache.hadoop.hive.ql.exec.UDF）可以胜任</p>
</li>
<li><p>但是，如果你想写一个UDF用来操作内嵌数据结构，如Map，List和Set，那么你要去熟悉org.apache.hadoop.hive.ql.udf.generic.GenericUDF这个API</p>
<h2 id="简单API"><a href="#简单API" class="headerlink" title="简单API"></a>简单API</h2><p>用简单UDF API来构建一个UDF只涉及到编写一个类继承实现一个方法（evaluate），以下是示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleUDFExample</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;  </span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text input)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(input == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(<span class="string">"Hello "</span> + input.toString());  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后加了一个测试去验证它</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span>  </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUDFNullCheck</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">  SimpleUDFExample example = <span class="keyword">new</span> SimpleUDFExample();  </span><br><span class="line">  Assert.assertNull(example.evaluate(<span class="keyword">null</span>));  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="复杂的API"><a href="#复杂的API" class="headerlink" title="复杂的API"></a>复杂的API</h2><p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF API提供了一种方法去处理那些不是可写类型的对象，例如：struct，map和array类型。</p>
<p>这个API需要你亲自去为函数的参数去管理对象存储格式（object inspectors），验证接收的参数的数量与类型。一个object inspector为内在的数据类型提供一个一致性接口，以至不同实现的对象可以在hive中以一致的方式去访问（例如，只要你能提供一个对应的object inspector，你可以实现一个如Map的复合对象）。</p>
<p>这个API要求你去实现以下方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这个类似于简单API的evaluat方法，它可以读取输入数据和返回结果  </span></span><br><span class="line"><span class="function"><span class="keyword">abstract</span> Object <span class="title">evaluate</span><span class="params">(GenericUDF.DeferredObject[] arguments)</span></span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// 该方法无关紧要，我们可以返回任何东西，但应当是描述该方法的字符串  </span></span><br><span class="line"><span class="function"><span class="keyword">abstract</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span></span>;  </span><br><span class="line">  </span><br><span class="line"><span class="comment">// 只调用一次，在任何evaluate()调用之前，你可以接收到一个可以表示函数输入参数类型的object inspectors数组  </span></span><br><span class="line"><span class="comment">// 这是你用来验证该函数是否接收正确的参数类型和参数个数的地方  </span></span><br><span class="line"><span class="function"><span class="keyword">abstract</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span></span>;</span><br></pre></td></tr></table></figure>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>我将通过建立一个UDF函数：containsString，来加深对该API了解，该函数接收两个参数：</p>
<p>一个String的列表（list）</p>
<p>一个String</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">containsString(List(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>), <span class="string">"b"</span>); <span class="comment">// true  </span></span><br><span class="line">containsString(List(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>), <span class="string">"d"</span>); <span class="comment">// false</span></span><br></pre></td></tr></table></figure>
<p>不同于UDF接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ComplexUDFExample</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">  ListObjectInspector listOI;  </span><br><span class="line">  StringObjectInspector elementOI;  </span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span>  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] arg0)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="string">"arrayContainsExample()"</span>; <span class="comment">// this should probably be better  </span></span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span>  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> (arguments.length != <span class="number">2</span>) &#123;  </span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">"arrayContainsExample only takes 2 arguments: List&lt;T&gt;, T"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">// 1. 检查是否接收到正确的参数类型  </span></span><br><span class="line">    ObjectInspector a = arguments[<span class="number">0</span>];  </span><br><span class="line">    ObjectInspector b = arguments[<span class="number">1</span>];  </span><br><span class="line">    <span class="keyword">if</span> (!(a <span class="keyword">instanceof</span> ListObjectInspector) || !(b <span class="keyword">instanceof</span> StringObjectInspector)) &#123;  </span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentException(<span class="string">"first argument must be a list / array, second argument must be a string"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">this</span>.listOI = (ListObjectInspector) a;  </span><br><span class="line">    <span class="keyword">this</span>.elementOI = (StringObjectInspector) b;  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 2. 检查list是否包含的元素都是string  </span></span><br><span class="line">    <span class="keyword">if</span>(!(listOI.getListElementObjectInspector() <span class="keyword">instanceof</span> StringObjectInspector)) &#123;  </span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentException(<span class="string">"first argument must be a list of strings"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 返回类型是boolean，所以我们提供了正确的object inspector  </span></span><br><span class="line">    <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaBooleanObjectInspector;  </span><br><span class="line">  &#125;  </span><br><span class="line">    </span><br><span class="line">  <span class="meta">@Override</span>  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 利用object inspectors从传递的对象中得到list与string  </span></span><br><span class="line">    List&lt;String&gt; list = (List&lt;String&gt;) <span class="keyword">this</span>.listOI.getList(arguments[<span class="number">0</span>].get());  </span><br><span class="line">    String arg = elementOI.getPrimitiveJavaObject(arguments[<span class="number">1</span>].get());  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 检查空值  </span></span><br><span class="line">    <span class="keyword">if</span> (list == <span class="keyword">null</span> || arg == <span class="keyword">null</span>) &#123;  </span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 判断是否list中包含目标值  </span></span><br><span class="line">    <span class="keyword">for</span>(String s: list) &#123;  </span><br><span class="line">      <span class="keyword">if</span> (arg.equals(s)) <span class="keyword">return</span> <span class="keyword">new</span> Boolean(<span class="keyword">true</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Boolean(<span class="keyword">false</span>);  </span><br><span class="line">  &#125;  </span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码走读<br>函数的调用模块如下：</p>
<p>1、该UDF用默认的构造器来初始化</p>
<p>2、udf.initialize() 被调用，传人udf参数的object instructors数组，（ListObjectInstructor, StringObjectInstructor）</p>
<p>1) 检查传人的参数有两个与该参数的数据类型是正确的（见上面）<br>2) 我们保存object instructors用以供evaluate()使用（listOI, elementOI）<br>3) 返回 object inspector，让Hive能够读取该函数的返回结果（BooleanObjectInspector）</p>
<p>3、对于查询中的每一行，evaluate方法都会被调用，传人该行的指定的列（例如，evaluate(List(“a”, “b”, “c”), “c”) ）。</p>
<p>1) 我们利用initialize方法中存储的object instructors来抽取出正确的值。<br>2) 我们在这处理我们的逻辑然后用initialize返回的object inspector来序列化返回来的值(list.contains(elemement) ? true : false)。</p>
<h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ComplexUDFExampleTest</span> </span>&#123;  </span><br><span class="line">    </span><br><span class="line">  <span class="meta">@Test</span>  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testComplexUDFReturnsCorrectValues</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 建立需要的模型  </span></span><br><span class="line">    ComplexUDFExample example = <span class="keyword">new</span> ComplexUDFExample();  </span><br><span class="line">    ObjectInspector stringOI = PrimitiveObjectInspectorFactory.javaStringObjectInspector;  </span><br><span class="line">    ObjectInspector listOI = ObjectInspectorFactory.getStandardListObjectInspector(stringOI);  </span><br><span class="line">    JavaBooleanObjectInspector resultInspector = (JavaBooleanObjectInspector) example.initialize(<span class="keyword">new</span> ObjectInspector[]&#123;listOI, stringOI&#125;);  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// create the actual UDF arguments  </span></span><br><span class="line">    List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();  </span><br><span class="line">    list.add(<span class="string">"a"</span>);  </span><br><span class="line">    list.add(<span class="string">"b"</span>);  </span><br><span class="line">    list.add(<span class="string">"c"</span>);  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 测试结果  </span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 存在的值  </span></span><br><span class="line">    Object result = example.evaluate(<span class="keyword">new</span> DeferredObject[]&#123;<span class="keyword">new</span> DeferredJavaObject(list), <span class="keyword">new</span> DeferredJavaObject(<span class="string">"a"</span>)&#125;);  </span><br><span class="line">    Assert.assertEquals(<span class="keyword">true</span>, resultInspector.get(result));  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 不存在的值  </span></span><br><span class="line">    Object result2 = example.evaluate(<span class="keyword">new</span> DeferredObject[]&#123;<span class="keyword">new</span> DeferredJavaObject(list), <span class="keyword">new</span> DeferredJavaObject(<span class="string">"d"</span>)&#125;);  </span><br><span class="line">    Assert.assertEquals(<span class="keyword">false</span>, resultInspector.get(result2));  </span><br><span class="line">      </span><br><span class="line">    <span class="comment">// 为null的参数  </span></span><br><span class="line">    Object result3 = example.evaluate(<span class="keyword">new</span> DeferredObject[]&#123;<span class="keyword">new</span> DeferredJavaObject(<span class="keyword">null</span>), <span class="keyword">new</span> DeferredJavaObject(<span class="keyword">null</span>)&#125;);  </span><br><span class="line">    Assert.assertNull(result3);  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>Streamsets Mongo to Kudu</title>
    <url>/2019/08/08/StreamSets%20Mongo%20to%20kudu/</url>
    <content><![CDATA[<p>本次采用Streamsets作为数据管道，将Mongo数据同步至kudu</p>
<p><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/vJAit.3xhuTQBg30jh7JIUhKqOr41UAJIJfMHYGQuCo!/b/dMMAAAAAAAAA&bo=6AQeAQAAAAARB8M!&rf=viewer_4" alt="image"></p>
<h1 id="一-数据起点"><a href="#一-数据起点" class="headerlink" title="一.数据起点"></a>一.数据起点</h1><p><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/gjQw2cp6byGRIczq3KWcqQ8q2D0ecOnb5d5GSNGUwus!/b/dL8AAAAAAAAA&bo=twACAQAAAAARB4Q!&rf=viewer_4" alt="image"></p>
<p>从图中我们可以看到关于Mongo的有两个数据起点</p>
<h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><p>MongoDB源从MongoDB读取数据.MongoDB源自上限和无上限集合</p>
<h2 id="MongoDBOplog"><a href="#MongoDBOplog" class="headerlink" title="MongoDBOplog"></a>MongoDBOplog</h2><p>MongoDB Oplog源从MongoDB Oplog读取条目,<br>MongoDB将有关数据库更改的信息存储在名为Oplog的本地上限集合中。Oplog包含有关数据更改以及数据库更改的信息。MongoDB Oplog源可以读取写入Oplog的任何操作。</p>
<p>使用MongoDB Oplog源来捕获数据或数据库中的更改。要处理写入标准上限或无上限集合的MongoDB数据，请使用MongoDB源</p>
<p>MongoDB Oplog源包含记录头属性中的CRUD操作类型，因此生成的记录可以由启用CRUD的目标轻松处理</p>
<p>本次要抽取的两个集合为非上限集合，因为存取的为日志数据无需更新与删除操作故采用MongoDB数据源</p>
<h3 id="MongoDB连接"><a href="#MongoDB连接" class="headerlink" title="MongoDB连接"></a>MongoDB连接</h3><p><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/o5RyV9Npq9GQDkB3oWa3kicr.praoylMhRdnxQC0Ybw!/b/dLgAAAAAAAAA&bo=aAXtAgAAAAARB7I!&rf=viewer_4" alt="image"></p>
<ul>
<li>单节点连接<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">mongodb://username:password@host[<span class="string">:port</span>][<span class="symbol">/[database</span>][<span class="string">?options</span>]]</span><br></pre></td></tr></table></figure></li>
<li>多节点连接</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">mongodb://host1[<span class="string">:port1</span>][<span class="symbol">,host2[:port2</span>],...[<span class="string">,hostN[:portN</span>]]][<span class="string">/[database</span>][<span class="symbol">?options</span>]]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>详情参阅:<a href="https://streamsets.com/documentation/controlhub/latest/onpremhelp/datacollector/UserGuide/Origins/MongoDB.html#concept_bk4_2rs_ns" target="_blank" rel="noopener">MongoDB 源</a></p>
</blockquote>
<h1 id="二-数据转换"><a href="#二-数据转换" class="headerlink" title="二.数据转换"></a>二.数据转换</h1><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="KafkaProducer"><a href="#KafkaProducer" class="headerlink" title="KafkaProducer"></a>KafkaProducer</h3><ul>
<li>kafka<br><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/sXpFjBnfcISqZj6S1hEUby2M54aLECrUlwvgt1RlIBQ!/b/dMUAAAAAAAAA&bo=KwW5AQAAAAARB6Y!&rf=viewer_4" alt="image"><br>如图所示<h3 id="KafkaConsumer"><a href="#KafkaConsumer" class="headerlink" title="KafkaConsumer"></a>KafkaConsumer</h3></li>
<li>kafka<br><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/8s1om94YpWGJGQRNP26Fedwga7aNQX2oBCFg*qpLEMs!/b/dFIBAAAAAAAA&bo=*gSsAgAAAAARB2Q!&rf=viewer_4" alt="image"></li>
<li>Dataformat<br><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/wpiOGabHgTfYrSYdb.Onsh50OUxZQSDcBbGoVmBOfZQ!/b/dL8AAAAAAAAA&bo=.gRJAQAAAAARB4Y!&rf=viewer_4" alt="image"><h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2></li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"_id"</span> : <span class="string">"5c89b9ee2c9d450014bd5aa8"</span>,</span><br><span class="line">    <span class="attr">"_class"</span> : <span class="string">"com.belle.petrel.entity.BehaviourRecord"</span>,</span><br><span class="line">    <span class="attr">"loginName"</span> : <span class="string">"admin"</span>,</span><br><span class="line">    <span class="attr">"userName"</span> : <span class="string">"超级管理员"</span>,</span><br><span class="line">    <span class="attr">"requestURI"</span> : <span class="string">"GET/petrel/petrel-itg-api/sysOrganization/byParams"</span>,</span><br><span class="line">    <span class="attr">"queryParams"</span> : <span class="string">"&#123;\"resCode\":[\"2\"],\"parentId\":[\"0\"]&#125;"</span>,</span><br><span class="line">    <span class="attr">"remoteAddr"</span> : <span class="string">"172.17.225.164"</span>,</span><br><span class="line">    <span class="attr">"userAgent"</span> : <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36"</span>,</span><br><span class="line">    <span class="attr">"operateTime"</span> : <span class="number">1552526382287</span>,</span><br><span class="line">    <span class="attr">"flag"</span> : <span class="string">"1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我采用的是Jython</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> records:</span><br><span class="line">  operate_smonth = time.strftime(<span class="string">"%Y-%m"</span>, time.localtime(record.value[<span class="string">'operateTime'</span>]/<span class="number">1000</span>))</span><br><span class="line">  operateTimes = time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>, time.localtime(record.value[<span class="string">'operateTime'</span>]/<span class="number">1000</span>))</span><br><span class="line">  record.value[<span class="string">'operate_smonth'</span>] = operate_smonth</span><br><span class="line">  record.value[<span class="string">'ods_update_time'</span>] = datetime.datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">  record.value[<span class="string">'operateTime'</span>] = operateTimes</span><br><span class="line">  record.attributes[<span class="string">'sdc.operation.type'</span>]=<span class="string">'4'</span></span><br><span class="line">  record.value[<span class="string">'Type'</span>]=<span class="string">'UPSERT'</span></span><br><span class="line">  <span class="keyword">for</span> key <span class="keyword">in</span> record.value.keys():</span><br><span class="line">	record.value.setdefault(key,<span class="string">''</span>)</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    output.write(record)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># Send record to error</span></span><br><span class="line">    error.write(record, str(e))</span><br></pre></td></tr></table></figure>
<ul>
<li>operate_smonth:根据从Mongo传过来的字段operateTime转换得到<blockquote>
<p>前面我们在KakfaConsumer的kafka配置页中的Auto Offset Reset选项选的是Timestamp,所以到后面的operateTime是时间戳类型，我们需要新增加一个只年月的operate_smonth的字段</p>
</blockquote>
</li>
<li>ods_update_time:kudu的入库时间，截取的是系统当前的时间<h1 id="三-数据落地"><a href="#三-数据落地" class="headerlink" title="三.数据落地"></a>三.数据落地</h1><h2 id="kudu"><a href="#kudu" class="headerlink" title="kudu"></a>kudu</h2><img data-src="http://m.qpic.cn/psb?/V13VPBDG4Ve2nJ/zuYL.UScwtIHtijTDXMxV1XH51PxgL3BGpzTLOLXyEc!/b/dL8AAAAAAAAA&bo=aAWJAQAAAAARB9U!&rf=viewer_4" alt="image"></li>
</ul>
<h2 id="kudu建表语句"><a href="#kudu建表语句" class="headerlink" title="kudu建表语句"></a>kudu建表语句</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> p_ods.ods_bas_bhv_menu_record  (</span><br><span class="line">_id <span class="keyword">string</span>  <span class="keyword">comment</span> <span class="string">'mongo_ID'</span>,</span><br><span class="line">operate_smonth <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'操作月份'</span>,</span><br><span class="line">_class  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">''</span>,</span><br><span class="line">loginName <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'登录名'</span>,</span><br><span class="line">userName <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'用户名'</span>,</span><br><span class="line">requestURI <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'操作URI'</span>,</span><br><span class="line">queryParams <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'请求参数'</span>,</span><br><span class="line">remoteAddr <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'操作人ip'</span>,</span><br><span class="line">userAgent <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'用户系统信息'</span>,</span><br><span class="line">operateTime <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'操作时间'</span>,</span><br><span class="line">flag <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'是否拆分标记，1-未拆分，2-已拆分'</span>,</span><br><span class="line">ods_update_time <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'ods入库时间'</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (_id, operate_smonth)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">RANGE</span> (operate_smonth) (</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-07'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-08'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-09'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-10'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-11'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2018-12'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-01'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-02'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-03'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-04'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-05'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-06'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-07'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-08'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-09'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-10'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-11'</span>,</span><br><span class="line">    <span class="keyword">PARTITION</span> <span class="keyword">VALUE</span> = <span class="string">'2019-12'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> KUDU;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Streamsets</category>
      </categories>
      <tags>
        <tag>Streamsets</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase 导入导出-export import</title>
    <url>/2019/03/04/hbase%20import%20export/</url>
    <content><![CDATA[<h2 id="Export"><a href="#Export" class="headerlink" title="Export"></a>Export</h2><p>源端</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">hbase</span> <span class="selector-tag">org</span><span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.Export</span> <span class="selector-tag">TableName</span> <span class="selector-tag">OutputPath</span></span><br></pre></td></tr></table></figure>

<p>以表’wang’为例</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">[hbase@dn012005 ~]$ hdfs dfs -mkdir /hbase/export_wang</span><br><span class="line">[hbase@dn012005 ~]$ hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.Export</span> wang hdfs:<span class="comment">//nn012018:9000/hbase/export_wang</span></span><br></pre></td></tr></table></figure>
<h2 id="Import"><a href="#Import" class="headerlink" title="Import"></a>Import</h2><p>目标端</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">hbase</span> <span class="selector-tag">org</span><span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.Import</span> <span class="selector-tag">TableName</span> <span class="selector-tag">InputPath</span></span><br></pre></td></tr></table></figure>
<p>以表’wang’为例</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">wang.sb@belle<span class="selector-class">.lan</span>:/home/wang.sb]$ hbase shell</span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">013</span>:<span class="number">0</span>&gt; create <span class="string">'wang'</span>,<span class="string">'f1'</span>,<span class="string">'f2'</span></span><br><span class="line">wang.sb@belle<span class="selector-class">.lan</span>:/home/wang.sb]$ hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.Import</span> ShopSkuRecommends_Result hdfs:<span class="comment">//10.24.20.2:8020/user/wang.sb/hbase/import_wang/*</span></span><br></pre></td></tr></table></figure>
<h2 id="附注"><a href="#附注" class="headerlink" title="附注"></a>附注</h2><h3 id="hdfs命令："><a href="#hdfs命令：" class="headerlink" title="hdfs命令："></a>hdfs命令：</h3><ul>
<li>hdfs 删除文件夹</li>
</ul>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hdfs dfs -rm  -r <span class="regexp">/user/</span>wang.sb<span class="regexp">/hbase/im</span>port_wang</span><br></pre></td></tr></table></figure>
<ul>
<li>hdfs 上传文件至hdfs</li>
</ul>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">hdfs dfs -<span class="keyword">put</span> 本地路径 hdfs路径</span><br></pre></td></tr></table></figure>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hdfs dfs -put <span class="regexp">/home/</span>wang.sb<span class="regexp">/hbase/im</span>port_wang<span class="regexp">/* /u</span>ser<span class="regexp">/wang.sb/</span>hbase<span class="regexp">/import_wang</span></span><br></pre></td></tr></table></figure>
<ul>
<li>hdfs 创建文件夹</li>
</ul>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir -p <span class="regexp">/user/</span>wang.sb<span class="regexp">/hbase/im</span>port_wang</span><br></pre></td></tr></table></figure>
<h3 id="hbase-命令"><a href="#hbase-命令" class="headerlink" title="hbase 命令"></a>hbase 命令</h3><ul>
<li>查看表结构</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">012</span>:<span class="number">0</span>&gt; desc <span class="string">'wang'</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：创建的表结构与导出数据的表结构不同，会报错</p>
</blockquote>
<p>eg:<br>源端表结构</p>
<figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line">hbase(main):014:0&gt; desc <span class="string">'wang'</span></span><br><span class="line">Table old_user is ENABLED           </span><br><span class="line">old_user                            </span><br><span class="line">COLUMN FAMILIES DESCRIPTION         </span><br><span class="line">&#123;NAME =&gt; <span class="string">'cf'</span>, VERSIONS =&gt; <span class="string">'1'</span>, EVICT_BLOCKS_ON_CLOSE =&gt; <span class="string">'false'</span>, NEW_VERSION_BEHAVIOR =&gt; <span class="string">'false'</span>, KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>, CACHE_DATA_ON_WRITE =&gt; <span class="string">'false'</span>, DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>, T</span><br><span class="line">TL =&gt; <span class="string">'FOREVER'</span>, MIN_VERSIONS =&gt; <span class="string">'0'</span>, REPLICATION_SCOPE =&gt; <span class="string">'0'</span>, BLOOMFILTER =&gt; <span class="string">'ROW'</span>, CACHE_INDEX_ON_WRITE =&gt; <span class="string">'false'</span>, IN_MEMORY =&gt; <span class="string">'false'</span>, CACHE_BLOOMS_ON_WRITE =&gt; <span class="string">'false'</span>, PREFETCH_BLOCKS_ON_</span><br><span class="line">OPEN =&gt; <span class="string">'false'</span>, COMPRESSION =&gt; <span class="string">'NONE'</span>, BLOCKCACHE =&gt; <span class="string">'true'</span>, BLOCKSIZE =&gt; <span class="string">'65536'</span>&#125;               </span><br><span class="line">1 row(s)</span><br></pre></td></tr></table></figure>
<p>目标端</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">014</span>:<span class="number">0</span>&gt; create <span class="string">'wang'</span>,<span class="string">'cf'</span>,<span class="string">'info'</span></span><br></pre></td></tr></table></figure>
<p>两端表结构不同是不能迁移数据的</p>
<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line">&#123;NAME =&gt; <span class="string">'cf'</span>, <span class="comment">//列族</span></span><br><span class="line">BLOOMFILTER =&gt; <span class="string">'NONE'</span>, <span class="comment">//布隆过滤器</span></span><br><span class="line">VERSIONS =&gt; <span class="string">'3'</span>,<span class="comment">//设置保存的版本数</span></span><br><span class="line">IN_MEMORY =&gt;<span class="string">'false'</span>,<span class="comment">//设置激进缓存，优先考虑将该列族放入块缓存中，</span></span><br><span class="line"> <span class="comment">//针对随机读操作相对较多的列族可以设置该属性为true</span></span><br><span class="line">KEEP_DELETED_CELLS =&gt; <span class="string">'FALSE'</span>,<span class="comment">//参见：http://hbase.apache.org/book.html#cf.keep.deleted</span></span><br><span class="line">DATA_BLOCK_ENCODING =&gt; <span class="string">'NONE'</span>,<span class="comment">//数据块编码方式设置</span></span><br><span class="line">COMPRESSION =&gt; <span class="string">'NONE'</span>,<span class="comment">//设置压缩算法</span></span><br><span class="line">TTL =&gt; <span class="string">'FOREVER'</span>, <span class="comment">//参见：http://hbase.apache.org/book.html#ttl</span></span><br><span class="line">MIN_VERSIONS =&gt; <span class="string">'0'</span>,<span class="comment">//最小存储版本数</span></span><br><span class="line">BLOCKCACHE =&gt; <span class="string">'false'</span>, <span class="comment">//数据块缓存属性</span></span><br><span class="line">BLOCKSIZE =&gt; <span class="string">'65536'</span>, <span class="comment">//设置HFile数据块大小（默认64kb）</span></span><br><span class="line">REPLICATION_SCOPE =&gt; <span class="string">'0'</span><span class="comment">//配置HBase集群replication时需要将该参数设置为1.</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>hbse</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/06/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>hive-udf函数</title>
    <url>/2019/01/30/hive-udf%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h2 id="UDF函数开发"><a href="#UDF函数开发" class="headerlink" title="UDF函数开发"></a>UDF函数开发</h2><ul>
<li>标准函数（UDF）：以一行数据中的一列或者多列数据作为参数然后返回解雇欧式一个值的函数，同样也可以返回一个复杂的对象，例如array，map，struct。</li>
<li>聚合函数(UDAF)：接受从零行到多行的零个到多个列，然后返回单一值。例如sum函数。</li>
<li>生成函数（UDTF）:接受零个或者多个输入，然后产生多列或者多行输出。<h3 id="udf函数开发"><a href="#udf函数开发" class="headerlink" title="udf函数开发"></a>udf函数开发</h3></li>
</ul>
<p>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数，用户自定义函数（user defined function)，针对单条记录。编写一个UDF，需要继承UDF类，并实现evaluate()函数。在查询执行过程中，查询中对应的每个应用到这个函数的地方都会对这个类进行实例化。对于每行输入都会调用到evaluate()函数。而evaluate()函数处理的值会返回给Hive。同时用户是可以重载evaluate方法的。Hive会像Java的方法重载一样，自动选择匹配的方法。</p>
<h3 id="准备数据："><a href="#准备数据：" class="headerlink" title="准备数据："></a>准备数据：</h3><ul>
<li>littlebigdata.txt</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">edward capriolo,edward@media6degrees.com,2-12-1981,209.191.139.200,M,10</span><br><span class="line">bob,bob@test.net,10-10-2004,10.10.10.1,M,50</span><br><span class="line">sara connor,sara@sky.net,4-5-1974,64.64.5.1,F,2</span><br></pre></td></tr></table></figure>

<h3 id="创建表："><a href="#创建表：" class="headerlink" title="创建表："></a>创建表：</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> littlebigdata(</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">email <span class="keyword">string</span>,</span><br><span class="line">bday <span class="keyword">string</span>,</span><br><span class="line">ip <span class="keyword">string</span>,</span><br><span class="line">gender <span class="keyword">string</span>,</span><br><span class="line">anum <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>


<h3 id="加载数据："><a href="#加载数据：" class="headerlink" title="加载数据："></a>加载数据：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash">load data inpath <span class="string">'hdfs://nameservice1/user/hive/udfdata/littlebigdata.txt'</span> into table littlebigdata;</span></span><br></pre></td></tr></table></figure>

<h3 id="代码示例："><a href="#代码示例：" class="headerlink" title="代码示例："></a>代码示例：</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.rttx.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: king</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Email</span>: wang.shubing@szrttx.com</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Datetime</span>: Created In 2018/4/19 15:58</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Desc</span>: as follows.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDFZodiacSign</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SimpleDateFormat df;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UDFZodiacSign</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        df = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"yyyy-MM-dd"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(Date bday)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> evaluate(bday.getMonth(), bday.getDay());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(String bday)</span> </span>&#123;</span><br><span class="line">        Date date = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            date = df.parse(bday);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">            System.out.println(<span class="string">"异常"</span>);</span><br><span class="line">            ex.printStackTrace();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> evaluate(date.getMonth() + <span class="number">1</span>, date.getDay());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">evaluate</span><span class="params">(Integer month, Integer day)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">20</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Capricorn"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Aquarius"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">19</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Capricorn"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Pisces"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">20</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Pisces"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Aries"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">4</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">20</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Aries"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Taurus"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">5</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">20</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Taurus"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Gemini"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">6</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">21</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Gemini"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Cancer"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">7</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">22</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Cancer"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Leo"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">8</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">23</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Leo"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Virgo"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">9</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">22</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Virgo"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Libra"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">10</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">24</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Libra"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Scorpio"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">22</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Scorpio"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Sagittarius"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (month == <span class="number">12</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (day &lt; <span class="number">22</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Sagittarius"</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"Capricorn"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        UDFZodiacSign aa = <span class="keyword">new</span> UDFZodiacSign();</span><br><span class="line">        String str = aa.evaluate(<span class="string">"1992-11-05"</span>);</span><br><span class="line">        System.out.println(str);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="加载："><a href="#加载：" class="headerlink" title="加载："></a>加载：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> ADD JAR hdfs://nameservice1/user/hive/udfdata/hive-udf-test1.jar;</span></span><br><span class="line"></span><br><span class="line">Added [/root/kingudftest/hive-udf-test1.jar] to class path</span><br><span class="line">Added resources: [/root/kingudftest/hive-udf-test1.jar]</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> CREATE TEMPORARY FUNCTION <span class="built_in">test</span> AS <span class="string">'com.rttx.hive.udf.UDFZodiacSign'</span>;</span></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>


<h3 id="查询："><a href="#查询：" class="headerlink" title="查询："></a>查询：</h3><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line">hive&gt; <span class="keyword">select</span> name,bday,test(bday) <span class="keyword">from</span> littlebigdata;</span><br></pre></td></tr></table></figure>

<h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">edward capriolo    2-12-1981    Capricorn</span><br><span class="line">bob    10-10-2004    Libra</span><br><span class="line">sara connor    4-5-1974    Aries</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="UDF四种加载方式"><a href="#UDF四种加载方式" class="headerlink" title="UDF四种加载方式"></a>UDF四种加载方式</h2><h3 id="第一种："><a href="#第一种：" class="headerlink" title="第一种："></a>第一种：</h3><ul>
<li><p>是最常见但也不招人喜欢的方式是使用ADD JAR(s)语句，之所以说是不招人喜欢是，通过该方式添加的jar文件只存在于当前会话中，当会话关闭后不能够继续使用该jar文件，最常见的问题是创建了永久函数到metastore中，再次使用该函数时却提示ClassNotFoundException。所以使用该方式每次都要使用ADD JAR(s)语句添加相关的jar文件到Classpath中。</p>
<h3 id="第二种："><a href="#第二种：" class="headerlink" title="第二种："></a>第二种：</h3></li>
<li><p>是修改hive-site.xml文件。修改参数hive.aux.jars.path的值指向UDF文件所在的路径。，该参数需要手动添加到hive-site.xml文件中。</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.aux.jars.path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///jarpath/all_new1.jar,file:///jarpath/all_new2.jar<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="第三种："><a href="#第三种：" class="headerlink" title="第三种："></a>第三种：</h3><p>是在${HIVE_HOME}下创建auxlib目录，将UDF文件放到该目录中，这样hive在启动时会将其中的jar文件加载到classpath中。（推荐）</p>
<h3 id="第四种："><a href="#第四种：" class="headerlink" title="第四种："></a>第四种：</h3><ul>
<li>是设置HIVE_AUX_JARS_PATH环境变量，变量的值为放置jar文件的目录，可以拷贝${HIVE_HOME}/conf中的hive-env.sh.template为hive-env.sh文件，并修改最后一行的#export HIVE_AUX_JARS_PATH=为exportHIVE_AUX_JARS_PATH=jar文件目录来实现，或者在系统中直接添加HIVE_AUX_JARS_PATH环境变量。</li>
</ul>
]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>UDF</tag>
      </tags>
  </entry>
  <entry>
    <title>hive内部表与外部表区别</title>
    <url>/2019/02/02/hive%E5%86%85%E9%83%A8%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>总结一下Hive的内部表和外部表以及两者的区别。</p>
<h2 id="1、建表语句"><a href="#1、建表语句" class="headerlink" title="1、建表语句"></a>1、建表语句</h2><h3 id="1-1-内部表"><a href="#1-1-内部表" class="headerlink" title="1.1 内部表"></a>1.1 内部表</h3><p>平时创建的普通表为内部表(默认创建的就是内部表)</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_internal (</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试内部表'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-外部表"><a href="#1-2-外部表" class="headerlink" title="1.2 外部表"></a>1.2 外部表</h3><p>带external关键字的为外部表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="string">`test_external`</span> (</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试外部表'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> ;</span><br></pre></td></tr></table></figure>
<p>仅从建表语句上看，内部表和外部表的区别为是否带有external关键字。</p>
<h2 id="2、location"><a href="#2、location" class="headerlink" title="2、location"></a>2、location</h2><ul>
<li>其实不管是内部表还是外部表都可以加location关键字指定hive表的存储路径，当然也可以不加，从这点看是没有区别的。</li>
<li>如果不加location那么不管是内部表还是外部表都会在默认的hive配置的hdfs路径下下新建一个和表名相同的文件夹。</li>
<li>如果加了location指定另外的文件夹，那么在查询时，该文件夹下对应的数据会加载到hive表里。<h3 id="2-1-数据"><a href="#2-1-数据" class="headerlink" title="2.1 数据"></a>2.1 数据</h3></li>
</ul>
<p>data.txt</p>
<p>002,张三 <br>003,李四</p>
<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span> -p /tmp/king/internal_location</span><br><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span> -p /tmp/king/external_location</span><br><span class="line">hadoop <span class="built_in">fs</span> -put data.txt /tmp/king/internal_location</span><br><span class="line">hadoop <span class="built_in">fs</span> -put data.txt /tmp/king/external_location</span><br></pre></td></tr></table></figure>
<h3 id="2-2-建表"><a href="#2-2-建表" class="headerlink" title="2.2 建表"></a>2.2 建表</h3><h4 id="2-2-1-内部表"><a href="#2-2-1-内部表" class="headerlink" title="2.2.1 内部表"></a>2.2.1 内部表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_internal_location (</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试内部表location'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> </span><br><span class="line">location <span class="string">'/tmp/king/internal_location'</span>;</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-外部表"><a href="#2-2-2-外部表" class="headerlink" title="2.2.2 外部表"></a>2.2.2 外部表</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> test_external_location (</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'ID'</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'名字'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'测试外部表location'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span> </span><br><span class="line">location <span class="string">'/tmp/king/external_location'</span>;</span><br></pre></td></tr></table></figure>
<h2 id="3、删除表"><a href="#3、删除表" class="headerlink" title="3、删除表"></a>3、删除表</h2><p>内部表和外部表的区别主要体现在删除表，将上面建立的四个表都删掉。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test_internal;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test_external;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test_internal_location;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test_external_location;</span><br></pre></td></tr></table></figure>

<p>看一下对应的hdfs路径有啥变化<br>hadoop命令</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls <span class="regexp">/user/</span>hive<span class="regexp">/warehouse/</span>db_king.db</span><br><span class="line">hadoop fs -ls <span class="regexp">/tmp/</span>king</span><br></pre></td></tr></table></figure>

<p>发现外部表test_external文件夹和external_location文件夹都存在，而内部表的两个文件夹都没了，这也就是内部表和外部表的区别：</p>
<p>内部表删除表时，对应的hdfs的路径下的文件会删掉；外部表删除表时，对应的HDFS的路径下的文件则不会删掉，无论是建表是指定location还是不指定location</p>
<h2 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h2><ul>
<li>建表时带有external关键字为外部表，否则为内部表</li>
<li>内部表和外部表建表时都可以自己指定location</li>
<li>在删除表的时候，Hive将会把属于表的元数据和数据全部删掉；而删除外部表的时候，Hive仅仅删除外部表的元数据，数据是不会删除的！</li>
<li>其他用法是一样的</li>
</ul>
]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>hive跨集群数据迁移</title>
    <url>/2019/03/01/hive%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h4 id="1-源端查看要传递的表建表语句"><a href="#1-源端查看要传递的表建表语句" class="headerlink" title="1.源端查看要传递的表建表语句"></a>1.源端查看要传递的表建表语句</h4><p>源端hive shell执行</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> tablename</span><br></pre></td></tr></table></figure>
<h4 id="2-目标端建表"><a href="#2-目标端建表" class="headerlink" title="2.目标端建表"></a>2.目标端建表</h4><p>目标端hive shell执行</p>
<p>根据第一步中建表语句建表即可</p>
<h4 id="3-distcp"><a href="#3-distcp" class="headerlink" title="3.distcp"></a>3.distcp</h4><p>源端shell执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop distcp -D ipc.client.fallback-to-simple-auth-allowed=true -i -skipcrccheck -update hdfs://nn012018:9000/hive/warehouse/king_test.db/king_test_table webhdfs://目标端ip:9870/user/hive/warehouse/king_test.db/king_test_table1</span><br></pre></td></tr></table></figure>
<h4 id="4-目标端加载数据"><a href="#4-目标端加载数据" class="headerlink" title="4.目标端加载数据"></a>4.目标端加载数据</h4><p>目标端hive shell执行</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hive&gt;  load data inpath 'user/hive/warehouse/king_test.db/king_test_table1' into table king_test_table1;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>数据迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>hive count(*) count(1) 故障</title>
    <url>/2019/03/19/%E8%AE%B0%E4%B8%80%E6%AC%A1hive%E7%AE%80%E5%8D%95%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%87%86%E7%A1%AE%E6%95%85%E9%9A%9C/</url>
    <content><![CDATA[<ul>
<li>首先我通过hadoop distcp的命令进行hive的跨集群迁移数据<br>迁移完之后我们查询某一张表的总数</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> king_db.test;</span><br></pre></td></tr></table></figure>
<p>结果：10</p>
<ul>
<li>然后我们通过sqoop的增量更新一条数据之后<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> king_db.test;</span><br></pre></td></tr></table></figure>
结果：10</li>
</ul>
<p>问题来了<br>我的数据已经增加了一条，为什么查询结果还是10？<br>为此我们先去看看hdfs里的文件数据是否已经收到</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadfs dfs -cat /user/hive/warehouse/king_db.db/test/part-0000 wc|-l</span><br></pre></td></tr></table></figure>
<p>结果：11<br>那么说明我的数据是已经更新过来了，使用impala—shell查询结果同样是11，然后我们又使用了hive</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> king_db.test;</span><br></pre></td></tr></table></figure>
<p>结果：11<br>问题出在哪？<br>对hive 查询计划</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> king_db.test;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> <span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> king_db.test;</span><br></pre></td></tr></table></figure>
<p>此时发现原因：两次的查询计划不一致，count(id) 查询走的是MR，而Hive哪些查询会执行mr<br>hive 0.10.0为了执行效率考虑，简单的查询，就是只是select，不带count,sum,group by这样的，都不走map/reduce，直接读取hdfs文件进行filter过滤. <br>这样做的好处就是不新开mr任务，执行效率要提高不少，但是不好的地方就是用户界面不友好，有时候数据量大还是要等很长时间，但是又没有任何返回。<br>改这个很简单，在hive-site.xml里面有个配置参数叫<br>hive.fetch.task.conversion<br>将这个参数设置为more，简单查询就不走map/reduce了，设置为minimal，就任何简单select都会走map/reduce</p>
<p>因为是生产上hive，暂时不能随意更改配置，那么我们的方法就是：<br>使用ANALYZE命令对表重新更新统计信息并重新统计后结果正确，</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">hive&gt; analyze table king_db.test compute statistics;</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>error</category>
      </categories>
      <tags>
        <tag>Error</tag>
      </tags>
  </entry>
  <entry>
    <title>Azkaban Install</title>
    <url>/2019/01/23/Azkaban-install/</url>
    <content><![CDATA[<h4 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> CREATE DATABASE db_azkaban;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> GRANT ALL ON db_azkaban.* TO <span class="string">'cdh6'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'123456'</span>;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> FLUSH PRIVILEGES;</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="Azkaban-Web-Server-安装和配置-基于master机器操作"><a href="#Azkaban-Web-Server-安装和配置-基于master机器操作" class="headerlink" title="Azkaban Web Server 安装和配置 (基于master机器操作)"></a>Azkaban Web Server 安装和配置 (基于master机器操作)</h4><h4 id="1-Clone-the-repo"><a href="#1-Clone-the-repo" class="headerlink" title="1. Clone the repo:"></a>1. Clone the repo:</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/azkaban/azkaban.git</span></span><br></pre></td></tr></table></figure>
<h4 id="2-Build-Azkaban-and-create-an-installation-package"><a href="#2-Build-Azkaban-and-create-an-installation-package" class="headerlink" title="2. Build Azkaban and create an installation package:"></a>2. Build Azkaban and create an installation package:</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> azkaban; </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ./gradlew build installDist</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意！</p>
</blockquote>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">&gt; Task :az-crypto:test </span><br><span class="line"></span><br><span class="line">azkaban<span class="selector-class">.crypto</span><span class="selector-class">.EncryptionTest</span> &gt; testEncryption FAILED</span><br><span class="line">    org<span class="selector-class">.jasypt</span><span class="selector-class">.exceptions</span><span class="selector-class">.EncryptionOperationNotPossibleException</span>: Encryption raised an exception. A possible cause is you are using <span class="selector-tag">strong</span> encryption algorithms and you have not installed the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files <span class="keyword">in</span> this Java Virtual Machine</span><br><span class="line">        at org<span class="selector-class">.jasypt</span><span class="selector-class">.encryption</span><span class="selector-class">.pbe</span><span class="selector-class">.StandardPBEByteEncryptor</span>.handleInvalidKeyException(StandardPBEByteEncryptor<span class="selector-class">.java</span>:<span class="number">1073</span>)</span><br><span class="line">        at org<span class="selector-class">.jasypt</span><span class="selector-class">.encryption</span><span class="selector-class">.pbe</span><span class="selector-class">.StandardPBEByteEncryptor</span>.encrypt(StandardPBEByteEncryptor<span class="selector-class">.java</span>:<span class="number">924</span>)</span><br><span class="line">        at org<span class="selector-class">.jasypt</span><span class="selector-class">.encryption</span><span class="selector-class">.pbe</span><span class="selector-class">.StandardPBEStringEncryptor</span>.encrypt(StandardPBEStringEncryptor<span class="selector-class">.java</span>:<span class="number">642</span>)</span><br><span class="line">        at azkaban<span class="selector-class">.crypto</span><span class="selector-class">.CryptoV1_1</span>.encrypt(CryptoV1_1<span class="selector-class">.java</span>:<span class="number">42</span>)</span><br><span class="line">        at azkaban<span class="selector-class">.crypto</span><span class="selector-class">.Crypto</span>.encrypt(Crypto<span class="selector-class">.java</span>:<span class="number">58</span>)</span><br><span class="line">        at azkaban<span class="selector-class">.crypto</span><span class="selector-class">.EncryptionTest</span>.testEncryption(EncryptionTest<span class="selector-class">.java</span>:<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">Download https:<span class="comment">//repo.maven.apache.org/maven2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.pom</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>解决方法：<br>下载JCE<br><a href="https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/cn/java/javase/downloads/jce8-download-2133166-zhs.html</a> <br>我这边使用的JDK8，包含了JCE所需要的jre8</p>
</blockquote>
<p>解压</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> unzip jce_policy-8.zip</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 复制到JDK里</span></span><br><span class="line">cp UnlimitedJCEPolicyJDK8/* /usr/java/jdk1.8.0_131/jre/lib/security</span><br></pre></td></tr></table></figure>
<p>重新编译</p>
<p>等待编译完成</p>
<h4 id="3-Start-the-solo-server"><a href="#3-Start-the-solo-server" class="headerlink" title="3. Start the solo server:"></a>3. Start the solo server:</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> azkaban-solo-server/build/install/azkaban-solo-server; </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bin/start-solo.sh</span></span><br><span class="line"></span><br><span class="line">http//localhost:8081</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> bin/shutdown-solo.sh</span></span><br></pre></td></tr></table></figure>
<h4 id="4-隔离出web-server与exec-server"><a href="#4-隔离出web-server与exec-server" class="headerlink" title="4.隔离出web-server与exec-server"></a>4.隔离出web-server与exec-server</h4><figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"># <span class="meta">cp</span> -r /root/azkaban/azkaban-exec-server/<span class="keyword">build/install/azkaban-exec-server/* </span>/<span class="meta">opt</span>/azkaban/azkaban-exec-server</span><br><span class="line"></span><br><span class="line"># <span class="meta">cp</span> -r /root/azkaban/azkaban-web-server/<span class="keyword">build/install/azkaban-web-server/* </span>/<span class="meta">opt</span>/azkaban/azkaban-web-server</span><br></pre></td></tr></table></figure>
<h4 id="5-Azkaban-Web-Server-配置"><a href="#5-Azkaban-Web-Server-配置" class="headerlink" title="5.Azkaban Web Server 配置"></a>5.Azkaban Web Server 配置</h4><ul>
<li>azkaban配置<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@k8sn125 azkaban-web-server]# mkdir logs extlib plugins</span><br><span class="line">[root@k8sn125 azkaban-web-server]#cd conf</span><br><span class="line">[root@k8sn125 conf]# vim azkaban.properties</span><br><span class="line"></span><br><span class="line">***************************************************************************BEGIN</span><br><span class="line"><span class="meta">#</span><span class="bash">设置项目名称</span></span><br><span class="line">zkaban.name=BigData</span><br><span class="line"><span class="meta">#</span><span class="bash">设置项目副标题</span></span><br><span class="line">azkaban.label=besttone</span><br><span class="line">azkaban.color=#FF3601</span><br><span class="line">azkaban.default.servlet.path=/index</span><br><span class="line"><span class="meta">#</span><span class="bash">这个是azkaban-web-server工程web目录下的静态资源绝对路径</span></span><br><span class="line">web.resource.dir=/opt/azkaban/azkaban-web-server/web/</span><br><span class="line"><span class="meta">#</span><span class="bash">一定要设置为上海，否则按美国时间执行</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">多个executors节点</span></span><br><span class="line">azkaban.use.multiple.executors=true</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span><br><span class="line">azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1</span><br><span class="line">azkaban.executorselector.comparator.Memory=1</span><br><span class="line">azkaban.executorselector.comparator.LastDispatched=1</span><br><span class="line">azkaban.executorselector.comparator.CpuUsage=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">用户权限管理默认类</span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager</span><br><span class="line">user.manager.xml.file=/opt/azkaban/azkaban-web-server/conf/azkaban-users.xml</span><br><span class="line"><span class="meta">#</span><span class="bash"> global配置文件所在位置</span></span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">mysql数据库配置</span></span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=10.0.30.39</span><br><span class="line"><span class="meta">#</span><span class="bash">改为自己的数据库名称</span></span><br><span class="line">mysql.database=db_azkaban</span><br><span class="line"><span class="meta">#</span><span class="bash">改为自己的数据库账号</span></span><br><span class="line">mysql.user=cdh6</span><br><span class="line"><span class="meta">#</span><span class="bash">改为自己的数据库密码</span></span><br><span class="line">mysql.numconnections=100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">该工程基于jetty容器允许，配置jetty</span></span><br><span class="line">jetty.maxThreads=25</span><br><span class="line">jetty.ssl.port=8443</span><br><span class="line">jetty.port=8081</span><br><span class="line">jetty.password=azkaban</span><br><span class="line">jetty.keypassword=azkaban</span><br><span class="line"><span class="meta">#</span><span class="bash">SSL证书地址, 后面会解释怎么生成证书</span></span><br><span class="line">jetty.keystore=/opt/azkaban/azkaban-web-server/conf/keystore</span><br><span class="line">jetty.truststore=/opt/azkaban/azkaban-web-server/conf/truststore</span><br><span class="line">jetty.trustpassword=azkaban</span><br><span class="line">***************************************************************************END</span><br></pre></td></tr></table></figure></li>
<li>jetty ssl配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">jetty ssl keystore生成</span></span><br><span class="line">keytool -genkey -keystore keystore -alias jetty-azkaban -keyalg RSA -validity 3560</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">根据提示输入相关信息，我的密码为azkaban，和上面的password对应</span></span><br><span class="line">keytool -export -alias jetty-azkaban -keystore keystore -rfc -file selfsignedcert.cer</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">过程中如果要输入密码，请输入之前密码</span></span><br><span class="line">keytool -import -alias certificatekey -file selfsignedcert.cer -keystore truststore</span><br></pre></td></tr></table></figure>
<ul>
<li>用户设置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim azkaban-users.xml</span></span><br><span class="line"><span class="meta">#</span><span class="bash">加入以下内容，配置登陆的用户和相关角色</span></span><br><span class="line">&lt;azkaban-users&gt;</span><br><span class="line">  &lt;user groups="azkaban" password="azkaban" roles="admin" username="azkaban"/&gt;</span><br><span class="line">  &lt;user password="metrics" roles="metrics" username="metrics"/&gt;</span><br><span class="line">  &lt;role name="admin" permissions="ADMIN"/&gt;</span><br><span class="line">  &lt;role name="metrics" permissions="METRICS"/&gt;</span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>log4j配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">必须得有这个配置文件而且有内容，不然会报奇怪的错误</span></span><br><span class="line">cd conf </span><br><span class="line">vi log4j.properties</span><br><span class="line"><span class="meta">#</span><span class="bash">日志级别为INFO</span></span><br><span class="line">log4j.rootLogger=INFO,C</span><br><span class="line">log4j.appender.C=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.C.Target=System.err</span><br><span class="line">log4j.appender.C.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.C.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %-5p %c&#123;1&#125;:%L - %m%n</span><br></pre></td></tr></table></figure>
<h4 id="6-Azkaban-Executor-Server-配置"><a href="#6-Azkaban-Executor-Server-配置" class="headerlink" title="6.Azkaban Executor Server 配置"></a>6.Azkaban Executor Server 配置</h4><figure class="highlight autoit"><table><tr><td class="code"><pre><span class="line">[root<span class="symbol">@k8sn125</span> azkaban]<span class="meta"># cd azkaban-exec-server/</span></span><br><span class="line">[root<span class="symbol">@k8sn125</span> azkaban-exec-server]<span class="meta"># cd conf/</span></span><br><span class="line">[root<span class="symbol">@k8sn125</span> conf]<span class="meta"># ls</span></span><br><span class="line">azkaban.properties  <span class="keyword">global</span>.properties  log4j.properties</span><br></pre></td></tr></table></figure>
<ul>
<li>azkaban 配置</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">vim azkaban.properties</span><br><span class="line"></span><br><span class="line"><span class="comment"># Azkaban Personalization Settings</span></span><br><span class="line">default.timezone.<span class="attribute">id</span>=Asia/Shanghai</span><br><span class="line">executor.global.<span class="attribute">properties</span>=/opt/azkaban/azkaban-exec-server/conf/global.properties</span><br><span class="line">azkaban.project.<span class="attribute">dir</span>=projects</span><br><span class="line"><span class="comment"># JMX stats</span></span><br><span class="line">jetty.connector.<span class="attribute">stats</span>=<span class="literal">true</span></span><br><span class="line">executor.connector.<span class="attribute">stats</span>=<span class="literal">true</span></span><br><span class="line"><span class="comment"># Azkaban plugin settings</span></span><br><span class="line">azkaban.jobtype.plugin.<span class="attribute">dir</span>=/opt/azkaban/azkaban-exec-server/plugins/jobtypes</span><br><span class="line"><span class="comment"># Azkaban mysql settings by default. Users should configure their own username and password.</span></span><br><span class="line">database.<span class="attribute">type</span>=mysql</span><br><span class="line">mysql.<span class="attribute">port</span>=3306</span><br><span class="line">mysql.<span class="attribute">host</span>=10.0.30.39</span><br><span class="line">mysql.<span class="attribute">database</span>=db_azkaban</span><br><span class="line">mysql.<span class="attribute">user</span>=cdh6</span><br><span class="line">mysql.<span class="attribute">password</span>=123456</span><br><span class="line">mysql.<span class="attribute">numconnections</span>=100</span><br><span class="line"><span class="comment"># Azkaban Executor settings</span></span><br><span class="line">executor.<span class="attribute">maxThreads</span>=50</span><br><span class="line">executor.<span class="attribute">port</span>=12321</span><br><span class="line">executor.flow.<span class="attribute">threads</span>=30</span><br></pre></td></tr></table></figure>
<ul>
<li>log4j配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">必须得有这个配置文件而且有内容，不然会报奇怪的错误</span></span><br><span class="line">vi log4j.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">加上以下内容</span></span><br><span class="line">log4j.rootLogger=INFO, Console</span><br><span class="line">log4j.logger.azkaban=INFO, server</span><br><span class="line">log4j.appender.server=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.server.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.server.File=var/log/azkaban-webserver.log</span><br><span class="line">log4j.appender.server.layout.ConversionPattern=%d&#123;yyyy/MM/dd HH:mm:ss.SSS Z&#125; %p [%c&#123;1&#125;] [Azkaban] %m%n</span><br><span class="line">log4j.appender.server.MaxFileSize=102400MB</span><br><span class="line">log4j.appender.server.MaxBackupIndex=2</span><br><span class="line">log4j.appender.Console=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.Console.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.Console.layout.ConversionPattern=%d&#123;yyyy/MM/dd HH:mm:ss.SSS Z&#125; %p [%c&#123;1&#125;] [Azkaban] %m%n</span><br></pre></td></tr></table></figure>
<ul>
<li>修改commonprivate.properties文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">修改commonprivate.properties文件</span></span><br><span class="line">cd azkaban-exec-server/plugins/jobtypes</span><br><span class="line">vi commonprivate.properties</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认是<span class="literal">true</span>，表示特定用户执行任务</span></span><br><span class="line">execute.as.user=false</span><br><span class="line">azkaban.native.lib=false</span><br></pre></td></tr></table></figure>
<h4 id="7-把azakaban-exec-server发送到slave节点"><a href="#7-把azakaban-exec-server发送到slave节点" class="headerlink" title="7.把azakaban-exec-server发送到slave节点"></a>7.把azakaban-exec-server发送到slave节点</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -rp &#x2F;opt&#x2F;azkaban&#x2F;azkaban-exec-server&#x2F; root@k8sm134:&#x2F;opt&#x2F;azkaban&#x2F;</span><br><span class="line">scp -rp &#x2F;opt&#x2F;azkaban&#x2F;azkaban-exec-server&#x2F; root@k8sm126:&#x2F;opt&#x2F;azkaban&#x2F;</span><br></pre></td></tr></table></figure>
<h4 id="8-mysql插入数据"><a href="#8-mysql插入数据" class="headerlink" title="8.mysql插入数据"></a>8.mysql插入数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash">use db_azkaban; </span></span><br><span class="line">insert into executors(host,port,active) values('k8sn125',12321,1);</span><br><span class="line">insert into executors(host,port,active) values('k8sm126',12321,1);</span><br><span class="line">insert into executors(host,port,active) values('k8sm134',12321,1);</span><br></pre></td></tr></table></figure>

<h4 id="9-启动"><a href="#9-启动" class="headerlink" title="9.启动"></a>9.启动</h4><ul>
<li>k8sm126 ,k8sm134</li>
</ul>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="regexp">/opt/</span>azkaban<span class="regexp">/bin/</span>start-exec.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>k8sn125</li>
</ul>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="regexp">/opt/</span>azkaban<span class="regexp">/azkaban-web-server/</span>bin<span class="regexp">/start-web.sh</span></span><br></pre></td></tr></table></figure>
<p>访问地址<br><a href="https://k8sn125:8443" target="_blank" rel="noopener">https://k8sn125:8443</a><br>azkaban/azkaban</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>安装部署</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS7.5+CDH6安装部署</title>
    <url>/2019/01/24/CentOS7.5,CDH6%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><h4 id="1-CDH简介"><a href="#1-CDH简介" class="headerlink" title="1.CDH简介　"></a>1.CDH简介　</h4><p>Cloudera’s Distribution, including Apache Hadoop<br>基于Web的用户界面,支持大多数Hadoop组件，包括HDFS、MapReduce、Hive、Pig、 Hbase、Zookeeper、Sqoop,简化了大数据平台的安装、使用难度。</p>
<a id="more"></a>
<h4 id="2-硬件配置"><a href="#2-硬件配置" class="headerlink" title="2.硬件配置"></a>2.硬件配置</h4><ul>
<li>-u root -p 123456</li>
</ul>
<table>
<thead>
<tr>
<th>IP</th>
<th>HostName</th>
<th>OS</th>
<th>Cores</th>
<th>Memory</th>
<th>Disk</th>
<th>Remark</th>
</tr>
</thead>
<tbody><tr>
<td>172.20.32.1</td>
<td>master</td>
<td>CentOS7.3</td>
<td>4</td>
<td>32G</td>
<td>3T</td>
<td>NameNode，cloudera  Server</td>
</tr>
<tr>
<td>172.20.32.2</td>
<td>slave1</td>
<td>CentOS7.3</td>
<td>4</td>
<td>32G</td>
<td>1.5T</td>
<td>NameNode，cloudera  agent</td>
</tr>
<tr>
<td>172.20.32.3</td>
<td>slave2</td>
<td>CentOS7.3</td>
<td>2</td>
<td>16G</td>
<td>6.6T</td>
<td>DataNode，cloudera  agent</td>
</tr>
</tbody></table>
<blockquote>
<p>注意：假如是虚拟机，磁盘挂载单目录（无需分盘）</p>
</blockquote>
<ul>
<li>其中namenode用来部署HA，其中mysql,hdfs.namenode，Hbase.Master,Yarn.ResourceManager等</li>
<li>datanode用来作为worker节点，还需承担zk，kafka等其他服务的部署</li>
</ul>
<h4 id="软件配置"><a href="#软件配置" class="headerlink" title="软件配置"></a>软件配置</h4><ul>
<li>JDK1.8.0_131</li>
<li>mysql5.6 <figure class="highlight accesslog"><table><tr><td class="code"><pre><span class="line"><span class="number">172.20.32.37</span> <span class="number">3306</span> </span><br><span class="line">-u cdh6 -p <span class="number">123456</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>name</th>
<th>版本</th>
<th>来源</th>
<th>remak</th>
</tr>
</thead>
<tbody><tr>
<td>CM</td>
<td>6.0.1</td>
<td><a href="https://archive.cloudera.com/cm6/6.0.1/" target="_blank" rel="noopener">https://archive.cloudera.com/cm6/6.0.1/</a></td>
<td></td>
</tr>
<tr>
<td>CDH</td>
<td>6.0.1</td>
<td><a href="https://archive.cloudera.com/cdh6/6.0.1/parcels/" target="_blank" rel="noopener">https://archive.cloudera.com/cdh6/6.0.1/parcels/</a></td>
<td></td>
</tr>
<tr>
<td>hadoop</td>
<td>3.0.0-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>hive</td>
<td>2.1.1-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>hbase</td>
<td>2.0.0-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>solr</td>
<td>7.0.0+cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>impala</td>
<td>3.0.0-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>hue</td>
<td>3.9.0+cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>sqoop</td>
<td>1.4.7-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>kafka</td>
<td>1.0.0-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>spark</td>
<td>2.2.0+cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>oozie</td>
<td>5.0.0-beta1-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>kudu</td>
<td>1.6.0-cdh6.0.1</td>
<td>cdh自带包</td>
<td></td>
</tr>
<tr>
<td>flume-ng</td>
<td>flume-ng</td>
<td>cdh自带包</td>
<td></td>
</tr>
</tbody></table>
</li>
</ul>
<blockquote>
<p>注意：其他组件需要单独安装部署，eg:flink,kylin,ELK等</p>
</blockquote>
<h2 id="基础环境配置准备"><a href="#基础环境配置准备" class="headerlink" title="基础环境配置准备"></a>基础环境配置准备</h2><h4 id="1-关闭防火墙-master-slave1-2"><a href="#1-关闭防火墙-master-slave1-2" class="headerlink" title="1.关闭防火墙[master,slave1-2]"></a>1.关闭防火墙[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">临时关闭防火墙</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl stop firewalld</span></span><br><span class="line">永久防火墙开机自启动</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl <span class="built_in">disable</span> firewalld </span></span><br><span class="line">查看防火墙状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl status firewalld</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  systemctl status firewalld</span></span><br><span class="line">Unit firewalld.service could not be found.</span><br></pre></td></tr></table></figure>

<h4 id="2-selinux关闭-master-slave1-2"><a href="#2-selinux关闭-master-slave1-2" class="headerlink" title="2.selinux关闭[master,slave1-2]"></a>2.selinux关闭[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/sysconfig/selinux</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line">重启机器，命令：</span><br><span class="line"><span class="meta">#</span><span class="bash"> reboot</span></span><br><span class="line">重启后检查：</span><br><span class="line"><span class="meta">#</span><span class="bash"> sestatus –v</span></span><br></pre></td></tr></table></figure>
<h4 id="3-hostname-config-master-slave1-2"><a href="#3-hostname-config-master-slave1-2" class="headerlink" title="3.hostname config[master,slave1-2]"></a>3.hostname config[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">master上执行</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/hosts</span></span><br><span class="line">172.20.32.2 slave1</span><br><span class="line">172.20.32.1 master</span><br><span class="line">172.20.32.3 slave2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/sysconfig/network</span></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=slave1</span><br><span class="line"><span class="meta">#</span><span class="bash"> ---------------------------</span></span><br><span class="line">slave1上执行</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/hosts</span></span><br><span class="line">172.20.32.2 slave1</span><br><span class="line">172.20.32.1 master</span><br><span class="line">172.20.32.3 slave2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/sysconfig/network</span></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=master</span><br><span class="line"><span class="meta">#</span><span class="bash"> -----------------------------</span></span><br><span class="line">slave2上执行</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/hosts</span></span><br><span class="line">172.20.32.2 slave1</span><br><span class="line">172.20.32.1 master</span><br><span class="line">172.20.32.3 slave2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/sysconfig/network</span></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=slave2</span><br><span class="line"><span class="meta">#</span><span class="bash"> --------------------------</span></span><br></pre></td></tr></table></figure>
<h4 id="4-NTP服务器设置-master-slave1-2"><a href="#4-NTP服务器设置-master-slave1-2" class="headerlink" title="4.NTP服务器设置[master,slave1-2]"></a>4.NTP服务器设置[master,slave1-2]</h4><p>一般默认即可！如有问题还请查阅相关资料</p>
<h4 id="5-安装jdk-master-slave1-2"><a href="#5-安装jdk-master-slave1-2" class="headerlink" title="5.安装jdk[master,slave1-2]"></a>5.安装jdk[master,slave1-2]</h4><p>安装之前先卸载CentOS7自带的JDK</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">查看系统已经装的jdk： </span><br><span class="line"><span class="meta">#</span><span class="bash"> rpm -qa|grep jdk</span></span><br><span class="line">卸载jdk：</span><br><span class="line"><span class="meta">#</span><span class="bash"> rpm -e --nodeps java-1.6.0-openjdk-javadoc-1.6.0.0-1.66.1.13.0.el6.x86_64</span></span><br><span class="line"></span><br><span class="line">1.下载“jdk-8u131-linux-x64.tar.gz”，放到/usr/java/目录下</span><br><span class="line">2.解压</span><br><span class="line"><span class="meta">#</span><span class="bash"> tar -xzvf jdk-8u131-linux-x64.tar.gz</span></span><br><span class="line">3.在任何目录下均可编辑  </span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile.d/java_home.sh</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">export JRE_HOME=$JAVA_HOME/jre</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile.d/java_home.sh</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意！必须在该目录下安装JDK，后续cdh默认加载此路径的JDK**</p>
</blockquote>
<h4 id="6-配置各节点之间ssh免密登录-master-slave1-2"><a href="#6-配置各节点之间ssh免密登录-master-slave1-2" class="headerlink" title="6. 配置各节点之间ssh免密登录[master,slave1-2]"></a>6. 配置各节点之间ssh免密登录[master,slave1-2]</h4><p>此步骤交给运维去做</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.  CentOS默认没有启动ssh无密登录，</span><br><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/ssh/sshd_config</span></span><br><span class="line">去掉其中3行的注释，每台服务器都要置</span><br><span class="line">开启  RSAAuthentication yes</span><br><span class="line">开启  PubkeyAuthentication yes</span><br><span class="line">开启  AuthorizedKeyFile  .ssh/authorized_keys</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh-keygen -t rsa</span></span><br><span class="line">一直按回车</span><br><span class="line"><span class="meta">#</span><span class="bash"> ls /root/.ssh/ </span></span><br><span class="line">id_rsa 和 id_rsa.pub</span><br><span class="line"><span class="meta">#</span><span class="bash"> cp id_rsa.pub authorized_keys</span></span><br><span class="line"> 两个woker节点执行        </span><br><span class="line"><span class="meta">#</span><span class="bash"> cat ~/.ssh/authorized_keys | ssh root@172.20.32.1 <span class="string">'cat &gt;&gt; ~/.ssh/authorized_keys'</span></span></span><br><span class="line"></span><br><span class="line">master上执行</span><br><span class="line"><span class="meta">#</span><span class="bash"> scp authorized_keys root@172.20.32.2:/root/.ssh/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> scp authorized_keys root@172.20.32.3:/root/.ssh/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> scp known_hosts root@172.20.32.2:/root/.ssh/</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> scp known_hosts root@172.20.32.3:/root/.ssh/</span></span><br><span class="line">slave执行</span><br><span class="line"><span class="meta">#</span><span class="bash"> chmod 700 ~/.ssh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> chmod 600 ~/.ssh/authorized_keys</span></span><br><span class="line">重启ssh服务</span><br><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart sshd.service</span></span><br><span class="line">  验证能否无密码ssh，在master服务器上执行操作 </span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh slave1</span></span><br><span class="line">不同用户之间退出</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">logout</span></span></span><br></pre></td></tr></table></figure>
<h4 id="7-优化虚拟内存需求率-master-slave1-2"><a href="#7-优化虚拟内存需求率-master-slave1-2" class="headerlink" title="7.优化虚拟内存需求率[master,slave1-2]"></a>7.优化虚拟内存需求率[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">检查虚拟内存需求率</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /proc/sys/vm/swappiness</span></span><br><span class="line">显示如下：</span><br><span class="line"> 30</span><br><span class="line">临时降低虚拟内存需求率</span><br><span class="line"><span class="meta">#</span><span class="bash"> sysctl vm.swappiness=10</span></span><br><span class="line">永久降低虚拟内存需求率</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">'vm.swappiness = 10'</span> &gt; /etc/sysctl.d/swappiness.conf</span></span><br><span class="line">并运行如下命令使生效</span><br><span class="line"><span class="meta">#</span><span class="bash"> sysctl -p</span></span><br></pre></td></tr></table></figure>

<h4 id="8-解决透明大页面问题-master-slave1-2"><a href="#8-解决透明大页面问题-master-slave1-2" class="headerlink" title="8. 解决透明大页面问题[master,slave1-2]"></a>8. 解决透明大页面问题[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">检查透明大页面问题</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /sys/kernel/mm/transparent_hugepage/defrag</span></span><br><span class="line">如果显示为：</span><br><span class="line">[always] madvise never</span><br><span class="line">临时关闭透明大页面问题</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span></span><br><span class="line"></span><br><span class="line">确认配置生效</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat /sys/kernel/mm/transparent_hugepage/defrag</span></span><br><span class="line">应该显示为：</span><br><span class="line">always madvise [never]</span><br><span class="line">配置开机自动生效</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span></span><br></pre></td></tr></table></figure>
<h4 id="9-mysql安装"><a href="#9-mysql安装" class="headerlink" title="9.mysql安装"></a>9.mysql安装</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">FAQ</span><br><span class="line">采用的是DBA提供的<span class="number">172.20</span><span class="number">.32</span><span class="number">.37</span> 这个服务器上的mysql</span><br></pre></td></tr></table></figure>
<h4 id="10-拷贝mysql-connector-java到各个节点指定目录下-master-slave1-2"><a href="#10-拷贝mysql-connector-java到各个节点指定目录下-master-slave1-2" class="headerlink" title="10 拷贝mysql-connector-java到各个节点指定目录下[master,slave1-2]"></a>10 拷贝mysql-connector-java到各个节点指定目录下[master,slave1-2]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cp mysql-connector-java-5.1.44-bin.jar /usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>
<h4 id="11-创建数据库-172-20-32-37"><a href="#11-创建数据库-172-20-32-37" class="headerlink" title="11.创建数据库[172.20.32.37]"></a>11.创建数据库[172.20.32.37]</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> mysql -u root -p 123456</span></span><br><span class="line">CREATE DATABASE db_cdh6_amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">CREATE DATABASE db_cdh6_hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">CREATE DATABASE db_cdh6_metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">CREATE DATABASE db_cdh6_sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">CREATE DATABASE db_cdh6_oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci</span><br><span class="line">CREATE DATABASE db_cdh6_hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GRANT ALL ON db_cdh6_scm.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_amon.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_hue.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_metastore.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_sentry.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_oozie.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line">GRANT ALL ON db_cdh6_hive.* TO 'cdh6'@'%' IDENTIFIED BY '123456';</span><br><span class="line"></span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Cloudera-Manager安装"><a href="#Cloudera-Manager安装" class="headerlink" title="Cloudera Manager安装"></a>Cloudera Manager安装</h2><h4 id="1、配置-Cloudera-Manager-仓库master-slave1-2"><a href="#1、配置-Cloudera-Manager-仓库master-slave1-2" class="headerlink" title="1、配置 Cloudera Manager 仓库master,slave1-2"></a>1、配置 Cloudera Manager 仓库<a href="1">master,slave1-2</a></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# wget https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/ </span><br><span class="line">[root@master ~]# rpm --import https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line">[root@slave1 ~]# wget https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/ </span><br><span class="line">[root@slave1 ~]# rpm --import https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line">[root@slave2 ~]# wget https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/ </span><br><span class="line">[root@slave2 ~]# rpm --import https://archive.cloudera.com/cm6/6.0.1/redhat7/yum/RPM-GPG-KEY-cloudera</span><br></pre></td></tr></table></figure>
<p>使用仓库安装会比较慢，建议先把需要的rpm下载下来，进行离线安装或者建私有仓库，主要下面的三个软件包：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">cloudera-manager-agent<span class="number">-6.0</span><span class="number">.1</span><span class="number">-610811.</span>el7.x86_64.rpm</span><br><span class="line">cloudera-manager-daemons<span class="number">-6.0</span><span class="number">.1</span><span class="number">-610811.</span>el7.x86_64.rpm</span><br><span class="line">cloudera-manager-server<span class="number">-6.0</span><span class="number">.1</span><span class="number">-610811.</span>el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">//cloudera-manager-daemons 是 server 和 agent 必须安装的。</span></span><br></pre></td></tr></table></figure>
<h4 id="构建本地仓库-2-任意服务器"><a href="#构建本地仓库-2-任意服务器" class="headerlink" title="构建本地仓库(2) [任意服务器]"></a>构建本地仓库(2) [任意服务器]</h4><ul>
<li>master<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cm6.0.1</span><br><span class="line">[root@slave3 ~]# sudo yum install httpd</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo systemctl start httpd</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo mkdir -p /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.1/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cm6</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- cdh6.0.1</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.1/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.1/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel.sha256 -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cdh6/6.0.1/parcels/manifest.json -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/gplextras6/6.0.1/redhat7/ -P /var/www/html/cloudera-repos</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# sudo chmod -R ugo+rX /var/www/html/cloudera-repos/cdh6</span><br></pre></td></tr></table></figure>

<h5 id="创建访问端口"><a href="#创建访问端口" class="headerlink" title="创建访问端口"></a>创建访问端口</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@slave3 ~]# cd /var/www/html</span><br><span class="line"></span><br><span class="line">[root@slave3 ~]# python -m SimpleHTTPServer 8900</span><br></pre></td></tr></table></figure>

<h5 id="构建yum源-master-slave1-2"><a href="#构建yum源-master-slave1-2" class="headerlink" title="构建yum源[master,slave1-2]"></a>构建yum源[master,slave1-2]</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# vim /etc/yum.repos.d/cloudera-repo.repo</span><br><span class="line">[root@slave1 ~]# vim /etc/yum.repos.d/cloudera-repo.repo</span><br><span class="line">[root@slave2 ~]# vim /etc/yum.repos.d/cloudera-repo.repo</span><br><span class="line"></span><br><span class="line">[cloudera-repo]</span><br><span class="line">name=cloudera-repo</span><br><span class="line">baseurl=http://master:8900/cloudera-repos/cm6/6.0.1/redhat7/yum/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure>




<h4 id="2-安装-CM-Server-和-Agent"><a href="#2-安装-CM-Server-和-Agent" class="headerlink" title="2.安装 CM Server 和 Agent"></a>2.安装 CM Server 和 Agent</h4><p>建议离线安装，把rpm包下载到服务器上面，传到其他节点一份，再本地安装，速度会快很多。</p>
<ul>
<li><input disabled="" type="checkbox"> master：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><input disabled="" type="checkbox"> slave1,slave2：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@slave1 ~]# yum install cloudera-manager-daemons cloudera-manager-agent</span><br><span class="line">[root@slave2 ~]# yum install cloudera-manager-daemons cloudera-manager-agent</span><br></pre></td></tr></table></figure>
<h4 id="3-设置-Cloudera-Manager-数据库"><a href="#3-设置-Cloudera-Manager-数据库" class="headerlink" title="3.设置 Cloudera Manager 数据库"></a>3.设置 Cloudera Manager 数据库</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql在本地时执行：</span><br><span class="line">[root@master ~]# rm -rf //cloudera-scm-server/db.mgmt.properties</span><br><span class="line">[root@master ~]# /opt/cloudera/cm/schema/scm_prepare_database.sh mysql db_cdh6_scm -h172.20.32.37  -uroot -p'blf1#root' --scm-host 172.20.32.1 db_cdh6_scm scm scm</span><br><span class="line"></span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_131</span><br><span class="line">Verifying that we can write to /etc/cloudera-scm-server</span><br><span class="line">Creating SCM configuration file in /etc/cloudera-scm-server</span><br><span class="line">Executing:  /usr/java/jdk1.8.0_131/bin/java -cp /usr/share/java/mysql-connector-java.jar:/usr/share/java/oracle-connector-java.jar:/usr/share/java/postgresql-connector-java.jar:/opt/cloudera/cm/schema/../lib/* com.cloudera.enterprise.dbutil.DbCommandExecutor /etc/cloudera-scm-server/db.properties com.cloudera.cmf.db.</span><br><span class="line">[                          main] DbCommandExecutor              INFO  Successfully connected to database.</span><br><span class="line">All done, your SCM database is configured correctly!</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意！上面最好指定ip,切勿使用hostname很容易出现问题</p>
</blockquote>
<h4 id="4-安装-CDH-master"><a href="#4-安装-CDH-master" class="headerlink" title="4.安装 CDH[master]"></a>4.安装 CDH[master]</h4><p>配置CDH的软件包 parcels(namenode01)<br>CM安装成功之后，接下来我们就可以通过CM安装CDH的方式构建企业大数据平台。所以首先需要把CDH的parcels包下载到CM主服务器上。<br>同样的，我们为了加速我们的安装，我们可以把需要下载的软件包提前下载下来，也可以创建CDH私有仓库。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cd /opt/cloudera/parcel-repo</span><br><span class="line">[root@master ~]# wget http://172.20.32.36/package/CDH/CDH-6.0.1-1/CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel</span><br><span class="line">[root@master ~]# wget https://archive.cloudera.com/cdh6/6.0.1/parcels/manifest.json</span><br><span class="line">[root@master ~]# wget https://archive.cloudera.com/cdh6/6.0.1/parcels/CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel.sha256</span><br><span class="line"></span><br><span class="line">[root@master ~]# mv CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel.sha256 CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel.sha</span><br></pre></td></tr></table></figure>
<p>也可以执行下面的操作<br>在manifest.json文件中，找到对应版本的秘钥，复制到.sha文件中。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# echo "2e650f1f1ea020a3efc98a231b85c2df1a50b030" &gt; CDH-6.0.1-1.cdh6.0.1.p0.590678-el7.parcel.sha</span><br></pre></td></tr></table></figure>

<p>修改属主属组。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# chown cloudera-scm.cloudera-scm /opt/cloudera/parcel-repo/ *</span><br></pre></td></tr></table></figure>

<p>启动 Cloudera Manager Server[master]</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# systemctl start cloudera-scm-server</span><br></pre></td></tr></table></figure>

<p>如果启动中有什么问题，可以查看日志。[master]</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@master ~]# tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log</span><br></pre></td></tr></table></figure>
<p>参考：<br><a href="http://blog.51cto.com/wzlinux/2321433" target="_blank" rel="noopener">http://blog.51cto.com/wzlinux/2321433</a><br><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/installation.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.0/topics/installation.html</a></p>
]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink DataSet Transformations</title>
    <url>/2019/01/25/Flink%20DataSet%20Transformations/</url>
    <content><![CDATA[<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><ul>
<li>Map转换在DataSet的每个元素上应用用户定义的map函数。它实现了一对一的映射，也就是说，函数必须返回一个元素。</li>
</ul>
<p>以下代码将Integer对的DataSet转换为Integers的DataSet：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> intPairs: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> intSums = intPairs.map &#123; pair =&gt; pair._1 + pair._2 &#125;</span><br></pre></td></tr></table></figure>
<h3 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h3><ul>
<li>FlatMap转换在DataSet的每个元素上应用用户定义的平面映射函数。map函数的这种变体可以为每个输入元素返回任意多个结果元素（包括none）。</li>
</ul>
<p>以下代码将文本行的DataSet转换为单词的DataSet：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> textLines: <span class="type">DataSet</span>[<span class="type">String</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> words = textLines.flatMap &#123; _.split(<span class="string">" "</span>) &#125;</span><br></pre></td></tr></table></figure>
<h3 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h3><ul>
<li>MapPartition在单个函数调用中转换并行分区。map-partition函数将分区作为Iterable获取，并且可以生成任意数量的结果值。每个分区中的元素数量取决于并行度和先前的操作。</li>
</ul>
<p>以下代码将文本行的DataSet转换为每个分区的计数数据集：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> textLines: <span class="type">DataSet</span>[<span class="type">String</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// Some is required because the return value must be a Collection.</span></span><br><span class="line"><span class="comment">// There is an implicit conversion from Option to a Collection.</span></span><br><span class="line"><span class="keyword">val</span> counts = texLines.mapPartition &#123; in =&gt; <span class="type">Some</span>(in.size) &#125;</span><br></pre></td></tr></table></figure>
<h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><ul>
<li>Filter转换在DataSet的每个元素上应用用户定义的过滤器函数，并仅保留函数返回的元素true。</li>
</ul>
<p>以下代码从DataSet中删除所有小于零的整数：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> intNumbers: <span class="type">DataSet</span>[<span class="type">Int</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> naturalNumbers = intNumbers.filter &#123; _ &gt; <span class="number">0</span> &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>重要信息：系统假定该函数不会修改应用谓词的元素。违反此假设可能会导致错误的结果。</p>
</blockquote>
<h3 id="Projection-of-Tuple-DataSet"><a href="#Projection-of-Tuple-DataSet" class="headerlink" title="Projection of Tuple DataSet"></a>Projection of Tuple DataSet</h3><ul>
<li>Project转换删除或移动元组DataSet的Tuple字段。该project(int…)方法选择应由其索引保留的元组字段，并在输出元组中定义它们的顺序。</li>
</ul>
<p>预测不需要定义用户功能。</p>
<p>以下代码显示了在DataSet上应用项目转换的不同方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple3&lt;Integer, Double, String&gt;&gt; in = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// converts Tuple3&lt;Integer, Double, String&gt; into Tuple2&lt;String, Integer&gt;</span></span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; out = in.project(<span class="number">2</span>,<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>scala 不支持</p>
</blockquote>
<h4 id="Projection-with-Type-Hint"><a href="#Projection-with-Type-Hint" class="headerlink" title="Projection with Type Hint"></a>Projection with Type Hint</h4><blockquote>
<p>请注意，Java编译器无法推断project运算符的返回类型。如果您对运算符的结果调用另一个运算符，则可能会导致问题，project例如：</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple5&lt;String,String,String,String,String&gt;&gt; ds = ....</span><br><span class="line">DataSet&lt;Tuple1&lt;String&gt;&gt; ds2 = ds.project(<span class="number">0</span>).distinct(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>通过提示返回类型的project运算符可以克服此问题，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataSet&lt;Tuple1&lt;String&gt;&gt; ds2 = ds.&lt;Tuple1&lt;String&gt;&gt;project(<span class="number">0</span>).distinct(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h3 id="分组数据集的转换"><a href="#分组数据集的转换" class="headerlink" title="分组数据集的转换"></a>分组数据集的转换</h3><p>reduce操作可以对分组数据集进行操作。指定用于分组的密钥可以通过多种方式完成：</p>
<ul>
<li>关键表达</li>
<li>键选择器功能</li>
<li>一个或多个字段位置键（仅限元组数据集）</li>
<li>案例类字段（仅限案例类）<br>请查看reduce示例以了解如何指定分组键。</li>
</ul>
<h3 id="减少分组数据集"><a href="#减少分组数据集" class="headerlink" title="减少分组数据集"></a>减少分组数据集</h3><ul>
<li>应用于分组DataSet的Reduce转换使用用户定义的reduce函数将每个组减少为单个元素。对于每组输入元素，reduce函数连续地将元素对组合成一个元素，直到每个组只剩下一个元素。</li>
</ul>
<blockquote>
<p>请注意，对于ReduceFunction返回对象的键控字段，应与输入值匹配。这是因为reduce是可隐式组合的，并且从组合运算符发出的对象在传递给reduce运算符时再次按键分组。</p>
</blockquote>
<h4 id="减少由键表达式分组的DataSet"><a href="#减少由键表达式分组的DataSet" class="headerlink" title="减少由键表达式分组的DataSet"></a>减少由键表达式分组的DataSet</h4><p>键表达式指定DataSet的每个元素的一个或多个字段。每个键表达式都是公共字段的名称或getter方法。点可用于向下钻取对象。关键表达式“*”选择所有字段。以下代码显示如何使用键表达式对POJO DataSet进行分组，并使用reduce函数对其进行缩减。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// some ordinary POJO</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WC</span>(<span class="params">val word: <span class="type">String</span>, val count: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() &#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="literal">null</span>, <span class="number">-1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// [...]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words: <span class="type">DataSet</span>[<span class="type">WC</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> wordCounts = words.groupBy(<span class="string">"word"</span>).reduce &#123;</span><br><span class="line">  (w1, w2) =&gt; <span class="keyword">new</span> <span class="type">WC</span>(w1.word, w1.count + w2.count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="减少由KeySelector函数分组的DataSet"><a href="#减少由KeySelector函数分组的DataSet" class="headerlink" title="减少由KeySelector函数分组的DataSet"></a>减少由KeySelector函数分组的DataSet</h4><ul>
<li>键选择器函数从DataSet的每个元素中提取键值。提取的键值用于对DataSet进行分组。以下代码显示如何使用键选择器函数对POJO DataSet进行分组，并使用reduce函数对其进行缩减。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// some ordinary POJO</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WC</span>(<span class="params">val word: <span class="type">String</span>, val count: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() &#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="literal">null</span>, <span class="number">-1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// [...]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words: <span class="type">DataSet</span>[<span class="type">WC</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> wordCounts = words.groupBy &#123; _.word &#125; reduce &#123;</span><br><span class="line">  (w1, w2) =&gt; <span class="keyword">new</span> <span class="type">WC</span>(w1.word, w1.count + w2.count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="减少由字段位置键分组的DataSet（仅限元组数据集）"><a href="#减少由字段位置键分组的DataSet（仅限元组数据集）" class="headerlink" title="减少由字段位置键分组的DataSet（仅限元组数据集）"></a>减少由字段位置键分组的DataSet（仅限元组数据集）</h4><ul>
<li>字段位置键指定用作分组键的元组数据集的一个或多个字段。以下代码显示如何使用字段位置键并应用reduce函数</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tuples = <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>, <span class="type">Double</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// group on the first and second Tuple field</span></span><br><span class="line"><span class="keyword">val</span> reducedTuples = tuples.groupBy(<span class="number">0</span>, <span class="number">1</span>).reduce &#123; ... &#125;</span><br></pre></td></tr></table></figure>
<h4 id="按案例类字段分组的DataSet减少"><a href="#按案例类字段分组的DataSet减少" class="headerlink" title="按案例类字段分组的DataSet减少"></a>按案例类字段分组的DataSet减少</h4><ul>
<li>使用Case Classes时，您还可以使用字段名称指定分组键：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>(<span class="params">val a: <span class="type">String</span>, b: <span class="type">Int</span>, c: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">tuples</span> </span>= <span class="type">DataSet</span>[<span class="type">MyClass</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="comment">// group on the first and second field</span></span><br><span class="line"><span class="keyword">val</span> reducedTuples = tuples.groupBy(<span class="string">"a"</span>, <span class="string">"b"</span>).reduce &#123; ... &#125;</span><br></pre></td></tr></table></figure>
<h4 id="GroupReduce在分组数据集上"><a href="#GroupReduce在分组数据集上" class="headerlink" title="GroupReduce在分组数据集上"></a>GroupReduce在分组数据集上</h4><ul>
<li><p>应用于分组DataSet的GroupReduce转换为每个组调用用户定义的group-reduce函数。这与Reduce之间的区别在于用户定义的函数会立即获得整个组。在组的所有元素上使用Iterable调用该函数，并且可以返回任意数量的结果元素。</p>
</li>
<li><p>由字段位置键分组的DataSet上的GroupReduce（仅限元组数据集）<br>以下代码显示如何从按Integer分组的DataSet中删除重复的字符串。</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output = input.groupBy(<span class="number">0</span>).reduceGroup &#123;</span><br><span class="line">      (in, out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">String</span>)]) =&gt;</span><br><span class="line">        in.toSet foreach (out.collect)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="按键表达式，键选择器函数或案例类字段分组的DataSet上的GroupReduce"><a href="#按键表达式，键选择器函数或案例类字段分组的DataSet上的GroupReduce" class="headerlink" title="按键表达式，键选择器函数或案例类字段分组的DataSet上的GroupReduce"></a>按键表达式，键选择器函数或案例类字段分组的DataSet上的GroupReduce</h4><p>类似于Reduce转换中的键表达式， 键选择器函数和案例类字段的工作。</p>
<h4 id="对已排序的组进行GroupReduce"><a href="#对已排序的组进行GroupReduce" class="headerlink" title="对已排序的组进行GroupReduce"></a>对已排序的组进行GroupReduce</h4><ul>
<li>group-reduce函数使用Iterable访问组的元素。可选地，Iterable可以按指定的顺序分发组的元素。在许多情况下，这可以帮助降低用户定义的组减少功能的复杂性并提高其效率。</li>
</ul>
<p>下面的代码显示了如何删除由Integer分组并按String排序的DataSet中的重复字符串的另一个示例。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output = input.groupBy(<span class="number">0</span>).sortGroup(<span class="number">1</span>, <span class="type">Order</span>.<span class="type">ASCENDING</span>).reduceGroup &#123;</span><br><span class="line">      (in, out: <span class="type">Collector</span>[(<span class="type">Int</span>, <span class="type">String</span>)]) =&gt;</span><br><span class="line">        <span class="keyword">var</span> prev: (<span class="type">Int</span>, <span class="type">String</span>) = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">for</span> (t &lt;- in) &#123;</span><br><span class="line">          <span class="keyword">if</span> (prev == <span class="literal">null</span> || prev != t)</span><br><span class="line">            out.collect(t)</span><br><span class="line">            prev = t</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果在reduce操作之前使用运算符的基于排序的执行策略建立分组，则GroupSort通常是免费的。</p>
</blockquote>
<h4 id="可组合的GroupReduce函数"><a href="#可组合的GroupReduce函数" class="headerlink" title="可组合的GroupReduce函数"></a>可组合的GroupReduce函数</h4><ul>
<li>与reduce函数相比，group-reduce函数不是可隐式组合的。为了使组合 - 缩减功能可组合，它必须实现GroupCombineFunction接口。</li>
</ul>
<blockquote>
<p>要点：接口的通用输入和输出类型GroupCombineFunction必须等于GroupReduceFunction以下示例中所示的通用输入类型：</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Combinable GroupReduceFunction that computes two sums.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCombinableGroupReducer</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">GroupReduceFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">String</span>]</span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">GroupCombineFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">Int</span>)]</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(</span><br><span class="line">    in: java.lang.<span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)],</span><br><span class="line">    out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> =</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">val</span> r: (<span class="type">String</span>, <span class="type">Int</span>) =</span><br><span class="line">      in.iterator.asScala.reduce( (a,b) =&gt; (a._1, a._2 + b._2) )</span><br><span class="line">    <span class="comment">// concat key and sum and emit</span></span><br><span class="line">    out.collect (r._1 + <span class="string">"-"</span> + r._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">combine</span></span>(</span><br><span class="line">    in: java.lang.<span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)],</span><br><span class="line">    out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> =</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">val</span> r: (<span class="type">String</span>, <span class="type">Int</span>) =</span><br><span class="line">      in.iterator.asScala.reduce( (a,b) =&gt; (a._1, a._2 + b._2) )</span><br><span class="line">    <span class="comment">// emit tuple with key and sum</span></span><br><span class="line">    out.collect(r)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="GroupCombine在分组数据集上"><a href="#GroupCombine在分组数据集上" class="headerlink" title="GroupCombine在分组数据集上"></a>GroupCombine在分组数据集上</h3><ul>
<li><p>GroupCombine变换是可组合GroupReduceFunction中的组合步骤的一般形式。从某种意义上说，它允许将输入类型组合I到任意输出类型O。相反，GroupReduce中的组合步骤仅允许从输入类型I到输出类型的组合I。这是因为GroupReduceFunction中的reduce步骤需要输入类型I。</p>
</li>
<li><p>在一些应用中，期望在执行附加变换（例如，减小数据大小）之前将DataSet组合成中间格式。这可以通过CombineGroup转换以非常低的成本实现。</p>
</li>
</ul>
<blockquote>
<p>注意： Grouped DataSet上的GroupCombine是在内存中使用贪婪策略执行的，该策略可能不会一次处理所有数据，而是分多步处理。它也可以在各个分区上执行，而无需像GroupReduce转换那样进行数据交换。这可能会导致部分结果。</p>
</blockquote>
<p>以下示例演示了如何将CombineGroup转换用于备用WordCount实现。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[<span class="type">String</span>] = [..] <span class="comment">// The words received as input</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> combinedWords: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = input</span><br><span class="line">  .groupBy(<span class="number">0</span>)</span><br><span class="line">  .combineGroup &#123;</span><br><span class="line">    (words, out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]) =&gt;</span><br><span class="line">        <span class="keyword">var</span> key: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (word &lt;- words) &#123;</span><br><span class="line">            key = word</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        out.collect((key, count))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> output: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = combinedWords</span><br><span class="line">  .groupBy(<span class="number">0</span>)</span><br><span class="line">  .reduceGroup &#123;</span><br><span class="line">    (words, out: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]) =&gt;</span><br><span class="line">        <span class="keyword">var</span> key: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ((word, sum) &lt;- words) &#123;</span><br><span class="line">            key = word</span><br><span class="line">            sum += count</span><br><span class="line">        &#125;</span><br><span class="line">        out.collect((key, sum))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的替代WordCount实现演示了GroupCombine在执行GroupReduce转换之前如何组合单词。上面的例子只是一个概念证明。注意，组合步骤如何更改DataSet的类型，这通常需要在执行GroupReduce之前进行额外的Map转换。</p>
<h3 id="聚合在分组元组数据集上"><a href="#聚合在分组元组数据集上" class="headerlink" title="聚合在分组元组数据集上"></a>聚合在分组元组数据集上</h3><p>有一些常用的聚合操作经常使用。Aggregate转换提供以下内置聚合函数：</p>
<ul>
<li>Sum,</li>
<li>Min, and</li>
<li>Max.<br>聚合转换只能应用于元组数据集，并且仅支持字段位置键进行分组。</li>
</ul>
<p>以下代码显示如何对按字段位置键分组的DataSet应用聚合转换：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output = input.groupBy(<span class="number">1</span>).aggregate(<span class="type">SUM</span>, <span class="number">0</span>).and(<span class="type">MIN</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>要在DataSet上应用多个聚合，必须.and()在第一个聚合之后使用该函数，这意味着.aggregate(SUM, 0).and(MIN, 2)生成字段0的总和和原始DataSet的字段2的最小值。与此相反，.aggregate(SUM, 0).aggregate(MIN, 2)将在聚合上应用聚合。在给定的示例中，在计算由字段1分组的字段0的总和之后，它将产生字段2的最小值。</p>
<blockquote>
<p>注意：将来会扩展聚合函数集。</p>
</blockquote>
<h3 id="MinBy-MaxBy在Grouped-Tuple-DataSet上"><a href="#MinBy-MaxBy在Grouped-Tuple-DataSet上" class="headerlink" title="MinBy / MaxBy在Grouped Tuple DataSet上"></a>MinBy / MaxBy在Grouped Tuple DataSet上</h3><p>8 MinBy（MaxBy）转换为每组元组选择一个元组。选定的元组是一个元组，其一个或多个指定字段的值最小（最大）。用于比较的字段必须是有效的关键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</p>
<p>下面的代码显示了如何选择具有最小值的元组，每个元组的字段Integer和Double字段具有相同的String值DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = input</span><br><span class="line">                                   .groupBy(<span class="number">1</span>)  <span class="comment">// group DataSet on second field</span></span><br><span class="line">                                   .minBy(<span class="number">0</span>, <span class="number">2</span>) <span class="comment">// select tuple with minimum values for first and third field.</span></span><br></pre></td></tr></table></figure>
<h3 id="减少完整的DataSet"><a href="#减少完整的DataSet" class="headerlink" title="减少完整的DataSet"></a>减少完整的DataSet</h3><ul>
<li>Reduce转换将用户定义的reduce函数应用于DataSet的所有元素。reduce函数随后将元素对组合成一个元素，直到只剩下一个元素。</li>
</ul>
<p>以下代码显示了如何对Integer DataSet的所有元素求和：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> intNumbers = env.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> sum = intNumbers.reduce (_ + _)</span><br></pre></td></tr></table></figure>
<p>使用Reduce转换减少完整的DataSet意味着最终的Reduce操作不能并行完成。但是，reduce函数可以自动组合，因此Reduce转换不会限制大多数用例的可伸缩性。</p>
<h3 id="完整DataSet上的GroupReduce"><a href="#完整DataSet上的GroupReduce" class="headerlink" title="完整DataSet上的GroupReduce"></a>完整DataSet上的GroupReduce</h3><ul>
<li>GroupReduce转换在DataSet的所有元素上应用用户定义的group-reduce函数。group-reduce可以迭代DataSet的所有元素并返回任意数量的结果元素。</li>
</ul>
<p>以下示例显示如何在完整DataSet上应用GroupReduce转换：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[<span class="type">Int</span>] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output = input.reduceGroup(<span class="keyword">new</span> <span class="type">MyGroupReducer</span>())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果group-reduce函数不可组合，则无法并行完成对完整DataSet的GroupReduce转换。因此，这可能是计算密集型操作。请参阅上面的“可组合GroupReduceFunctions”一节，了解如何实现可组合的group-reduce功能。</p>
</blockquote>
<h3 id="GroupCombine在完整的DataSet上"><a href="#GroupCombine在完整的DataSet上" class="headerlink" title="GroupCombine在完整的DataSet上"></a>GroupCombine在完整的DataSet上</h3><ul>
<li>完整DataSet上的GroupCombine与分组DataSet上的GroupCombine类似。数据在所有节点上分区，然后以贪婪的方式组合（即，只有一次合并到存储器中的数据）。</li>
</ul>
<h3 id="在完整的Tuple-DataSet上聚合"><a href="#在完整的Tuple-DataSet上聚合" class="headerlink" title="在完整的Tuple DataSet上聚合"></a>在完整的Tuple DataSet上聚合</h3><p>有一些常用的聚合操作经常使用。Aggregate转换提供以下内置聚合函数：</p>
<ul>
<li>Sum,</li>
<li>Min, and</li>
<li>Max.<br>聚合转换只能应用于元组数据集。</li>
</ul>
<p>以下代码显示如何在完整DataSet上应用聚合转换：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output = input.aggregate(<span class="type">SUM</span>, <span class="number">0</span>).and(<span class="type">MIN</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：扩展支持的聚合功能集在我们的路线图中。</p>
</blockquote>
<h3 id="完整的Tuple-DataSet上的MinBy-MaxBy"><a href="#完整的Tuple-DataSet上的MinBy-MaxBy" class="headerlink" title="完整的Tuple DataSet上的MinBy / MaxBy"></a>完整的Tuple DataSet上的MinBy / MaxBy</h3><ul>
<li>MinBy（MaxBy）转换从元组的DataSet中选择一个元组。选定的元组是一个元组，其一个或多个指定字段的值最小（最大）。用于比较的字段必须是有效的关键字段，即可比较。如果多个元组具有最小（最大）字段值，则返回这些元组的任意元组。</li>
</ul>
<p>下面的代码演示如何选择与为最大值的元组Integer，并Double从一个领域DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt;：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = <span class="comment">// [...]</span></span><br><span class="line"><span class="keyword">val</span> output: <span class="type">DataSet</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Double</span>)] = input                          </span><br><span class="line">                                   .maxBy(<span class="number">0</span>, <span class="number">2</span>) <span class="comment">// select tuple with maximum values for first and third field.</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>flink</category>
      </categories>
  </entry>
  <entry>
    <title>Flink流处理之迭代案例</title>
    <url>/2019/01/29/Flink%E6%B5%81%E5%A4%84%E7%90%86%E4%B9%8B%E8%BF%AD%E4%BB%A3%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<ul>
<li><p>对于流处理（DataStream），Flink相同提供了对迭代的支持。这一节我们主要来分析流处理中的迭代，我们将会看到流处理中的迭代相较于批处理有类似之处。但差异也是十分之明显。</p>
</li>
<li><p>可迭代的流处理程序同意定义“步函数”（step function）并将其内嵌到一个可迭代的流（IterativeStream）中。由于一个流处理程序可能永不终止，因此不同于批处理中的迭代机制，流处理中无法设置迭代的最大次数。取而代之的是，你能够指定等待反馈输入的最大时间间隔（假设超过该时间间隔没有反馈元素到来。那么该迭代将会终止）。通过应用split或filter转换，你能够指定流的哪一部分用于反馈给迭代头，哪一部分分发给下游。这里我们以filter作为演示样例来展示可迭代的流处理程序的API使用模式。</p>
</li>
</ul>
<ol>
<li>首先。基于输入流构建IterativeStream。这是一个迭代的起始。通常称之为迭代头：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">IterativeStream&lt;Integer&gt; iteration = inputStream.iterate();</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>接着。我们指定一系列的转换操作用于表述在迭代过程中运行的逻辑（这里简单以map转换作为演示样例）。map API所接受的UDF就是我们上文所说的步函数：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; iteratedStream = iteration.map(<span class="comment">/* this is executed many times */</span>);</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>然后。作为迭代我们肯定须要有数据反馈给迭代头进行反复计算，所以我们从迭代过的流中过滤出符合条件的元素组成的部分流，我们称之为反馈流：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; feedbackStream = iteratedStream.filter(<span class="comment">/* one part of the stream */</span>);</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>将反馈流反馈给迭代头就意味着一个迭代的完整逻辑的完毕，那么它就能够“关闭”这个闭合的“环”了。通过调用IterativeStream的closeWith这一实例方法能够关闭一个迭代（也可表述为定义了迭代尾）。传递给closeWith的数据流将会反馈给迭代头：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">iteration.closeWith(feedbackStream);</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>另外，一个惯用的模式是过滤出须要继续向前分发的部分流，这个过滤转换事实上定义的是“终止迭代”的逻辑条件，符合条件的元素将被分发给下游而不用于进行下一次迭代：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; output = iteratedStream.filter(<span class="comment">/* some other part of the stream */</span>);</span><br></pre></td></tr></table></figure>
<h3 id="eg"><a href="#eg" class="headerlink" title="eg:"></a>eg:</h3><ol>
<li>首先，我们先通过source函数创建初始的流对象inputStream：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;Integer, Integer&gt;&gt; inputStream = env.addSource(<span class="keyword">new</span> RandomFibonacciSource());</span><br></pre></td></tr></table></figure>
<p>该source函数会生成二元组序列，二元组的两个字段值是随机生成的作为斐波那契数列的初始值：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RandomFibonacciSource</span>        </span></span><br><span class="line"><span class="class">    <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Random random = <span class="keyword">new</span> Random();    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> isRunning = <span class="keyword">true</span>;    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> counter = <span class="number">0</span>;   </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Tuple2&lt;Integer, Integer&gt;&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;        </span><br><span class="line">        <span class="keyword">while</span> (isRunning &amp;&amp; counter &lt; MAX_RANDOM_VALUE) &#123;            </span><br><span class="line">            <span class="keyword">int</span> first = random.nextInt(MAX_RANDOM_VALUE / <span class="number">2</span> - <span class="number">1</span>) + <span class="number">1</span>;            </span><br><span class="line">            <span class="keyword">int</span> second = random.nextInt(MAX_RANDOM_VALUE / <span class="number">2</span> -<span class="number">1</span>) + <span class="number">1</span>;  </span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (first &gt; second) <span class="keyword">continue</span>;            </span><br><span class="line"></span><br><span class="line">            ctx.collect(<span class="keyword">new</span> Tuple2&lt;Integer, Integer&gt;(first, second));            </span><br><span class="line">            counter++;            </span><br><span class="line">            Thread.sleep(<span class="number">50</span>);        </span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;        </span><br><span class="line">        isRunning = <span class="keyword">false</span>;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>为了对新计算的斐波那契数列中的值以及累加的迭代次数进行存储，我们须要将二元组数据流转换为五元组数据流，并据此创建迭代对象：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">IterativeStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; iterativeStream =        </span><br><span class="line">    inputStream.map(<span class="keyword">new</span> TupleTransformMapFunction()).iterate(<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>注意上面代码段中iterate API的參数5000，不是指迭代5000次，而是等待反馈输入的最大时间间隔为5秒。</li>
<li>流被觉得是无界的。所以无法像批处理迭代那样指定最大迭代次数。但它同意指定一个最大等待间隔，假设在给定的时间间隔里没有元素到来。那么将会终止迭代。</li>
</ul>
</blockquote>
<p>元组转换的map函数实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TupleTransformMapFunction</span> <span class="keyword">extends</span> <span class="title">RichMapFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Integer</span>,        </span></span><br><span class="line"><span class="class">    <span class="title">Integer</span>&gt;, <span class="title">Tuple5</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; <span class="title">map</span><span class="params">(            </span></span></span><br><span class="line"><span class="function"><span class="params">        Tuple2&lt;Integer, Integer&gt; inputTuples)</span> <span class="keyword">throws</span> Exception </span>&#123;        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;(                </span><br><span class="line">            inputTuples.f0,                </span><br><span class="line">            inputTuples.f1,                </span><br><span class="line">            inputTuples.f0,                </span><br><span class="line">            inputTuples.f1,                </span><br><span class="line">            <span class="number">0</span>);    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上面五元组中，当中索引为0。1这两个位置的元素，始终都是最初生成的两个元素不会变化，而后三个字段都会随着迭代而变化。</p>
</blockquote>
<ol start="3">
<li>在迭代流iterativeStream创建完毕之后，我们将基于它运行斐波那契数列的步函数并产生斐波那契数列流fibonacciStream：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; fibonacciStream =        </span><br><span class="line">    iterativeStream.map(<span class="keyword">new</span> FibonacciCalcStepFunction());</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里的fibonacciStream仅仅是一个代称，当中的数据并非真正的斐波那契数列，事实上就是上面那个五元组。</p>
</blockquote>
<p>当中用于计算斐波那契数列的步函数实现例如以下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FibonacciCalcStepFunction</span> <span class="keyword">extends</span>        </span></span><br><span class="line"><span class="class">    <span class="title">RichMapFunction</span>&lt;<span class="title">Tuple5</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;,        </span></span><br><span class="line"><span class="class">    <span class="title">Tuple5</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; <span class="title">map</span><span class="params">(            </span></span></span><br><span class="line"><span class="function"><span class="params">        Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; inputTuple)</span> <span class="keyword">throws</span> Exception </span>&#123;        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;(                </span><br><span class="line">            inputTuple.f0,                </span><br><span class="line">            inputTuple.f1,                </span><br><span class="line">            inputTuple.f3,                </span><br><span class="line">            inputTuple.f2 + inputTuple.f3,                </span><br><span class="line">            ++inputTuple.f4);    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>正如上文所述。后三个字段会产生变化。在计算之前，数列最后一个元素会被保留。也就是f3相应的元素，然后通过f2元素加上f3元素会产生最新值并更新f3元素。而f4则会累加。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>随着迭代次数添加，不是整个数列都会被保留。仅仅有最初的两个元素和最新的两个元素会被保留，这里也不是必需保留整个数列，由于我们不须要完整的数列。我们仅仅须要对最新的两个元素进行推断就可以。</li>
</ul>
</blockquote>
<ol start="4">
<li>每一个元素计算斐波那契数列的新值并产生了fibonacciStream，可是我们须要对最新的两个值进行推断。看它们是否超过了指定的阈值。超过了阈值的元组将会被输出，而没有超过的则会再次參与迭代。因此这将产生两个不同的分支。我们也为此构建了分支流：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SplitStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; branchedStream =        </span><br><span class="line">    fibonacciStream.split(<span class="keyword">new</span> FibonacciOverflowSelector());</span><br></pre></td></tr></table></figure>
<p>而对是否超过阈值的元组进行推断并分离的实现例如以下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FibonacciOverflowSelector</span> <span class="keyword">implements</span> <span class="title">OutputSelector</span>&lt;</span></span><br><span class="line"><span class="class">    <span class="title">Tuple5</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(            </span></span></span><br><span class="line"><span class="function"><span class="params">        Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt; inputTuple)</span> </span>&#123;        </span><br><span class="line">        <span class="keyword">if</span> (inputTuple.f2 &lt; OVERFLOW_THRESHOLD &amp;&amp; inputTuple.f3 &lt; OVERFLOW_THRESHOLD) &#123;            </span><br><span class="line">            <span class="keyword">return</span> Collections.singleton(ITERATE_FLAG);        </span><br><span class="line">        &#125;        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Collections.singleton(OUTPUT_FLAG);    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在筛选方法select中，我们对不同的分支以不同的常量标识符进行标识：ITERATE_FLAG（还要继续迭代）和OUTPUT_FLAG（直接输出）。</p>
<p>产生了分支流之后。我们就能够从中检出不同的流分支做迭代或者输出处理。</p>
<ol start="5">
<li>对须要再次迭代的，就通过迭代流的closeWith方法反馈给迭代头：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">iterativeStream.closeWith(branchedStream.select(ITERATE_FLAG));</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>而对于不须要的迭代就直接让其流向下游处理，这里我们仅仅是简单得将流“重构”了一下然后直接输出：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; outputStream = branchedStream        </span><br><span class="line">    .select(OUTPUT_FLAG).map(<span class="keyword">new</span> BuildOutputTupleMapFunction());</span><br><span class="line">outputStream.print();</span><br></pre></td></tr></table></figure>
<p>所谓的重构就是将之前的五元组又一次缩减为三元组，实现例如以下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">BuildOutputTupleMapFunction</span> <span class="keyword">extends</span> <span class="title">RichMapFunction</span>&lt;        </span></span><br><span class="line"><span class="class">    <span class="title">Tuple5</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;,        </span></span><br><span class="line"><span class="class">    <span class="title">Tuple3</span>&lt;<span class="title">Integer</span>, <span class="title">Integer</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple3&lt;Integer, Integer, Integer&gt; <span class="title">map</span><span class="params">(Tuple5&lt;Integer, Integer, Integer, Integer,            </span></span></span><br><span class="line"><span class="function"><span class="params">        Integer&gt; inputTuple)</span> <span class="keyword">throws</span> Exception </span>&#123;        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple3&lt;Integer, Integer, Integer&gt;(                </span><br><span class="line">            inputTuple.f0,                </span><br><span class="line">            inputTuple.f1,                </span><br><span class="line">            inputTuple.f4);    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整的主干程序代码例如以下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;    </span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment            </span><br><span class="line">        .getExecutionEnvironment().setBufferTimeout(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple2&lt;Integer, Integer&gt;&gt; inputStream = env.addSource(<span class="keyword">new</span> RandomFibonacciSource());    </span><br><span class="line"></span><br><span class="line">    IterativeStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; iterativeStream =            </span><br><span class="line">        inputStream.map(<span class="keyword">new</span> TupleTransformMapFunction()).iterate(<span class="number">5000</span>);    </span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; fibonacciStream =            </span><br><span class="line">        iterativeStream.map(<span class="keyword">new</span> FibonacciCalcStepFunction());    </span><br><span class="line"></span><br><span class="line">    SplitStream&lt;Tuple5&lt;Integer, Integer, Integer, Integer, Integer&gt;&gt; branchedStream =            </span><br><span class="line">        fibonacciStream.split(<span class="keyword">new</span> FibonacciOverflowSelector());    </span><br><span class="line"></span><br><span class="line">    iterativeStream.closeWith(branchedStream.select(ITERATE_FLAG));    </span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple3&lt;Integer, Integer, Integer&gt;&gt; outputStream = branchedStream            </span><br><span class="line">        .select(OUTPUT_FLAG).map(<span class="keyword">new</span> BuildOutputTupleMapFunction());    </span><br><span class="line"></span><br><span class="line">    outputStream.print();    </span><br><span class="line">    env.execute(<span class="string">"Streaming Iteration Example"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">1&gt;</span><span class="bash"> (2,3,3)</span></span><br><span class="line"><span class="meta">2&gt;</span><span class="bash"> (3,4,2)</span></span><br><span class="line"><span class="meta">3&gt;</span><span class="bash"> (2,4,2)</span></span><br><span class="line"><span class="meta">4&gt;</span><span class="bash"> (1,3,3)</span></span><br><span class="line"><span class="meta">5&gt;</span><span class="bash"> (2,4,2)</span></span><br><span class="line"><span class="meta">6&gt;</span><span class="bash"> (2,3,3)</span></span><br><span class="line"><span class="meta">7&gt;</span><span class="bash"> (4,4,2)</span></span><br><span class="line"><span class="meta">8&gt;</span><span class="bash"> (2,3,3)</span></span><br><span class="line"><span class="meta">9&gt;</span><span class="bash"> (2,4,2)</span></span><br><span class="line"><span class="meta">10&gt;</span><span class="bash"> (2,3,3)</span></span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase四种数据迁移方案</title>
    <url>/2019/03/05/Hbase%E5%9B%9B%E7%A7%8D%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<p><a href="https://hbase.apache.org/book.html#ops.backup" target="_blank" rel="noopener">官方文档</a></p>
<p><a href="https://www.cnblogs.com/ballwql/p/hbase_data_transfer.html" target="_blank" rel="noopener">参考文档</a></p>
<h1 id="1-HDFS层次迁移"><a href="#1-HDFS层次迁移" class="headerlink" title="1. HDFS层次迁移"></a>1. HDFS层次迁移</h1><h2 id="一-完全关机备份"><a href="#一-完全关机备份" class="headerlink" title="一.完全关机备份"></a>一.完全关机备份</h2><p>某些环境可以容忍其HBase群集的定期完全关闭，例如，如果它正在使用后端分析容量而不是服务于前端Web页面。好处是NameNode / Master是RegionServers已关闭，因此没有机会错过对StoreFiles或元数据的任何正在进行的更改。显而易见的是群集已关闭。步骤包括：</p>
<p>1.停止HBase<br>2.DistCp使用<br>Distcp可用于将HDFS中HBase目录的内容复制到另一个目录中的同一群集，或复制到另一个群集。</p>
<blockquote>
<p>注意：Distcp适用于这种情况，因为群集已关闭，并且没有对文件的正在进行的编辑。通常不建议在实时群集上对HBase目录中的文件进行分拣。</p>
</blockquote>
<p>3.恢复（如果需要）<br>从HDFS备份hbase目录通过distcp复制到’real’hbase目录。复制这些文件的行为会创建新的HDFS元数据，这就是为什么这种恢复不需要从HBase备份时恢复NameNode的原因，因为它是特定HDFS目录的恢复（通过distcp） （即HBase部分）不是整个HDFS文件系统。</p>
<h3 id="case1"><a href="#case1" class="headerlink" title="case1"></a>case1</h3><p>Hadoop层数据迁移<br>简单的distcp参数形式如下：</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">hadoop distcp hdf<span class="variable">s:</span>//src-hadoop-addres<span class="variable">s:9000</span>/table_name  hdf<span class="variable">s:</span>//dst-hadoop-addres<span class="variable">s:9000</span>/table_name</span><br></pre></td></tr></table></figure>
<p>如果是独立的MR集群来执行distcp，因为数据量很大，一般是按region目录粒度来传输，同时传输到目标集群时，我们先把文件传到临时目录，最后再目的集群上load表，我们用到的形式如下：</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">hadoop distcp \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dmapreduce</span>.</span></span>job.name=distcphbase \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dyarn</span>.</span></span>resourcemanager.webapp.address=mr-master-ip:<span class="number">8088</span>  \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dyarn</span>.</span></span>resourcemanager.resource-tracker.address=mr-master-dns:<span class="number">8093</span>   \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dyarn</span>.</span></span>resourcemanager.scheduler.address=mr-master-dns:<span class="number">8091</span>   \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dyarn</span>.</span></span>resourcemanager.address=mr-master-dns:<span class="number">8090</span>  \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dmapreduce</span>.</span></span>jobhistory.<span class="keyword">done</span>-dir=/history/<span class="keyword">done</span>/  \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dmapreduce</span>.</span></span>jobhistory.intermediate-<span class="keyword">done</span>-dir=/history/log/ \</span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dfs</span>.</span></span>defaultFS=hdfs:<span class="comment">//hbase-fs/ \</span></span><br><span class="line">-<span class="module-access"><span class="module"><span class="identifier">Dfs</span>.</span></span>default.name=hdfs:<span class="comment">//hbase-fs/ \</span></span><br><span class="line">-bandwidth <span class="number">20</span> \</span><br><span class="line">-m <span class="number">20</span> \</span><br><span class="line">hdfs:<span class="comment">//src-hadoop-address:9000/region-hdfs-path \</span></span><br><span class="line">hdfs:<span class="comment">//dst-hadoop-address:9000/tmp/region-hdfs-path</span></span><br></pre></td></tr></table></figure>

<p>在这个过程中，需要注意源端集群到目的端集群策略是通的，同时hadoop/hbase版本也要注意是否一致，如果版本不一致，最终load表时会报错。</p>
<h4 id="方案实施"><a href="#方案实施" class="headerlink" title="方案实施"></a>方案实施</h4><p>迁移方法如下：</p>
<h5 id="第一步，"><a href="#第一步，" class="headerlink" title="第一步，"></a>第一步，</h5><p>如果是迁移实时写的表，最好是停止集群对表的写入，迁移历史表的话就不用了，此处举例表名为test;</p>
<h5 id="第二步，"><a href="#第二步，" class="headerlink" title="第二步，"></a>第二步，</h5><p>flush表， 打开HBase Shell客户端，执行如下命令：</p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">hbase&gt; <span class="built_in">flush</span> <span class="string">'test'</span></span><br></pre></td></tr></table></figure>
<h5 id="第三步，"><a href="#第三步，" class="headerlink" title="第三步，"></a>第三步，</h5><p>拷贝表文件到目的路径，检查源集群到目标集群策略、版本等，确认没问题后，执行如上带MR参数的命令</p>
<h5 id="第四步，"><a href="#第四步，" class="headerlink" title="第四步，"></a>第四步，</h5><p>检查目标集群表是否存在，如果不存在需要创建与原集群相同的表结构</p>
<h5 id="第五步，"><a href="#第五步，" class="headerlink" title="第五步，"></a>第五步，</h5><p>在目标集群上，Load表到线上，在官方Load是执行如下命令：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hbase org<span class="selector-class">.jruby</span><span class="selector-class">.Main</span> add_table<span class="selector-class">.rb</span> /hbase/data/default/test</span><br></pre></td></tr></table></figure>

<p>对于我们来说，因我们先把文件同步到了临时目录，并不在原表目录，所以我们采用的另一种形式的load，即以region的维度来Load数据到线上表，怎么做呢，这里用到的是org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles这个类，即以bulkload的形式来load数据。上面同步时我们将文件同步到了目的集群的/tmp/region-hdfs-path目录，那么我们在Load时，可以用如下命令来Load region文件：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.LoadIncrementalHFiles</span> -Dhbase<span class="selector-class">.mapreduce</span><span class="selector-class">.bulkload</span><span class="selector-class">.max</span><span class="selector-class">.hfiles</span><span class="selector-class">.perRegion</span>.perFamily=<span class="number">1024</span> hdfs:<span class="comment">//dst-hadoop-address:9000/tmp/region-hdfs-path/region-name   table_name</span></span><br></pre></td></tr></table></figure>

<p>这里还用到一个参数hbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily, 这个表示在bulkload过程中，每个region列族的HFile数的上限，这里我们是限定了1024，也可以指定更少，根据实际需求来定。</p>
<h5 id="第六步，"><a href="#第六步，" class="headerlink" title="第六步，"></a>第六步，</h5><p>检查表数据是否OK，看bulkload过程是否有报错</p>
<p>在同步过程中，我们为加块同步速度，还会开个多线程来并发同步文件，这个可根据实际数据量和文件数来决定是否需要使用并发同步。</p>
<h2 id="二-实时群集备份-复制"><a href="#二-实时群集备份-复制" class="headerlink" title="二.实时群集备份 - 复制"></a>二.实时群集备份 - 复制</h2><h1 id="2-实时群集备份-CopyTable"><a href="#2-实时群集备份-CopyTable" class="headerlink" title="2.实时群集备份 - CopyTable"></a>2.实时群集备份 - CopyTable</h1><p>可复制实用程序可用于将数据从一个表复制到同一个集群上的另一个表，也可以将数据复制到另一个集群上的另一个表。</p>
<p>由于群集已启动，因此存在复制过程中可能遗漏编辑的风险。</p>
<p>copyTable也是属于HBase数据迁移的工具之一，以表级别进行数据迁移。copyTable的本质也是利用MapReduce进行同步的，与DistCp不同的时，它是利用MR去scan 原表的数据，然后把scan出来的数据写入到目标集群的表。这种方式也有很多局限，如一个表数据量达到T级，同时又在读写的情况下，全量scan表无疑会对集群性能造成影响。<br>来看下copyTable的一些使用参数：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="attribute">Usage</span>: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;</span><br><span class="line"></span><br><span class="line"><span class="attribute">Options:</span></span><br><span class="line"> rs.class     hbase.regionserver.class of the peer cluster</span><br><span class="line">              specify if different from current cluster</span><br><span class="line"> rs.impl      hbase.regionserver.impl of the peer cluster</span><br><span class="line"> startrow     the start row</span><br><span class="line"> stoprow      the stop row</span><br><span class="line"> starttime    beginning of the time range (unixtime in millis)</span><br><span class="line">              without endtime means from starttime to forever</span><br><span class="line"> endtime      end of the time range.  Ignored if no starttime specified.</span><br><span class="line"> versions     number of cell versions to copy</span><br><span class="line"> new.name     new table's name</span><br><span class="line"> peer.adr     Address of the peer cluster given in the format</span><br><span class="line">              hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</span><br><span class="line"> families     comma-separated list of families to copy</span><br><span class="line">              To copy from cf1 to cf2, give sourceCfName:destCfName. </span><br><span class="line">              To keep the same name, just give "cfName"</span><br><span class="line"> all.cells    also copy delete markers and deleted cells</span><br><span class="line"></span><br><span class="line"><span class="attribute">Args:</span></span><br><span class="line"> tablename    Name of the table to copy</span><br><span class="line"></span><br><span class="line"><span class="attribute">Examples:</span></span><br><span class="line"> To copy 'TestTable' to a cluster that uses replication for a 1 hour window:</span><br><span class="line"> $ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --starttime=1265875194289 --endtime=1265878794289 --peer.adr=server1,server2,server3:2181:/hbase --families=myOldCf:myNewCf,cf2,cf3 TestTable </span><br><span class="line">For performance consider the following general options:</span><br><span class="line">-Dhbase.client.scanner.caching=100</span><br><span class="line">-Dmapred.map.tasks.speculative.execution=false</span><br></pre></td></tr></table></figure>

<p>从上面参数，可以看出，copyTable支持设定需要复制的表的时间范围，cell的版本，也可以指定列簇，设定从集群的地址，起始/结束行键等。参数还是很灵活的。copyTable支持如下几个场景：</p>
<h4 id="1-表深度拷贝："><a href="#1-表深度拷贝：" class="headerlink" title="1.表深度拷贝："></a>1.表深度拷贝：</h4><p>相当于一个快照，不过这个快照是包含原表实际数据的，0.94.x版本之前是不支持snapshot快照命令的，所以用copyTable相当于可以实现对原表的拷贝， 使用方式如下：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">create <span class="string">'table_snapshot'</span>,&#123;NAME=&gt;<span class="string">"i"</span>&#125;</span><br><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.CopyTable</span> --new.name=tableCopy table_snapshot</span><br></pre></td></tr></table></figure>
<h4 id="2-集群间拷贝："><a href="#2-集群间拷贝：" class="headerlink" title="2.集群间拷贝："></a>2.集群间拷贝：</h4><p>在集群之间以表维度同步一个表数据，使用方式如下：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">create <span class="string">'table_test'</span>,&#123;NAME=&gt;<span class="string">"i"</span>&#125;#目的集群上先创建一个与原表结构相同的表</span><br><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.CopyTable</span> --peer.adr=zk-addr1,zk-addr2,zk-addr3:<span class="number">2181</span>:/hbase table_test</span><br></pre></td></tr></table></figure>

<h4 id="3-增量备份：增量备份表数据，参数中支持timeRange，指定要备份的时间范围，使用方式如下："><a href="#3-增量备份：增量备份表数据，参数中支持timeRange，指定要备份的时间范围，使用方式如下：" class="headerlink" title="3.增量备份：增量备份表数据，参数中支持timeRange，指定要备份的时间范围，使用方式如下："></a>3.增量备份：增量备份表数据，参数中支持timeRange，指定要备份的时间范围，使用方式如下：</h4><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable <span class="built_in">..</span>. <span class="attribute">--starttime</span>=start_timestamp <span class="attribute">--endtime</span>=end_timestamp</span><br></pre></td></tr></table></figure>

<h4 id="4-部分表备份：只备份其中某几个列族数据，比如一个表有很多列族，但我只想备份其中几个列族数据，CopyTable提供了families参数，同时还提供了copy列族到新列族形式，使用方式如下："><a href="#4-部分表备份：只备份其中某几个列族数据，比如一个表有很多列族，但我只想备份其中几个列族数据，CopyTable提供了families参数，同时还提供了copy列族到新列族形式，使用方式如下：" class="headerlink" title="4.部分表备份：只备份其中某几个列族数据，比如一个表有很多列族，但我只想备份其中几个列族数据，CopyTable提供了families参数，同时还提供了copy列族到新列族形式，使用方式如下："></a>4.部分表备份：只备份其中某几个列族数据，比如一个表有很多列族，但我只想备份其中几个列族数据，CopyTable提供了families参数，同时还提供了copy列族到新列族形式，使用方式如下：</h4><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.CopyTable</span> ... --families=srcCf1,srcCf2<span class="selector-id">#copy</span> cf1,cf2两个列族，不改变列族名字</span><br><span class="line">hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.CopyTable</span> ... --families=srcCf1:dstCf1, srcCf2:dstCf2 <span class="selector-id">#copy</span> srcCf1到目标dstCf1新列族</span><br></pre></td></tr></table></figure>
<p>总的来说，CopyTable支持的范围还是很多的，但因其涉及的是直接HBase层数据的拷贝，所以效率上会很低，同样需要在使用过程中限定扫描原表的速度和传输的带宽，这个工具实际上使用比较少，因为很难控制。</p>
<h3 id="实时群集备份-导出"><a href="#实时群集备份-导出" class="headerlink" title="实时群集备份 - 导出"></a>实时群集备份 - 导出</h3><p>该出口方式转储表到HDFS在同一个集群上的内容。要恢复数据，将使用导入实用程序。</p>
<p>由于群集已启动，因此存在导出过程中可能错过编辑的风险。</p>
<p>此方式与CopyTable类似，主要是将HBase表数据转换成Sequence File并dump到HDFS，也涉及Scan表数据，与CopyTable相比，还多支持不同版本数据的拷贝，同时它拷贝时不是将HBase数据直接Put到目标集群表，而是先转换成文件，把文件同步到目标集群后再通过Import到线上表。主要有两个阶段：</p>
<p><strong>Export阶段:</strong> 将原集群表数据Scan并转换成Sequence File到Hdfs上，因Export也是依赖于MR的，如果用到独立的MR集群的话，只要保证在MR集群上关于HBase的配置和原集群一样且能和原集群策略打通(master&amp;regionserver策略），就可直接用Export命令，如果没有独立MR集群，则只能在HBase集群上开MR，若需要同步多个版本数据，可以指定versions参数，否则默认同步最新版本的数据，还可以指定数据起始结束时间，使用如下：</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"> <span class="meta">#  output_hdfs_path可以直接是目标集群的hdfs路径，也可以是原集群的HDFS路径，如果需要指定版本号，起始结束时间</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export <span class="params">&lt;tableName&gt;</span> <span class="params">&lt;ouput_hdfs_path&gt;</span> <span class="params">&lt;versions&gt;</span> <span class="params">&lt;starttime&gt;</span> <span class="params">&lt;endtime&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>Import阶段:</strong>　将原集群Export出的SequenceFile导到目标集群对应表，使用如下：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">#如果原数据是存在原集群<span class="selector-tag">HDFS</span>，此处<span class="selector-tag">input_hdfs_path</span>可以是原集群的<span class="selector-tag">HDFS</span>路径，如果原数据存在目标集群<span class="selector-tag">HDFS</span>，则为目标集群的<span class="selector-tag">HDFS</span>路径</span><br><span class="line"><span class="selector-tag">hbase</span> <span class="selector-tag">org</span><span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.mapreduce</span><span class="selector-class">.Import</span> &lt;<span class="selector-tag">tableName</span>&gt; &lt;<span class="selector-tag">input_hdfs_path</span>&gt;</span><br></pre></td></tr></table></figure>
<h2 id="三-Snapshot方式"><a href="#三-Snapshot方式" class="headerlink" title="三 Snapshot方式"></a>三 Snapshot方式</h2><p>要打开快照支持，只需将该hbase.snapshot.enabled 属性设置为true即可。（默认情况下，快照在0.95+中启用，默认情况下在0.94.6+时关闭）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.snapshot.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>拍一张快照</li>
</ul>
<p>无论是启用还是禁用，您都可以创建表的快照。快照操作不涉及任何数据复制。</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">$ ./bin/hbase <span class="keyword">shell</span><span class="bash"> </span></span><br><span class="line">hbase&gt; snapshot <span class="string">'king_test'</span>,<span class="string">'snapshot_king_test'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>拍摄快照而不刷新</li>
</ul>
<p>默认行为是在拍摄快照之前在内存中执行数据刷新。这意味着内存中的数据包含在快照中。在大多数情况下，这是期望的行为。但是，如果您的设置可以容忍从快照中排除内存中的数据，则可以使用SKIP_FLUSH该snapshot命令的选项在拍摄快照时禁用和刷新。</p>
<figure class="highlight typescript"><table><tr><td class="code"><pre><span class="line">hbase&gt; snapshot <span class="string">'king_test'</span>,<span class="string">'snapshot_king_test'</span>,&#123;<span class="function"><span class="params">SKIP_FLUSH</span> =&gt;</span> <span class="literal">true</span>&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>列出快照</li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">$ ./bin/hbase <span class="keyword">shell</span><span class="bash"> </span></span><br><span class="line">hbase&gt; list_snapshots</span><br></pre></td></tr></table></figure>
<ul>
<li>删除快照</li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">$ ./bin/hbase <span class="keyword">shell</span><span class="bash"> </span></span><br><span class="line">hbase&gt; delete_snapshot <span class="string">'myTableSnapshot-122112'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>从快照克隆表<br>从快照中，您可以创建一个新表（克隆操作），其中包含拍摄快照时的相同数据。克隆操作不涉及数据副本，对克隆表的更改不会影响快照或原始表。</li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">$ ./bin/hbase <span class="keyword">shell</span><span class="bash"> </span></span><br><span class="line">hbase&gt; clone_snapshot <span class="string">'snapshot_king_test'</span>,<span class="string">'new_king_test'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>恢复快照</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/hbase shell</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">'king_test'</span></span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> restore_snapshot <span class="string">'snapshot_king_test'</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>导出到另一个群集<br>ExportSnapshot工具将与快照（hfiles，日志，快照元数据）相关的所有数据复制到另一个群集。该工具执行类似于distcp的Map-Reduce作业，以在两个集群之间复制文件，并且由于它在文件系统级别工作，因此hbase集群不必在线。</li>
</ul>
<p>使用16个映射器将名为MySnapshot的快照复制到HBase集群srv2（hdfs：/// srv2：8082 / hbase）：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">$ bin/hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.snapshot</span><span class="selector-class">.ExportSnapshot</span> -snapshot MySnapshot -copy-to hdfs:<span class="comment">//srv2:8082/hbase -mappers 16</span></span><br></pre></td></tr></table></figure>
<p>限制带宽消耗</p>
<p>您可以通过指定-bandwidth参数来限制导出快照时的带宽消耗，该参数需要一个表示每秒兆字节数的整数。以下示例将上述示例限制为200 MB /秒。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">$ bin/hbase org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.hbase</span><span class="selector-class">.snapshot</span><span class="selector-class">.ExportSnapshot</span> -snapshot MySnapshot -copy-to hdfs:<span class="comment">//srv2:8082/hbase -mappers 16 -bandwidth 200</span></span><br></pre></td></tr></table></figure>

<p>snapshot的流程主要有三个步骤：</p>
<h4 id="加锁"><a href="#加锁" class="headerlink" title="加锁:"></a>加锁:</h4><p>加锁对象是regionserver的memstore，目的是禁止在创建snapshot过程中对数据进行insert,update,delete操作</p>
<h4 id="刷盘："><a href="#刷盘：" class="headerlink" title="刷盘："></a>刷盘：</h4><p>刷盘是针对当前还在memstore中的数据刷到HDFS上，保证快照数据相对完整，此步也不是强制的，如果不刷会，快照中数据有不一致风险</p>
<h4 id="创建指针"><a href="#创建指针" class="headerlink" title="创建指针:"></a>创建指针:</h4><p>snapshot过程不拷贝数据，但会创建对HDFS文件的指针，snapshot中存储的就是这些指标元数据</p>
<h3 id="snapshot内部原理"><a href="#snapshot内部原理" class="headerlink" title="snapshot内部原理"></a>snapshot内部原理</h3><p>snapshot实际内部是怎么做的呢，上面说到，snapshot只是对元数据信息克隆，不拷贝实际数据文件，我们以表test为例，这个表有三个region, 每个region分别有两个HFile</p>
<p>创建的snapshot放在目录/hbase/.hbase-snapshot/下， 元数据信息放在/hbase/.hbase-snapshot/data.manifest中， snapshot中也分别包含对原表region HFile的引用，元数据信息具体包括哪哪些呢：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> snapshot元数据信息</span><br><span class="line"><span class="number">2.</span> 表的元数据信息&amp;schema，即原表的.tableinfo文件</span><br><span class="line"><span class="number">3.</span> 对原表Hfile的引用信息</span><br></pre></td></tr></table></figure>
<p>由于我们表的数据在实时变化，涉及region的Hfile合并删除等操作，对于snapshot而言，这部分数据HBase会怎么处理呢，实际上，当发现spit/compact等操作时，HBase会将原表发生变化的HFile拷贝到/hbase/.archive目录，如上图中如果Region3的F31&amp;F32发生变化，则F31和F32会被同步到.archive目录，这样发生修改的文件数据不至于失效</p>
<h3 id="snapshot数据迁移"><a href="#snapshot数据迁移" class="headerlink" title="snapshot数据迁移"></a>snapshot数据迁移</h3><p>snapshot的应用场景和上面CopyTable描述差不多，我们这里主要考虑的是数据迁移部分。数据迁移主要有以下几个步骤：</p>
<h4 id="A-创建快照："><a href="#A-创建快照：" class="headerlink" title="A.创建快照："></a>A.创建快照：</h4><p>在原集群上，用snapshot命令创建快照，命令如下：</p>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">hbase&gt; <span class="keyword">snapshot</span> <span class="string">'king_test'</span>, <span class="string">'snapshot_king_test'</span></span><br></pre></td></tr></table></figure>
<p>创建完快照后在/hbase根目录会产生一个目录：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="regexp">/hbase/</span>.hbase-snapshot<span class="regexp">/snapshot_king_test</span></span><br></pre></td></tr></table></figure>

<p>#子目录下有如下几个文件</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="regexp">/hbase/</span>.hbase-snapshot<span class="regexp">/ssnapshot_king_test/</span>.snapshotinfo  </span><br><span class="line"><span class="regexp">/hbase/</span>.hbase-snapshot<span class="regexp">/ssnapshot_king_test/</span>data.manifest</span><br></pre></td></tr></table></figure>

<h4 id="B-数据迁移"><a href="#B-数据迁移" class="headerlink" title="B.数据迁移:"></a>B.数据迁移:</h4><p>在上面创建好快照后，使用ExportSnapshot命令进行数据迁移，ExportSnapshot也是HDFS层的操作，本质还是利用MR进行迁移，这个过程主要涉及IO操作并消耗网络带宽，在迁移时要指定下map数和带宽，不然容易造成机房其它业务问题，如果是单独的MR集群，可以在MR集群上使用如下命令：</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">sudo su - hbase</span></span><br></pre></td></tr></table></figure>
<p>以下命令需要hbase账户执行</p>
<figure class="highlight haml"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-<span class="ruby">snapshot snapshot_king_test \</span></span><br><span class="line"><span class="ruby">-copy-from <span class="symbol">hdfs:</span>/<span class="regexp">/172.20.32.134:8020/hbase</span> \</span></span><br><span class="line"><span class="ruby">-copy-to <span class="symbol">hdfs:</span>/<span class="regexp">/172.17.194.18:8020/hbase</span> \</span></span><br><span class="line"><span class="ruby">-mappers <span class="number">20</span> \</span></span><br><span class="line"><span class="ruby">-bandwidth <span class="number">20</span></span></span><br></pre></td></tr></table></figure>
<p>目标集群</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>bin/hbase</span><br><span class="line">clone_snapshot <span class="string">'snapshot_king_test'</span>,<span class="string">'king_test'</span></span><br></pre></td></tr></table></figure>

<p>上面这些流程网上很多资料都有提到，对于我们业务来说，还有一种场景是要同步的表是正在实时写的，虽然用上面的也可以解决，但考虑到我们表数据规模很大，几十个T级别，同时又有实时业务在查的情况下，直接在原表上就算只是拷贝HFile，也会影响原集群机器性能，由于我们机器性能IO/内存方面本身就比较差，很容易导致机器异常，所以我们采用的其它一种方案，</p>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>oracle通过ogg传递数据到kafka</title>
    <url>/2019/02/25/oracle%E9%80%9A%E8%BF%87ogg%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE%E5%88%B0kafka/</url>
    <content><![CDATA[<h2 id="GoldenGate介绍"><a href="#GoldenGate介绍" class="headerlink" title="GoldenGate介绍"></a>GoldenGate介绍</h2><p>GoldenGate软件是一种基于日志的结构化数据复制软件。GoldenGate 能够实现大量交易数据的实时捕捉、变换和投递，实现源数据库与目标数据库的数据同步，保持亚秒级的数据延迟。</p>
<p>GoldenGate能够支持多种拓扑结构，包括一对一，一对多，多对一，层叠和双向复制等等。</p>
<h3 id="GoldenGate基本架构"><a href="#GoldenGate基本架构" class="headerlink" title="GoldenGate基本架构"></a>GoldenGate基本架构</h3><h4 id="Oracle-GoldenGate主要由如下组件组成"><a href="#Oracle-GoldenGate主要由如下组件组成" class="headerlink" title="Oracle GoldenGate主要由如下组件组成"></a>Oracle GoldenGate主要由如下组件组成</h4><ul>
<li>Extract</li>
<li>Data pump</li>
<li>Trails</li>
<li>Collector</li>
<li>Replicat</li>
<li>Manager<h4 id="Oracle-GoldenGate-数据复制过程如下："><a href="#Oracle-GoldenGate-数据复制过程如下：" class="headerlink" title="Oracle GoldenGate 数据复制过程如下："></a>Oracle GoldenGate 数据复制过程如下：</h4></li>
</ul>
<p>利用抽取进程(Extract Process)在源端数据库中读取Online Redo Log或者Archive Log，然后进行解析，只提取其中数据的变化信息，比如DML操作——增、删、改操作，将抽取的信息转换为GoldenGate自定义的中间格式存放在队列文件(trail file)中。再利用传输进程将队列文件(trail file)通过TCP/IP传送到目标系统。</p>
<p>目标端有一个进程叫Server Collector，这个进程接受了从源端传输过来的数据变化信息，把信息缓存到GoldenGate 队列文件(trail file)当中，等待目标端的复制进程读取数据。</p>
<p>GoldenGate 复制进程(replicat process)从队列文件(trail file)中读取数据变化信息，并创建对应的SQL语句，通过数据库的本地接口执行，提交到目标端数据库，提交成功后更新自己的检查点，记录已经完成复制的位置，数据的复制过程最终完成。</p>
<h4 id="①-Manager"><a href="#①-Manager" class="headerlink" title="① Manager"></a>① Manager</h4><ul>
<li><p>顾名思义、Manager进程是Golden Gate中进程的控制进程，用于管理 Extract，Data Pump，Replicat等进程</p>
</li>
<li><p>在 Extract、Data Pump、Replicat 进程启动之前，Manager 进程必须先要在源端和目标端启动</p>
</li>
<li><p>在整个 Golden Gate 运行期间，它必须保持运行状态</p>
</li>
</ul>
<p>⒈ 监控与启动 GoldenGate 的其它进程</p>
<p>⒉ 管理 trail 文件及 Reporting<br>在 Windows 系统上，Manager 进程是作为一个服务来启动的，在 Unix 系统下是一个进程</p>
<h4 id="②-Extract"><a href="#②-Extract" class="headerlink" title="② Extract"></a>② Extract</h4><p>Extract 进程运行在数据库源端上，它是Golden Gate的捕获机制，可以配置Extract 进程来做如下工作：</p>
<p>⒈ 初始数据装载：对于初始数据装载，Extract 进程直接从源对象中提取数据</p>
<p>⒉ 同步变化捕获：保持源数据与其它数据集的同步。初始数据同步完成后，Extract 进程捕获源数据的变化；如DML变化、 DDL变化等</p>
<h4 id="③-Replicat"><a href="#③-Replicat" class="headerlink" title="③ Replicat"></a>③ Replicat</h4><p>Replicat 进程是运行在目标端系统的一个进程，负责读取 Extract 进程提取到的数据（变更的事务或 DDL 变化）并应用到目标数据库</p>
<p>就像 Extract 进程一样，也可以配置 Replicat 进程来完成如下工作：</p>
<ol>
<li>初始化数据装载：对于初始化数据装载，Replicat 进程应用数据到目标对象或者路由它们到一个高速的 Bulk-load 工具上</li>
<li>数据同步，将 Extract 进程捕获到的提交了的事务应用到目标数据库中<h4 id="④-Collector"><a href="#④-Collector" class="headerlink" title="④ Collector"></a>④ Collector</h4>Collector 是运行在目标端的一个后台进程<br>接收从 TCP/IP网络传输过来的数据库变化，并写到 Trail 文件里动态</li>
</ol>
<p>collector：由管理进程自动启动的</p>
<p>collector 叫做动态</p>
<p>collector，用户不能与动态 collector 交互</p>
<p>静态 collector：可以配置成手工运行 collector，这个 collector 就称之为静态 collector</p>
<h4 id="⑤-Trails"><a href="#⑤-Trails" class="headerlink" title="⑤ Trails"></a>⑤ Trails</h4><p>为了持续地提取与复制数据库变化，GoldenGate 将捕获到的数据变化临时存放在磁盘上的一系列文件中，这些文件就叫做 Trail 文件</p>
<p>这些文件可以在 source DB 上也可以在目标 DB 上，也可以在中间系统上，这依赖于选择哪种配置情况</p>
<p>在数据库源端上的叫做 Local Trail 或者 Extract Trail；在目标端的叫做 Remote Trail</p>
<h4 id="⑥-Data-Pumps"><a href="#⑥-Data-Pumps" class="headerlink" title="⑥ Data Pumps"></a>⑥ Data Pumps</h4><p>Data Pump 是一个配置在源端的辅助的 Extract 机制</p>
<p>Data Pump 是一个可选组件，如果不配置 Data Pump，那么由 Extract 主进程将数据发送到目标端的 Remote Trail 文件中</p>
<p>如果配置了 Data Pump，会由 Data Pump将Extract 主进程写好的本地 Trail 文件通过网络发送到目标端的 Remote Trail 文件中</p>
<p>使用 Data Pump 的好处是：</p>
<p>⒈ 如果目标端或者网络失败，源端的 Extract 进程不会意外终止</p>
<p>⒉ 需要在不同的阶段实现数据的过滤或者转换</p>
<p>⒊ 多个源数据库复制到数据中心</p>
<p>⒋ 数据需要复制到多个目标数据库</p>
<h4 id="⑦-Data-source"><a href="#⑦-Data-source" class="headerlink" title="⑦ Data source"></a>⑦ Data source</h4><p>当处理事务的变更数据时，Extract 进程可以从数据库（Oracle, DB2, SQL Server, MySQL等）的事务日志中直接获取</p>
<p>或从 GoldenGate VAM中获取。通过 VAM，数据库厂商将提供所需的组件，用于 Extract 进程抽取数据的变更</p>
<h4 id="⑧-Groups"><a href="#⑧-Groups" class="headerlink" title="⑧ Groups"></a>⑧ Groups</h4><p>为了区分一个系统上的多个 Extract 和 Replicat 进程，我们可以定义进程组</p>
<p>例如：要并行复制不同的数据集，我们可以创建两个 Replicat 组</p>
<p>一个进程组由一个进程组成（Extract 进程或者 Replicat 进程），一个相应的参数文件，一个 Checkpoint 文件，以及其它与之相关的文件</p>
<p>如果处理组中的进程是 Replicat 进程，那么处理组还要包含一个 Checkpoint 表</p>
<h2 id="安装与基本配置"><a href="#安装与基本配置" class="headerlink" title="安装与基本配置"></a>安装与基本配置</h2><h5 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h5><p>软件配置 <a href="https://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html" target="_blank" rel="noopener">下载地址</a></p>
<table>
<thead>
<tr>
<th>角色</th>
<th>数据存储服务及版本</th>
<th>OGG版本</th>
<th>IP</th>
</tr>
</thead>
<tbody><tr>
<td>源服务器</td>
<td>OracleRelease11.2.0.4.0</td>
<td>Oracle GoldenGate 12.2.0.2 for Oracle on Linux x86-64</td>
<td>xxx.xxx.xxx.xx1</td>
</tr>
<tr>
<td>目标服务器</td>
<td>cdh6.0.0</td>
<td>Oracle GoldenGate for Big Data 12.3.2.1.1 on Linux x86-64</td>
<td>xxx.xxx.xxx.xx2</td>
</tr>
</tbody></table>
<blockquote>
<p>以上源服务器上OGG安装在Oracle用户下，目标服务器上OGG安装在root用户下。</p>
</blockquote>
<h3 id="2、源端（Oracle）配置"><a href="#2、源端（Oracle）配置" class="headerlink" title="2、源端（Oracle）配置"></a>2、源端（Oracle）配置</h3><p>注意：源端是安装了oracle的机器，oracle环境变量之前都配置好了</p>
<h4 id="2-1-解压"><a href="#2-1-解压" class="headerlink" title="2.1 解压"></a>2.1 解压</h4><p>先建立ogg目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/ogg</span><br></pre></td></tr></table></figure>

<p>解压后得到一个tar包，再解压这个tar</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /opt/ogg</span><br><span class="line">chown -R oracle:oinstall /opt/ogg</span><br></pre></td></tr></table></figure>
<p> （使oracle用户有ogg的权限，后面有些需要在oracle用户下执行才能成功</p>
<h4 id="2-2-配置ogg环境变量"><a href="#2-2-配置ogg环境变量" class="headerlink" title="2.2 配置ogg环境变量"></a>2.2 配置ogg环境变量</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export OGG_HOME=/opt/ogg</span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib</span><br><span class="line">export PATH=$OGG_HOME:$PATH</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>
<h5 id="Oracle打开归档模式"><a href="#Oracle打开归档模式" class="headerlink" title="Oracle打开归档模式"></a>Oracle打开归档模式</h5><p>使用如下命令查看当前是否为归档模式（archive）</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SQL</span>&gt; archive <span class="keyword">log</span> list </span><br><span class="line"><span class="keyword">Database</span> <span class="keyword">log</span> mode              Archive Mode</span><br><span class="line">Automatic archival             Enabled</span><br><span class="line">Archive destination            /u01/arch_log</span><br><span class="line">Oldest online <span class="keyword">log</span> <span class="keyword">sequence</span>     <span class="number">6</span></span><br><span class="line">Next <span class="keyword">log</span> <span class="keyword">sequence</span> <span class="keyword">to</span> archive   <span class="number">8</span></span><br><span class="line"><span class="keyword">Current</span> <span class="keyword">log</span> <span class="keyword">sequence</span>           <span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>如非以上状态，手动调整即可</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SQL</span>&gt; conn / <span class="keyword">as</span> sysdba(以DBA身份连接数据库) </span><br><span class="line"><span class="keyword">SQL</span>&gt; shutdown <span class="keyword">immediate</span>(立即关闭数据库)</span><br><span class="line"><span class="keyword">SQL</span>&gt; startup mount(启动实例并加载数据库，但不打开)</span><br><span class="line"><span class="keyword">SQL</span>&gt; <span class="keyword">alter</span> <span class="keyword">database</span> archivelog(更改数据库为归档模式)</span><br><span class="line"><span class="keyword">SQL</span>&gt; <span class="keyword">alter</span> <span class="keyword">database</span> <span class="keyword">open</span>(打开数据库)</span><br><span class="line"><span class="keyword">SQL</span>&gt; <span class="keyword">alter</span> <span class="keyword">system</span> archive <span class="keyword">log</span> <span class="keyword">start</span>(启用自动归档)</span><br></pre></td></tr></table></figure>

<h5 id="Oracle打开日志相关"><a href="#Oracle打开日志相关" class="headerlink" title="Oracle打开日志相关"></a>Oracle打开日志相关</h5><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容。通过一下命令查看当前状态：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SQL</span>&gt; <span class="keyword">select</span> force_logging, supplemental_log_data_min <span class="keyword">from</span> v$<span class="keyword">database</span>;</span><br><span class="line"><span class="keyword">FOR</span> SUPPLEME<span class="comment">--- --------</span></span><br><span class="line">YES YES</span><br></pre></td></tr></table></figure>

<p>如果以上查询结果非YES，可通过以下命令修改状态：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">SQL&gt; alter database force logging;</span><br><span class="line">SQL&gt; alter database add supplemental log data;</span><br></pre></td></tr></table></figure>

<h5 id="Oracle创建复制用户"><a href="#Oracle创建复制用户" class="headerlink" title="Oracle创建复制用户"></a>Oracle创建复制用户</h5><p>为了使Oracle里用户的复制权限更加单纯，故专门创建复制用户，并赋予dba权限</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SQL&gt; create tablespaceoggtbsdatafile '/u01/app/oracle/oradata/orcl/oggtbs01.dbf' size 1000M autoextend on;</span><br><span class="line">SQL&gt; create user ggs identified by ggs default tablespaceoggtbs;</span><br><span class="line">User created.</span><br><span class="line">SQL&gt; grant dba to ggs;</span><br><span class="line"><span class="keyword">Grant</span> succeeded.</span><br></pre></td></tr></table></figure>

<p>最终这个ggs帐号的权限如下所示：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SQL&gt; select * from dba_sys_privs where GRANTEE='GGS';</span><br><span class="line">GRANTEE                        PRIVILEGE                                ADM</span><br><span class="line">GGS                            <span class="keyword">DROP</span> <span class="keyword">ANY</span> <span class="keyword">DIRECTORY</span>                       <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">ALTER</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                          <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">ALTER</span> <span class="keyword">SESSION</span>                            <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">SELECT</span> <span class="keyword">ANY</span> DICTIONARY                    <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">CREATE</span> <span class="keyword">ANY</span> <span class="keyword">DIRECTORY</span>                     <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">RESTRICTED</span> <span class="keyword">SESSION</span>                       <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">FLASHBACK</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                      <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">UPDATE</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                         <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">DELETE</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                         <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">CREATE</span> <span class="keyword">TABLE</span>                             <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">INSERT</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                         <span class="keyword">NO</span></span><br><span class="line">GRANTEE                        PRIVILEGE                                ADM</span><br><span class="line">GGS                            <span class="keyword">UNLIMITED</span> <span class="keyword">TABLESPACE</span>                     <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">CREATE</span> <span class="keyword">SESSION</span>                           <span class="keyword">NO</span></span><br><span class="line">GGS                            <span class="keyword">SELECT</span> <span class="keyword">ANY</span> <span class="keyword">TABLE</span>                         <span class="keyword">NO</span></span><br></pre></td></tr></table></figure>

<h5 id="OGG初始化"><a href="#OGG初始化" class="headerlink" title="OGG初始化"></a>OGG初始化</h5><p>进入OGG的主目录执行./ggsci，进入OGG命令行</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">[oracle@VM_0_25_centos gg]$ ./ggsci </span><br><span class="line">Oracle GoldenGate Command Interpreter <span class="keyword">for</span> Oracle</span><br><span class="line">Version <span class="number">11.2</span>.<span class="number">1.0</span>.<span class="number">3</span> <span class="number">14400833</span> OGGCORE_11.<span class="number">2.1</span>.<span class="number">0.3</span>_PLATFORMS_120823.<span class="number">1258</span>_FBO</span><br><span class="line">Linux, x64, <span class="number">64</span>bit (optimized), Oracle <span class="number">11</span>g <span class="keyword">on</span> Aug <span class="number">23</span> <span class="number">2012</span> <span class="number">20</span>:<span class="number">20</span>:<span class="number">21</span></span><br><span class="line">Copyright (C) <span class="number">1995</span>, <span class="number">2012</span>, Oracle <span class="built_in">and</span>/<span class="built_in">or</span> its affiliates. All rights reserved.</span><br><span class="line">GGSCI (VM_0_25_centos) <span class="number">1</span>&gt;</span><br><span class="line">执行create subdirs进行目录创建</span><br><span class="line">GGSCI (VM_0_25_centos) <span class="number">4</span>&gt; create subdirs</span><br><span class="line">Creating subdirectories under current directory /u01/gg</span><br><span class="line">Parameter <span class="keyword">files</span>                /<span class="keyword">opt</span>/ogg/dirprm: already <span class="built_in">exists</span></span><br><span class="line">Report <span class="keyword">files</span>                   /<span class="keyword">opt</span>/ogg/dirrp<span class="variable">t:</span> already <span class="built_in">exists</span></span><br><span class="line">Checkpoint <span class="keyword">files</span>               /<span class="keyword">opt</span>/ogg/dirchk: already <span class="built_in">exists</span></span><br><span class="line">Process status <span class="keyword">files</span>           /<span class="keyword">opt</span>/ogg/dirpc<span class="variable">s:</span> already <span class="built_in">exists</span></span><br><span class="line">SQL script <span class="keyword">files</span>               /<span class="keyword">opt</span>/ogg/dirsq<span class="variable">l:</span> already <span class="built_in">exists</span></span><br><span class="line">Database definitions <span class="keyword">files</span>     /<span class="keyword">opt</span>/ogg/dirdef: already <span class="built_in">exists</span></span><br><span class="line">Extract data <span class="keyword">files</span>             /<span class="keyword">opt</span>/ogg/dirda<span class="variable">t:</span> already <span class="built_in">exists</span></span><br><span class="line">Temporary <span class="keyword">files</span>                /<span class="keyword">opt</span>/ogg/dirtmp: already <span class="built_in">exists</span></span><br><span class="line">Stdout <span class="keyword">files</span>                   /<span class="keyword">opt</span>/ogg/dirou<span class="variable">t:</span> already <span class="built_in">exists</span></span><br></pre></td></tr></table></figure>

<h5 id="Oracle创建模拟复制库表"><a href="#Oracle创建模拟复制库表" class="headerlink" title="Oracle创建模拟复制库表"></a>Oracle创建模拟复制库表</h5><p>模拟建一个用户叫tcloud，密码tcloud，同时基于这个用户建一张表，叫t_ogg。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">SQL&gt; create<span class="built_in"> user </span>tcloud  identified by tcloud<span class="built_in"> default </span>tablespace users;</span><br><span class="line">User created.</span><br><span class="line">SQL&gt; grant dba <span class="keyword">to</span> tcloud;</span><br><span class="line">Grant succeeded.</span><br><span class="line">SQL&gt; conn tcloud/tcloud;</span><br><span class="line">Connected.</span><br><span class="line">SQL&gt; create table t_ogg(id int ,text_name varchar(20),primary key(id));</span><br><span class="line">Table created.</span><br></pre></td></tr></table></figure>
<h4 id="配置全局变量"><a href="#配置全局变量" class="headerlink" title="配置全局变量"></a>配置全局变量</h4><p>在源端服务器OGG主目录下，执行./ggsci到OGG命令行下，执行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) 1&gt; dblogin userid ggs password ggs</span><br><span class="line">Successfully logged into database.</span><br><span class="line">GGSCI (VM_0_25_centos) 3&gt; view params ./globals</span><br><span class="line">ggschema ggs</span><br></pre></td></tr></table></figure>

<p>其中./globals变量没有的话可以用edit params ./globals来编辑添加即可（编辑器默认使用的vim）</p>
<h4 id="配置管理器mgr"><a href="#配置管理器mgr" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h4><p>在OGG命令行下执行如下命令：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">GGSCI</span> <span class="string">(VM_0_25_centos)</span> <span class="number">4</span><span class="string">&gt;</span> <span class="string">edit</span> <span class="string">param</span> <span class="string">mgr</span></span><br><span class="line"><span class="string">PORT</span> <span class="number">7809</span></span><br><span class="line"><span class="string">DYNAMICPORTLIST</span> <span class="number">7810</span><span class="number">-7909</span></span><br><span class="line"><span class="string">AUTORESTART</span> <span class="string">EXTRACT</span> <span class="string">*,RETRIES</span> <span class="number">5</span><span class="string">,WAITMINUTES</span> <span class="number">3</span></span><br><span class="line"><span class="string">PURGEOLDEXTRACTS</span> <span class="string">./dirdat/*,usecheckpoints,</span> <span class="string">minkeepdays</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>说明：PORT即mgr的默认监听端口；DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；PURGEOLDEXTRACTS即TRAIL文件的定期清理<br>在命令行下执行start mgr即可启动管理进程，通过info mgr可查看mgr状态</p>
</blockquote>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) 5&gt; <span class="builtin-name">info</span> mgr</span><br><span class="line">Manager is running (IP<span class="built_in"> port </span>VM_0_25_centos.7809).</span><br></pre></td></tr></table></figure>

<h5 id="添加复制表"><a href="#添加复制表" class="headerlink" title="添加复制表"></a>添加复制表</h5><p>在OGG命令行下执行添加需要复制的表的操作，如下：</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) 7&gt; <span class="builtin-name">add</span> trandata tcloud.t_ogg</span><br><span class="line">Logging of supplemental redo data enabled <span class="keyword">for</span> table TCLOUD.T_OGG.</span><br><span class="line">GGSCI (VM_0_25_centos) 8&gt; <span class="builtin-name">info</span> trandata tcloud.t_ogg</span><br><span class="line">Logging of supplemental redo log data is enabled <span class="keyword">for</span> table TCLOUD.T_OGG.</span><br><span class="line">Columns supplementally logged <span class="keyword">for</span> table TCLOUD.T_OGG: ID.</span><br></pre></td></tr></table></figure>

<h4 id="配置extract进程"><a href="#配置extract进程" class="headerlink" title="配置extract进程"></a>配置extract进程</h4><p>配置extract进程OGG命令行下执行如下命令：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">GGSCI</span> <span class="string">(VM_0_25_centos) 10&gt; edit params ext2hd</span></span><br><span class="line"><span class="attr">extract</span> <span class="string">ext2hd</span></span><br><span class="line"><span class="attr">dynamicresolution</span></span><br><span class="line"><span class="attr">SETENV</span> <span class="string">(ORACLE_SID = "orcl")</span></span><br><span class="line"><span class="attr">SETENV</span> <span class="string">(NLS_LANG = "american_america.AL32UTF8")</span></span><br><span class="line"><span class="attr">userid</span> <span class="string">ggs,password ggs</span></span><br><span class="line"><span class="attr">exttrail</span> <span class="string">/u01/gg/dirdat/tc</span></span><br><span class="line"><span class="attr">table</span> <span class="string">tcloud.t_ogg;</span></span><br></pre></td></tr></table></figure>

<p>说明：第一行指定extract进程名称；dynamicresolution动态解析；SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；userid ggs,password ggs即OGG连接Oracle数据库的帐号密码，这里使用2.3.4中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；table即复制表的表明，支持*通配，必须以;结尾<br>接下来在OGG命令行执行如下命令添加extract进程：</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) <span class="number">11</span>&gt; <span class="keyword">add </span><span class="keyword">extract </span><span class="keyword">ext2hd,tranlog,begin </span>now</span><br><span class="line"><span class="keyword">EXTRACT </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure>

<p>最后添加trail文件的定义与extract进程绑定：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) <span class="number">12</span>&gt; add exttrail <span class="regexp">/u01/gg</span><span class="regexp">/dirdat/</span>tc,extract ext2hd</span><br><span class="line">EXTTRAIL added</span><br></pre></td></tr></table></figure>

<p>可在OGG命令行下通过info命令查看状态：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) <span class="number">14</span>&gt; <span class="keyword">info</span> ext2hd</span><br><span class="line">EXTRACT    EXT2HD    Initialized   <span class="number">2016</span><span class="number">-11</span><span class="number">-09</span> <span class="number">15</span>:<span class="number">37</span>   Status STOPPED</span><br><span class="line"><span class="keyword">Checkpoint</span> Lag       <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> (updated <span class="number">00</span>:<span class="number">02</span>:<span class="number">32</span> ago)</span><br><span class="line"><span class="keyword">Log</span> <span class="keyword">Read</span> <span class="keyword">Checkpoint</span>  Oracle Redo Logs</span><br><span class="line">                     <span class="number">2016</span><span class="number">-11</span><span class="number">-09</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">14</span>  Seqno <span class="number">0</span>, RBA <span class="number">0</span></span><br><span class="line">                     SCN <span class="number">0.0</span> (<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h4 id="配置pump进程"><a href="#配置pump进程" class="headerlink" title="配置pump进程"></a>配置pump进程</h4><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程<br>在OGG命令行下执行：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">GGSCI</span> <span class="string">(VM_0_25_centos) 16&gt; edit params push2hd</span></span><br><span class="line"><span class="attr">extract</span> <span class="string">push2hd</span></span><br><span class="line"><span class="attr">passthru</span></span><br><span class="line"><span class="attr">dynamicresolution</span></span><br><span class="line"><span class="attr">userid</span> <span class="string">ggs,password ggs</span></span><br><span class="line"><span class="attr">rmthost</span> <span class="string">10.0.0.2 mgrport 7809</span></span><br><span class="line"><span class="attr">rmttrail</span> <span class="string">/data/gg/dirdat/tc</span></span><br><span class="line"><span class="attr">table</span> <span class="string">tcloud.t_ogg;</span></span><br></pre></td></tr></table></figure>

<p>说明：第一行指定extract进程名称；passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；dynamicresolution动态解析；userid ggs,password ggs即OGG连接Oracle数据库的帐号密码，这里使用2.3.4中特意创建的复制帐号；rmthost和mgrhost即目标端OGG的mgr服务的地址以及监听端口；rmttrail即目标端trail文件存储位置以及名称</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line">分别将本地trail文件和目标端的trail文件绑定到extract进程：</span><br><span class="line"></span><br><span class="line"><span class="symbol">GGSCI</span> (VM_0_25_centos) <span class="number">17</span>&gt; <span class="keyword">add </span>extract <span class="keyword">push2hd,exttrailsource </span>/u01/gg/dirdat/tc</span><br><span class="line"><span class="symbol">EXTRACT</span> <span class="keyword">added.</span></span><br><span class="line"><span class="keyword">GGSCI </span>(VM_0_25_centos) <span class="number">18</span>&gt; <span class="keyword">add </span>rmttrail /<span class="meta">data</span>/gg/dirdat/tc,extract <span class="keyword">push2hd</span></span><br><span class="line"><span class="keyword">RMTTRAIL </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure>

<p>同样可以在OGG命令行下使用info查看进程状态：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">GGSCI (VM_0_25_centos) <span class="number">19</span>&gt; <span class="keyword">info</span> push2hd</span><br><span class="line"></span><br><span class="line">EXTRACT    PUSH2HD   Initialized   <span class="number">2016</span><span class="number">-11</span><span class="number">-09</span> <span class="number">15</span>:<span class="number">52</span>   Status STOPPED</span><br><span class="line"><span class="keyword">Checkpoint</span> Lag       <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> (updated <span class="number">00</span>:<span class="number">01</span>:<span class="number">04</span> ago)</span><br><span class="line"><span class="keyword">Log</span> <span class="keyword">Read</span> <span class="keyword">Checkpoint</span>  File /u01/gg/dirdat/tc000000</span><br><span class="line">                     First <span class="type">Record</span>  RBA <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h4 id="配置define文件"><a href="#配置define文件" class="headerlink" title="配置define文件"></a>配置define文件</h4><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">GGSCI</span> <span class="string">(VM_0_25_centos) 20&gt; edit params tcloud</span></span><br><span class="line"><span class="attr">defsfile</span> <span class="string">/u01/gg/dirdef/tcloud.t_ogg</span></span><br><span class="line"><span class="attr">userid</span> <span class="string">ggs,password ggs</span></span><br><span class="line"><span class="attr">table</span> <span class="string">tcloud.t_ogg;</span></span><br></pre></td></tr></table></figure>

<p>在OGG主目录下执行：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./defgen paramfile dirprm/tcloud.prm</span><br></pre></td></tr></table></figure>

<p>完成之后会生成这样的文件/opt/ogg/dirdef/tcloud.t_ogg，将这个文件拷贝到目标端的OGG主目录下的dirdef目录即可。</p>
<h3 id="目标端的配置"><a href="#目标端的配置" class="headerlink" title="目标端的配置"></a>目标端的配置</h3><p>环境变量配置<br>vim /etc/profile</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#for ogg</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">OGG_HOME</span>=/opt/ogg</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LD_LIBRARY_PATH</span>=<span class="variable">$JAVA_HOME</span>/jre/lib/amd64:$JAVA_HOME/jre/lib/amd64/server:$JAVA_HOME/jre/lib/amd64/libjsig.so:$JAVA_HOME/jre/lib/amd64/server/libjvm.so:$OGG_HOME/lib</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=<span class="variable">$OGG_HOME</span>:$PATH</span><br></pre></td></tr></table></figure>
<h3 id="OGG初始化-1"><a href="#OGG初始化-1" class="headerlink" title="OGG初始化"></a>OGG初始化</h3><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"># ggsci</span><br><span class="line">GGSCI (dev-cdh<span class="number">-002</span>v) <span class="number">1</span>&gt; create subdirs</span><br></pre></td></tr></table></figure>
<h4 id="配置管理器manager"><a href="#配置管理器manager" class="headerlink" title="配置管理器manager"></a>配置管理器manager</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">GGSCI (dev-cdh<span class="number">-002</span>v) <span class="number">2</span>&gt; edit params mgr</span><br><span class="line">PORT <span class="number">7809</span></span><br><span class="line">DYNAMICPORTLIST <span class="number">7810</span><span class="number">-7909</span></span><br><span class="line">AUTORESTART EXTRACT *,RETRIES <span class="number">5</span>,WAITMINUTES <span class="number">3</span></span><br><span class="line">PURGEOLDEXTRACTS ./dirdat<span class="comment">/*,usecheckpoints, minkeepdays 3</span></span><br></pre></td></tr></table></figure>
<h4 id="配置checkpoint"><a href="#配置checkpoint" class="headerlink" title="配置checkpoint"></a>配置checkpoint</h4><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">GGSCI (dev-cdh<span class="number">-002</span>v) <span class="number">5</span>&gt; edit  params  ./GLOBALS</span><br><span class="line">CHECKPOINTTABLE tcloud.<span class="keyword">checkpoint</span></span><br></pre></td></tr></table></figure>

<p>保存即可</p>
<h4 id="配置replicate进程"><a href="#配置replicate进程" class="headerlink" title="配置replicate进程"></a>配置replicate进程</h4><p>本步骤接收端为hdfs文件<br>在OGG的命令行下执行：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">GGSCI</span> <span class="string">(dev-cdh-002v) 8&gt; edit params r2hdfs</span></span><br><span class="line"><span class="attr">REPLICAT</span> <span class="string">r2hdfs</span></span><br><span class="line"><span class="attr">sourcedefs</span> <span class="string">/data/gg/dirdef/tcloud.t_ogg</span></span><br><span class="line"><span class="attr">TARGETDB</span> <span class="string">LIBFILE libggjava.so SET property=dirprm/hdfs.props</span></span><br><span class="line"><span class="attr">REPORTCOUNT</span> <span class="string">EVERY 1 MINUTES, RATE </span></span><br><span class="line"><span class="attr">GROUPTRANSOPS</span> <span class="string">10000</span></span><br><span class="line"><span class="attr">MAP</span> <span class="string">tcloud.t_ogg, TARGET tcloud.t_ogg;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tcloud.t_ogg 文件由源端自动生成，放入主目录下dirdef 目录下即可</p>
</blockquote>
<p>其中上述中<br>管理器manager的配置文件 mgr.prm<br>replicate的配置文件 r2hdfs.prm<br>以及下面的hdfs接收配置文件 hdfs.props<br>均可在 /opt/ogg/dirprm/目录下编辑</p>
<p>hdfs.pops</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gg.handlerlist=hdfs</span><br><span class="line"></span><br><span class="line">gg.handler.hdfs.type=hdfs</span><br><span class="line">gg.handler.hdfs.pathMappingTemplate=/ogg/replication/hive</span><br><span class="line">gg.handler.hdfs.mode=op</span><br><span class="line">gg.handler.hdfs.format=delimitedtext</span><br><span class="line"></span><br><span class="line">goldengate.userexit.writers=javawriter</span><br><span class="line">javawriter.stats.display=TRUE</span><br><span class="line">javawriter.stats.full=TRUE</span><br><span class="line"></span><br><span class="line">gg.log=log4j</span><br><span class="line">gg.log.level=INFO</span><br><span class="line"></span><br><span class="line">gg.report.time=30sec</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">Sample gg.classpath <span class="keyword">for</span> Apache Hadoop</span></span><br><span class="line"><span class="meta">#</span><span class="bash">gg.classpath=/var/lib/hadoop/share/hadoop/common/*:/var/lib/hadoop/share/hadoop/common/lib/*:/var/lib/hadoop/share/hadoop/hdfs/*:/var/lib/hadoop/share/hadoop/hdfs/lib/*:/var/lib/hadoop/etc/hadoop/:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">Sample gg.classpath <span class="keyword">for</span> CDH</span></span><br><span class="line">gg.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/client/*:/etc/hadoop/conf</span><br><span class="line"><span class="meta">#</span><span class="bash">Sample gg.classpath <span class="keyword">for</span> HDP</span></span><br><span class="line"><span class="meta">#</span><span class="bash">gg.classpath=/usr/hdp/current/hadoop-client/client/*:/etc/hadoop/conf</span></span><br></pre></td></tr></table></figure>
<p>可参照 /opt/ogg/AdapterExamples/big-data/hdfs 目录下的样例文件进行编辑<br>具体的OGG for Big Data支持参数以及定义可参考地址<br><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-85A82B2E-CD51-463A-8674-3D686C3C0EC0.htm#GADBD376" target="_blank" rel="noopener">https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-85A82B2E-CD51-463A-8674-3D686C3C0EC0.htm#GADBD376</a></p>
<h3 id="kafka接收数据配置"><a href="#kafka接收数据配置" class="headerlink" title="kafka接收数据配置"></a>kafka接收数据配置</h3><p>新定义一个replicate 进程为rekafka.prm</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">REPLICAT rekafka</span><br><span class="line">sourcedefs /opt/ogg/dirdef/tcloud.t_ogg</span><br><span class="line">TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props</span><br><span class="line">REPORTCOUNT EVERY 1 MINUTES, RATE </span><br><span class="line">GROUPTRANSOPS 10000</span><br><span class="line">MAP tcloud.t_ogg, TARGET tcloud.t_ogg;</span><br></pre></td></tr></table></figure>


<p>另新建一个kafka接收配置文件<br>kafka.props</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gg.handlerlist=kafkahandler</span><br><span class="line">gg.handler.kafkahandler.type=kafka</span><br><span class="line">gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties</span><br><span class="line">gg.handler.kafkahandler.topicMappingTemplate=test_ogg</span><br><span class="line">gg.handler.kafkahandler.format=json</span><br><span class="line">gg.handler.kafkahandler.mode=op</span><br><span class="line"></span><br><span class="line">goldengate.userexit.writers=javawriter</span><br><span class="line">javawriter.stats.display=TRUE</span><br><span class="line">javawriter.stats.full=TRUE</span><br><span class="line"></span><br><span class="line">gg.log=log4j</span><br><span class="line">gg.log.level=INFO</span><br><span class="line"></span><br><span class="line">gg.report.time=30sec</span><br><span class="line"></span><br><span class="line">gg.classpath=dirprm/:/opt/cloudera/parcels/CDH/lib/kafka/libs/*:/opt/cloudera/parcels/CDH/etc/kafka/conf.dist/*:/opt/ogg/:/opt/ogg/lib/*</span><br><span class="line">javawriter.bootoptions=-Xmx512m -Xms32m -Djava.class.path=ggjava/ggjava.jar</span><br></pre></td></tr></table></figure>

<p>同时新建一个kafka信息配置文件custom_kafka_producer.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bootstrap.servers=172.17.194.18:9092,172.17.194.19:9092,172.17.194.20:9092</span><br><span class="line">acks=1</span><br><span class="line">compression.type=gzip</span><br><span class="line">reconnect.backoff.ms=1000</span><br><span class="line">value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">batch.size=102400</span><br><span class="line">linger.ms=10000</span><br></pre></td></tr></table></figure>

<p>以上文件同样可参考<br>/opt/ogg/AdapterExamples/big-data/kafka/目录下的样例文件或者<br><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449</a></p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="6-1-启动所有进程"><a href="#6-1-启动所有进程" class="headerlink" title="6.1 启动所有进程"></a>6.1 启动所有进程</h4><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。<br>启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。<br>全部需要在ogg目录下执行ggsci目录进入ogg命令行。 </p>
<p>源端依次是</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> extkafka</span><br><span class="line"><span class="literal">start</span> pukafka</span><br></pre></td></tr></table></figure>
<p>目标端</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> rekafka</span><br></pre></td></tr></table></figure>

<p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功<br>源端</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">GGSCI (ambari.master.com) <span class="number">5</span>&gt; info all</span><br><span class="line"></span><br><span class="line">Program     Status      Group       Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">EXTRACT     RUNNING     EXTKAFKA    <span class="number">04</span>:<span class="number">50</span>:<span class="number">21</span>      <span class="number">00</span>:<span class="number">00</span>:<span class="number">03</span>    </span><br><span class="line">EXTRACT     RUNNING     PUKAFKA     <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>      <span class="number">00</span>:<span class="number">00</span>:<span class="number">03</span></span><br></pre></td></tr></table></figure>

<p>目标端</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">GGSCI (ambari.slave1.com) 3&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">REPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</span><br></pre></td></tr></table></figure>


<h4 id="6-2-异常解决"><a href="#6-2-异常解决" class="headerlink" title="6.2 异常解决"></a>6.2 异常解决</h4><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p>
<p>vim ggser.log<br>1<br>或者ogg命令行,以rekafka进程为例</p>
<p>GGSCI (ambari.slave1.com) 2&gt; view report rekafka</p>
<h4 id="kafka查看过来的数据"><a href="#kafka查看过来的数据" class="headerlink" title="kafka查看过来的数据"></a>kafka查看过来的数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-topics --describe --zookeeper l172.17.194.18:2181,172.17.194.19:2181,172.17.194.20:2181 --topic test_ogg</span><br></pre></td></tr></table></figure>

<h4 id="添加主键"><a href="#添加主键" class="headerlink" title="添加主键"></a>添加主键</h4><p>在kafka.props添加</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">gg.handler.kafkahandler.format.includePrimaryKeys</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p><a href="https://www.cnblogs.com/purpleraintear/p/6071038.html" target="_blank" rel="noopener">https://www.cnblogs.com/purpleraintear/p/6071038.html</a></p>
<p><a href="https://blog.csdn.net/dkl12/article/details/80447154" target="_blank" rel="noopener">https://blog.csdn.net/dkl12/article/details/80447154</a></p>
<p><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449</a></p>
]]></content>
      <categories>
        <category>OGG</category>
      </categories>
      <tags>
        <tag>ogg</tag>
      </tags>
  </entry>
</search>
